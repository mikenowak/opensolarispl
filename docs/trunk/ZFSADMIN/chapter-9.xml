<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML//EN" "docbook.dtd"
[
<!ENTITY % xinclude SYSTEM "xinclude.mod">
]>
<chapter xmlns="http://docbook.org/ns/docbook" version="5.0" xml:id="gavwg">



<title>Usuwanie problemów i odzyskiwanie danych w ZFS-ie</title>
<toc>
<para>Ten rozdział opisuje w jaki sposłób identyfikować i naprawiać błędy w ZFS-ie. Zawarto równierz informacje dotyczącą zapobieganiu powstawania błędow.</para>
<para>W tym rozdziale znajduja sa nastepujace sekcje:</para>
<itemizedlist>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gbbth">Rodzaje błędów w ZFS-ie</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gbbwa">Sprawdzanie integralności danych w ZFS-ie</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gbbuw">Identyfikowanie problemów w ZFS-ie</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gbbve">Naprawa uszkodzonej konfiguracji ZFS</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gbbvb">Naprawa brakującego urządzenia</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gbbvf">Naprawa brakującego urządzenia</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gbbwl">Naprawa uszkodzonych danych</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gbbwc">Naprawa nie ładującego sie systemu</olink>
</para>
</listitem>
</itemizedlist>
</toc>
<sect1 xml:id="gbbth">
<title>Rodzaje błędów w ZFS-ie</title>
<para>Jako połączenie systemu plików oraz menedżera wolumenu, ZFS może zgłosić wiele
rodzajów błędów. Niniejszy rozdział rozpoczyna się przeglądem rodzajów awarii,
w późniejszej części opisuje w jaki sposób je identyfikować na działającym systemie
Na koniec podjęta jest dyskusja na temat usuwania znalezionych błędów.
ZFS może napotkać trzy podstawowe typy błędów:
<indexterm xml:id="indexterm-493">
<primary>Pule pamięci ZFS</primary>
<secondary>rodzaje błędów</secondary>
</indexterm>
<indexterm xml:id="indexterm-494">
<primary>rodzaje błędów</primary>
</indexterm>
<indexterm xml:id="indexterm-495">
<primary>usuwanie błędów</primary>
<secondary>rodzaje błędów w ZFS-ie</secondary>
</indexterm>
</para>
<itemizedlist>
<listitem>
<para>Brakujące urządzenia</para>
</listitem>
<listitem>
<para>Uszkodzone urządzenia</para>
</listitem>
<listitem>
<para>Uszkodzone dane</para>
</listitem>
</itemizedlist>
<para>Proszę zwrócić uwagę, że pojedyńcza pula może zawierać wszystkie trzy rodzaje
błędów, dlatego pełna procedura naprawy pociąga za sobą znajdowanie i usuwanie
wszystkich typów awarii.
</para>
<sect2 xml:id="gbbxj">
<title>Brakujące urządzenie w puli pamięci ZFS</title>
<para>Jeśli z systemu usunięto urządzenie, ZFS odkrywa, że nie może być
użyte i oznacza je stanem <literal>FAULTED</literal>. W zależności od poziomu 
replikacji, może to spowodować niedostępność całej puli. W przypadku usunięcia z macierzy 
RAID-Z lub mirrorowanego dysku jednego urządzenia, pula pozostaje dostępna. Jeżeli usunięto 
wszystkie mirrorowane komponenty, jedno urządzenie z macierzy RAID-Z, lub pojedyńczy dysk 
najwyższego poziomu, pula zostanie oznaczona stanem <literal>FAULTED</literal>. Do czasu 
ponownego podłączenia brakujących urządzeń, dostęp do danych z puli pozostaje 
niedostępny.<indexterm xml:id="indexterm-496">
<primary>Pule pamięci ZFS</primary>
<secondary>brakujące (uszkodzone) urządzenia</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-497">
<primary>rodzaje błędów</primary>
<secondary>brakujące (uszkodzone) urządzenia</secondary>
</indexterm>
<indexterm xml:id="indexterm-498">
<primary>usuwanie problemów</primary>
<secondary>brakujące (uszkodzone) urządzenia</secondary>
</indexterm>
</para>
</sect2>
<sect2 xml:id="gbbym">
<title>Uszkodzone urządzenia w puli pamięci ZFS</title>
<para>Określenie “uszkodzone” dotyczy szeregu możliwych błędów, które
przestawiono w poniższych przykładach:<indexterm xml:id="indexterm-499">
<primary>Pule pamięci ZFS</primary>
<secondary>uszkodzone urządzenia</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-500">
<primary>rodzaje błędów</primary>
<secondary>uszkodzone urządzenia</secondary>
</indexterm>
<indexterm xml:id="indexterm-501">
<primary>usuwanie problemów</primary>
<secondary>uszkodzone urządzenia</secondary>
</indexterm>
</para>
<itemizedlist>
<listitem>
<para>Przejściowe błędy we/wy związane z błędami dysku lub kontrolera</para>
</listitem>
<listitem>
<para>Uszkodzenie danych związane z promieniowaniem kosmicznym</para>
</listitem>
<listitem>
<para>Błędy w sterownikach, powodujące nieprawidłowy transfer danych w lub ze złych miejsc</para>
</listitem>
<listitem>
<para>Inny użytkownik, przypadkowo nadpisujący porcje danych na fizycznym urządzeniu</para>
</listitem>
</itemizedlist>
<para>W niektórych przypadkach błędy te są tymczasowe, jak na przykład losowe błędy we/wy podczas problemów z kontrolerem. W reszcie przypadków uszkodzenia są trwałe, jak na przykład fizyczne uszkodzenie dysku Nawet wtedy, kiedy uszkodzenia są trwałe, błędy niekoniecznie się powtórzą Na przykład, jeżeli administrator przez przypadek nadpisze część dysku, nie wystąpił błąd sprzętowy i urządzenie nie musi zostać wymienione. Identyfikacja problemów dyskowych nie jest prostym zadaniem i jest opisana bardziej szczegółowo w późniejszej sekcji.</para>
</sect2>
<sect2 xml:id="gbbwx">
<title>Zepsute dane ZFS</title>
<para>Uszkodzenie danych występuje, kiedy jedno lub więcej urządzeń (wliczając brakujące lub zniszczone urządzenia) dotyczy wirtualnego urządzenia najwyższego poziomu. Na przykład połowa mirroru może doświadczyć tysięcy błędów dyskowych, bez utraty danych. Kiedy błąd zostanie napotkany w tej samej lokacji po drugiej strony mirroru, rezultatem będzie uszkodzenie danych. <indexterm xml:id="indexterm-502"><primary>Pule pamięci</primary><secondary>uszkodzone dane</secondary><tertiary>opis</tertiary></indexterm><indexterm xml:id="indexterm-503"><primary>rodzaje błędów</primary><secondary>uszkodzone dane</secondary></indexterm><indexterm xml:id="indexterm-504"><primary>dane</primary><secondary>uszkodzone</secondary></indexterm>
</para>
<para>Zepsucie danych jest zawsze trwałe i wymaga specjalnej uwagi podczas naprawy. Nawet kiedy odpowiednie urządzenia zostaną naprawione lub wymienione, oryginalne dane są utracone na zawsze. W większości przypadków, ten scenariusz wymaga odtworzenia danych z kopi zapasowych. Podczas napotkania, błędy danych są rejestrowane i mogą zostać skontrolowane poprzez regularną weryfikację danych dysku, co zostało wyjaśnione w kolejnej sekcji. Kiedy popsuty blok zostanie usunięty, kolejna weryfikacja danych rozpoznaje, że uszkodzenie zostało naprawione i usuwa wszelkie ślady błędu z systemu.</para>
</sect2>
</sect1>
<sect1 xml:id="gbbwa">
<title>Sprawdzanie integralności danych w ZFS-ie</title>
<para>Dla ZFS nie istnieje równoważne narzędzie jak <command>fsck</command>. To narzędzie było stosowane w celu naprawiania danych oraz sprawdzania ich poprawności.</para>
<sect2 xml:id="gbbyc">
<title>Naprawa danych</title>
<para>W tradycyjnych systemach plików, sposób w jaki dane są zapisywane jest podatny na uszkodzenia co powoduje błędy w regularności danych. Ponieważ tradycyjne systemy plików nie są transakcyjne, niepowiązane bloki, uszkodzone linki lub inne niespójne struktury dane są dozwolone.  The addition of journaling does solve some of these problems, but can introduce additional problems when the log cannot be rolled back. Żaden z powyższych problemów nie istnieje dla ZFS. The only way for inconsistent data to exist on disk is through hardware failure (in which case the pool should have been replicated) or a bug in the ZFS software exists.<indexterm xml:id="indexterm-505"> <primary>Pule pamięci ZFS</primary> <secondary>data repair</secondary> <tertiary>opis</tertiary> </indexterm> <indexterm xml:id="indexterm-506"> <primary>checking</primary> <secondary>ZFS data integrity</secondary> </indexterm> <indexterm xml:id="indexterm-507"> <primary>data</primary> <secondary>repair</secondary> </indexterm>
</para>
<para>Biorąc pod uwagę, że narzędzie <command>fsck</command> zostało stworzone do naprawy znanych problemów specyficznych dla danych systemów plików, napisanie takiego narzędzia dla systemu plików, w którym problemy są nieznane nie jest możliwe. W przyszłości, problemy mogą okazać się na tyle wspólne, że stworzenie takiego narzędzia stanie się możliwe. Jednak problemy te mogą zostać zawsze ominięte przy użyciu replikowanych pul.</para>
<para>Jeśli pule nie są replikowane, szansa niedostępności danych jest zawsze możliwa.</para>
</sect2>
<sect2 xml:id="gbbyd">
<title>Sprawdzanie poprawności danych</title>
<para>Dodatkowo, oprócz naprawy danych, narzędzie <command>fsck</command> sprawdza dane na dysku pod względem ich poprawności. Tradycyjnie to zadanie odbywa się poprzez odmontowanie systemu plików oraz uruchomnienie narzędzia <command>fsck</command>. W tym przypadku może zaistnieć potrzeba uruchomienia systemu w trybie pojedyńczego użytkownika. Ten scenariusz powoduje przestój działania proporcjonalny do rozmiaru sprawdzanego systemu plików. Instead of requiring an explicit utility to perform the necessary checking, ZFS provides a mechanism to perform regular checking of all data. This functionality, known as <emphasis>scrubbing</emphasis>, is commonly used in memory and other systems as a method of detecting and preventing errors before they result in hardware or software failure.<indexterm xml:id="indexterm-508"> <primary>Pule pamięci ZFS</primary> <secondary>data validation</secondary> <tertiary>opis</tertiary> </indexterm> <indexterm xml:id="indexterm-509"> <primary>data</primary> <secondary>validation (scrubbing)</secondary> </indexterm> <indexterm xml:id="indexterm-510"> <primary>scrubbing</primary> <secondary>data validation</secondary> </indexterm>
</para>
</sect2>
<sect2 xml:id="gbbxi">
<title>Kontrola weryfikacji danych ZFS</title>
<para>Ilekroć ZFS napotka błąd poprzez operację weryfikacji lub zwykłego żądania dostępu do pliku, błąd ten jest logowany wewnętrznie. Daje to możliwość szybkiego przeglądu wszystkich znanych błędów w puli. <indexterm xml:id="indexterm-511"><primary>Pule pamięci ZFS</primary><secondary>weryfikacja danych</secondary><tertiary>opis&gt;</tertiary></indexterm><indexterm xml:id="indexterm-512"><primary>kontrola</primary><secondary> sprawdzanie poprawności danych (weryfikacja danych)</secondary></indexterm>
</para>
<sect3 xml:id="gbbws">
<title>Jawna kontrola danych ZFS</title>
<para>Najprostrzą metodą na sprawdzenie integralności danych jest zainicjowanie jawnej kontroli wszystkich danych dla puli. Ta operacja sprawdza jednokrotnie wszystkie dane w puli i weryfikuje, czy wszystkie bloki mogą być odczytane. Weryfikacja danych przebiega z maksymalną dostępną dla urządzenia prędkością, aczkolwiek priorytet każdej szczątkowej operacji we/wy pozostaje poniżej normalnych operacji. Weryfikacja danych może mieć negatywny wpływ na wydajność, jednak system powinien pozostawać czuły podczas weryfikacji. Aby zainicjować jawną kontrolę danych, użyj komendy <command>zpool scrub</command> Na przykład:<indexterm xml:id="indexterm-513"><primary>Pule pamięci ZFS</primary><secondary>weryfikacja danych</secondary><tertiary>(przykład)</tertiary></indexterm><indexterm xml:id="indexterm-514"><primary>weryfikacja</primary><secondary>(przykład)</secondary></indexterm><indexterm xml:id="indexterm-515"><primary>dane</primary><secondary>weryfikacja</secondary><tertiary>(przykład)</tertiary></indexterm>
</para>
<screen># <userinput>zpool scrub tank</userinput>
</screen>
<para>Status aktualnej jawnej kontroli może zostać wyświetlony przy użyciu komendy <command>zpool status</command>. Na przykład:</para>
<screen># <userinput>zpool status -v tank</userinput>
  pool: tank
 state: ONLINE
 scrub: scrub completed with 0 errors on Tue Mar 7 15:27:36 2006 config:

        NAME        STATE    READ  WRITE  CKSUM
        tank           ONLINE        0        0        0
          mirror        ONLINE        0        0        0
            c1t0d0      ONLINE        0        0        0
            c1t1d0       ONLINE        0        0        0

errors: No known data errors</screen>
<para>Należy zwrócić uwagę, że jednocześnie możliwa jest tylko jedna operacja weryfikacji danych dla każdej z puli.</para>
<para>Wykonywanie regularnej weryfikacji gwarantuje ciągłość we/wy dla wszystkich dysków w systemie. Regularna weryfikacja danych posiada uboczny efekt, powodujący uniemożliwienie menadżerowi zasilania postawienia niepracujących dysków w stanie niskiego poboru energii. Jeśli system w większości przypadków korzysta ciągle z operacji we/wy lub zużycie energii nie jest brane pod uwagę, wtedy ten problem może zostać bezpiecznie zignorowany.</para>
<para>Więcej informacji na temat interpretowania wyniku komendy <command>zpool status</command> można znaleźć w <olink targetdoc="" remap="internal" targetptr="gaynp">Status puli ZFS</olink></para>
</sect3>
<sect3 xml:id="gbbya">
<title>Jawna kontrola danych ZFS i Resilvering</title>
<para>Kiedy urządzenie zostaje wymienione, operacja resilveringu jest inicjowana. Zdrowe dane są kopiowane na nowe urządzenie. Ta akcja jest formą weryfikacji danych dysku. Dlatego jednocześnie tylko jedna taka akcja może się odbywać w obrębie jednej puli. If a scrubbing operation is in progress, a resilvering operation suspends the current scrubbing, and restarts after the resilvering is complete.<indexterm xml:id="indexterm-516"> <primary>Pule pamięci ZFS</primary> <secondary>data scrubbing and resilvering</secondary> <tertiary>opis</tertiary> </indexterm> <indexterm xml:id="indexterm-517"> <primary>resilvering and data scrubbing</primary> <secondary>opis</secondary> </indexterm> <indexterm xml:id="indexterm-518"> <primary>data</primary> <secondary>resilvering</secondary> <tertiary>opis</tertiary> </indexterm>
</para>
<para>Więcej informacji na temat resilweringu można znaleźć w <olink targetdoc="" remap="internal" targetptr="gbcus">Podgląd statusu resilweringu</olink></para>
</sect3>
</sect2>
</sect1>
<sect1 xml:id="gbbuw">
<title>Identyfikowanie problemów w ZFS-ie</title>
<para>Wykrywanie wszystkich usterek jest skupione wokół komendy <command>zpool status</command> This command analyzes the various failures in the system and identifies the most severe problem, presenting you with a suggested action and a link to a knowledge article for more information. Proszę zwrócić uwagę, że komenda ta identyfikuje pojedyńczy problem w puli, podczas gdy wiele problemów może istnieć. Na przykład, błędy uszkodzenia danych zawsze pociągają za sobą uszkodzenie jednego z urządzeń. Wymiana uszkodzonego urządzenia nie naprawia problemu uszkodzenia danych.</para>
<para>Dodatkowo, silnik diagnostyki ZFS zgłasza błędy puli oraz dysku. Błędy suma kontrolnej, we/wy, oraz puli związane z błędami urządzenia lub puli też są zgłaszane. Błędy ZFS zgłaszane przez <command>fmd</command> są wyświetlane zarówno na konsoli jak i w pliku wiadomości systemowych. In most cases, the <command>fmd</command> message directs you to the <command>zpool status</command> command for further recovery instructions.<indexterm xml:id="indexterm-519"> <primary>Pule pamięci ZFS</primary> <secondary>identifying problems</secondary> <tertiary>opis</tertiary> </indexterm> <indexterm xml:id="indexterm-520"> <primary>usuwanie problemów</primary> <secondary>identifying problems</secondary> </indexterm>
</para>
<para>Podstawowy proces odtworzenia wygląda następująco:</para>
<itemizedlist>
<listitem>
<para>Identify the errors through the <command>fmd</command> messages
that are displayed on the system console or in the <filename>/var/adm/messages</filename> files.</para>
</listitem>
<listitem>
<para>Znajdz przyszłe instrukcje naprawy przy użyciu komendy <command>zpool status -x</command>.</para>
</listitem>
<listitem>
<para>Napraw następujące błędy:</para>
<itemizedlist>
<listitem>
<para>Wymień brakujące lub uszkodzone urządzenie i podnieś je do stanu online.</para>
</listitem>
<listitem>
<para>Odzyskaj brakujące dane lub błędną konfiguracje z kopi zapasowej.</para>
</listitem>
<listitem>
<para>Potwierdź odzyskanie danych używając komendy <command>zpool status</command> <command>x</command>.</para>
</listitem>
<listitem>
<para>Stwórz kopie zapasową odzyskanej konfiguracji.</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para>This chapter describes how to interpret <command>zpool status</command> output
in order to diagnose the type of failure and directs you to one of the following
sections on how to repair the problem. While most of the work is performed
automatically by the command, it is important to understand exactly what problems
are being identified in order to diagnose the type of failure.</para>
<sect2 xml:id="gbcwb">
<title>Sprawdzanie, czy problem istnieje w puli ZFS.</title>
<para>Najprostrzym sposobem na sprawdzenie, czy istnieje jakikolwiek znany błąd w systemie, jest użycie komendy <command>zpool status</command> <option>x</option>. Ta komenda opisuje wyłącznie problemy związane z pulami. Jeśli w systemie nie istnieją uszkodzone pule, komenda wyświetla prosą wiadomość:</para>
<screen># <userinput>zpool status -x</userinput>
all pools are healthy</screen>
<para>Without the <option>
x</option> flag, the command displays the complete
status for all pools (or the requested pool, if specified on the command line),
even if the pools are otherwise healthy.<indexterm xml:id="indexterm-521">
<primary>Pule pamięci ZFS</primary>
<secondary>determining if problems exist (<command>zpool status</command> <option>
x</option>)</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-522">
<primary>usuwanie problemów</primary>
<secondary>determining if problems exist (<command>zpool status</command> <option>
x</option>)</secondary>
</indexterm>
</para>
<para>For more information about command-line options to the <command>zpool
status</command> command, see <olink targetdoc="" remap="internal" targetptr="gaynp">Querying ZFS Storage Pool Status</olink>.</para>
</sect2>
<sect2 xml:id="gbcve">
<title>Zrozumienie wyniku działania komendy <command>zpool status</command></title>
<para>Kompletny wynik komendy <command>zpool status</command> jest podobny do następującego:</para>
<screen># <userinput>zpool status tank</userinput>
  pool: tank
 state: DEGRADED
status: One or more devices has been taken offline by the administrator.
        Sufficient replicas exist for the pool to continue functioning in a
        degraded state.
action: Online the device using 'zpool online' or replace the device with
        'zpool replace'.
 scrub: none requested
 config:

        NAME        STATE    READ  WRITE  CKSUM
        tank         DEGRADED        0        0        0
          mirror      DEGRADED        0        0        0
            c1t0d0      ONLINE        0        0        0
            [fuzzy] c1t1d0       ONLINE        0        0        0

errors: No known data errors</screen>
<para>Wynik jest podzielony na kilka sekcji:</para>
<sect3 xml:id="gbcvl">
<title>Overall Pool Status Information</title>
<para>This header section in the <command>zpool status</command> output contains
the following fields, some of which are only displayed for pools exhibiting
problems:<indexterm xml:id="indexterm-523">
<primary>Pule pamięci ZFS</primary>
<secondary>overall pool status information for troubleshooting</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-524">
<primary>usuwanie problemów</primary>
<secondary>overall pool status information</secondary>
<tertiary>opis</tertiary>
</indexterm>
</para>
<variablelist>
<varlistentry>
<term>
<literal>pool</literal>
</term>
<listitem>
<para>The name of the pool.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>state</literal>
</term>
<listitem>
<para>The current health of the pool. This information refers only
to the ability of the pool to provide the necessary replication level. Pools
that are <literal>ONLINE</literal> might still have failing devices or data
corruption.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>status</literal>
</term>
<listitem>
<para>A description of what is wrong with the pool. To pole jest pominięte, jeśli nie zostały znalezione żadne problemy.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>action</literal>
</term>
<listitem>
<para>Rekomendowane czynności związane z naprawieniem błędów. This field is an abbreviated form directing the user to one of the following sections. To pole jest pominięte, jeśli nie zostały znalezione żadne problemy.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>zobacz</literal>
</term>
<listitem>
<para>A reference to a knowledge article containing detailed repair information. Online articles are updated more often than this guide can be updated, and should always be referenced for the most up-to-date repair procedures. To pole jest pominięte, jeśli nie zostały znalezione żadne problemy.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>scrub</literal>
</term>
<listitem>
<para>Identifies the current status of a scrub operation, which
might include the date and time that the last scrub was completed, a scrub
in progress, or if no scrubbing was requested.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>błędy</literal>
</term>
<listitem>
<para>Identifies known data errors or the absence of known data
errors.</para>
</listitem>
</varlistentry>
</variablelist>
</sect3>
<sect3 xml:id="gbcvv">
<title>Informacja o konfiguracji</title>
<para>The <literal>config</literal> field in the <command>zpool status</command> output
describes the configuration layout of the devices comprising the pool, as
well as their state and any errors generated from the devices. The state can
be one of the following: <literal>ONLINE</literal>, <literal>FAULTED</literal>, <literal>DEGRADED</literal>, <literal>UNAVAILABLE</literal>, or <literal>OFFLINE</literal>.
If the state is anything but <literal>ONLINE</literal>, the fault tolerance
of the pool has been compromised.</para>
<para>The second section of the configuration output displays error statistics. Te błędy są podzielone na trzy kategorie:</para>
<itemizedlist>
<listitem>
<para>
<literal>READ</literal> – I/O error occurred while issuing
a read request.</para>
</listitem>
<listitem>
<para>
<literal>WRITE</literal> – I/O error occurred while
issuing a write request.</para>
</listitem>
<listitem>
<para>
<literal>CKSUM</literal> – Checksum error. The device
returned corrupted data as the result of a read request.</para>
</listitem>
</itemizedlist>
<para>These errors can be used to determine if the damage is permanent. A
small number of I/O errors might indicate a temporary outage, while a large
number might indicate a permanent problem with the device. These errors do
not necessarily correspond to data corruption as interpreted by applications.
If the device is in a redundant configuration, the disk devices might show
uncorrectable errors, while no errors appear at the mirror or RAID-Z device
level. If this scenario is the case, then ZFS successfully retrieved the good
data and attempted to heal the damaged data from existing replicas.</para>
<para>For more information about interpreting these errors to determine device
failure, see <olink targetdoc="" remap="internal" targetptr="gbbzs">Determining the Type of Device Failure</olink>.</para>
<para>Finally, additional auxiliary information is displayed in the last column
of the <command>zpool status</command> output. This information expands on
the <literal>state</literal> field, aiding in diagnosis of failure modes/rodzaje błędów.
If a device is <literal>FAULTED</literal>, this field indicates whether the
device is inaccessible or whether the data on the device is corrupted. If
the device is undergoing resilvering, this field displays the current progress.</para>
<para>For more information about monitoring resilvering progress, see <olink targetdoc="" remap="internal" targetptr="gbcus">Viewing Resilvering Status</olink>.</para>
</sect3>
<sect3 xml:id="gbcvd">
<title>Scrubbing Status</title>
<para>The third section of the <command>zpool status</command> output describes
the current status of any explicit  scrubs. This
information is distinct from whether any errors are detected on the system,
though this information can be used to determine the accuracy of the data
corruption error reporting. If the last scrub ended recently, most likely,
any known data corruption has been discovered.</para>
<para>For more information about data scrubbing and how to interpret this
information, see <olink targetdoc="" remap="internal" targetptr="gbbwa">Checking ZFS Data Integrity</olink>.</para>
</sect3>
<sect3 xml:id="gbcwe">
<title>Błędy uszkodzenia danych</title>
<para>The <command>zpool status</command> command also shows whether any known
errors are associated with the pool. These errors might have been found during
disk scrubbing or during normal operation. ZFS maintains a persistent log
of all data errors associated with the pool. This log is rotated whenever
a complete scrub of the system finishes.</para>
<para>Błędy uszkodzenia danych są zawsze krytyczne. Their presence indicates that at least one application experienced an I/O error due to corrupt data within the pool. Device errors within a replicated pool do not result in data corruption and are not recorded as part of this log. Domyślnie, tylko znalezione błędy są wyświetlane. A complete list of errors and their specifics can be found by using the <command>zpool status</command> <option> v</option> option. For example:<indexterm xml:id="indexterm-525"> <primary>Pule pamięci ZFS</primary> <secondary>data corruption identified (<command>zpool status</command> <option> v</option>)</secondary> <tertiary>(example of)</tertiary> </indexterm> <indexterm xml:id="indexterm-526"> <primary>data</primary> <secondary>corruption identified (<command>zpool status</command> <option> v</option>)</secondary> <tertiary>(example of)</tertiary> </indexterm> <indexterm xml:id="indexterm-527"> <primary>usuwanie problemów</primary> <secondary>data corruption identified (<command>zpool status</command> <option> v</option>)</secondary> <tertiary>(example of)</tertiary> </indexterm>
</para>
<screen># <userinput>zpool status -v</userinput>
  pool: tank
 state: DEGRADED
status: One or more devices has experienced an error resulting in data
        corruption.   Applications may be affected.
action: Restore the file in question if possible.  Otherwise restore the
        entire pool from backup.
   see: http://www.sun.com/msg/ZFS-8000-8A
 scrub: resilver completed with 1 errors on Fri Mar 17 15:42:18 2006 
config:

        NAME        STATE    READ  WRITE  CKSUM
        tank         DEGRADED        0        0        1
          mirror      DEGRADED        0        0        1
            c1t0d0      ONLINE        0        0        2
            errors: The following persistent errors have been detected:

          DATASET  OBJECT   RANGE
          5        0       lvl=4294967295 blkid=0</screen>
<para>A similar message is also displayed by <command>fmd</command> on the
system console and the <filename>/var/adm/messages</filename> file. These
messages can also be tracked by using the <command>fmdump</command> command.</para>
<para>For more information about interpreting data corruption errors, see <olink targetdoc="" remap="internal" targetptr="gbcuz">Identifying the Type of Data Corruption</olink>.</para>
</sect3>
</sect2>
<sect2 xml:id="gbcvk">
<title>System Reporting of ZFS Error Messages</title>
<para>In addition to persistently keeping track of errors within the pool,
ZFS also displays syslog messages when events of interest occur. The following
scenarios generate events to notify the administrator:<indexterm xml:id="indexterm-528">
<primary>Pule pamięci ZFS</primary>
<secondary>system error messages</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-529">
<primary>displaying</primary>
<secondary>syslog reporting of ZFS error messages</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-530">
<primary>usuwanie problemów</primary>
<secondary>syslog reporting of ZFS error messages</secondary>
</indexterm>
</para>
<itemizedlist>
<listitem>
<para>
<emphasis role="strong">Device state transition</emphasis> – If a device becomes <literal>FAULTED</literal>, ZFS logs a message indicating that the fault tolerance of the pool might be compromised. Podobna wiadomość zostaje wysłana, jeżeli urządzenie </para>
</listitem>
<listitem>
<para>
<emphasis role="strong">Data corruption</emphasis> –
If any data corruption is detected, ZFS logs a message describing when and
where the corruption was detected. This message is only logged the first time
it is detected. Subsequent accesses do not generate a message.</para>
</listitem>
<listitem>
<para>
<emphasis role="strong">Pool failures and device failures</emphasis> –
If a pool failure or device failure occurs, the fault manager daemon reports
these errors through syslog messages as well as the <command>fmdump</command> command.</para>
</listitem>
</itemizedlist>
<para>If ZFS detects a device error and automatically recovers from it, no
notification occurs. Such errors do not constitute a failure in the pool redundancy
or data integrity. Moreover, such errors are typically the result of a driver
problem accompanied by its own set of error messages.</para>
</sect2>
</sect1>
<sect1 xml:id="gbbve">
<title>Naprawa uszkodzonej konfiguracji ZFS</title>
<para>ZFS maintains a cache of active pools and their configuration on the root file system. If this file is corrupted or somehow becomes out of sync with what is stored on disk, the pool can no longer be opened. ZFS tries to avoid this situation, though arbitrary corruption is always possible given the qualities of the underlying file system and storage. Ta sytuacja zwykle powoduje zniknięcie puli z systemu, kiedy w przeciwnym przypadku powinna być dostępna. This situation can also manifest itself as a partial configuration that is missing an unknown number of top-level virtual devices. In either case, the configuration can be recovered by exporting the pool (if it is visible at all), and re-importing it.<indexterm xml:id="indexterm-531"> <primary>Pule pamięci ZFS</primary> <secondary>repairing a damaged ZFS configuration</secondary> </indexterm> <indexterm xml:id="indexterm-532"> <primary>repairing</primary> <secondary>a damaged ZFS configuration</secondary> <tertiary>opis</tertiary> </indexterm> <indexterm xml:id="indexterm-533"> <primary>usuwanie problemów</primary> <secondary>repairing a damaged ZFS configuration</secondary> </indexterm>
</para>
<para>For more information about importing and exporting pools, see <olink targetdoc="" remap="internal" targetptr="gbchy">Migrating ZFS Storage Pools/Pule pamięci ZFS</olink>.</para>
</sect1>
<sect1 xml:id="gbbvb">
<title>Naprawa brakującego urządzenia</title>
<para>If a device cannot be opened, it displays as <literal>UNAVAILABLE</literal> in
the <command>zpool status</command> output. This status means that ZFS was
unable to open the device when the pool was first accessed, or the device
has since become unavailable. If the device causes a top-level virtual device
to be unavailable, then nothing in the pool can be accessed. Otherwise, the
fault tolerance of the pool might be compromised. In either case, the device
simply needs to be reattached to the system to restore normal operation.<indexterm xml:id="indexterm-534">
<primary>Pule pamięci ZFS</primary>
<secondary>replacing a missing device</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-535">
<primary>replacing</primary>
<secondary>a missing device</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-536">
<primary>usuwanie problemów</primary>
<secondary>replacing a missing device</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<para>Na przykład, po awarii urządzenia, możesz zobaczyć podobny komunikat pochodzący z <command>fmd</command>:</para>
<screen>SUNW-MSG-ID: ZFS-8000-D3, TYPE: Fault, VER: 1, SEVERITY: Major
EVENT-TIME: Fri Mar 17 14:38:47 MST 2006
PLATFORM: SUNW,Ultra-60, CSN: -, HOSTNAME: neo
SOURCE: zfs-diagnosis, REV: 1.0
EVENT-ID: 043bb0dd-f0a5-4b8f-a52d-8809e2ce2e0a
DESC: A ZFS device failed.  Refer to http://sun.com/msg/ZFS-8000-D3 for more information.
AUTO-RESPONSE: No automated response will occur.
IMPACT: Fault tolerance of the pool may be compromised.
RE-ACTION: Run 'zpool status -x' and replace the bad device.</screen>
<para>Następnym krokiem jest uzycie komendy <command>zpool status</command> <option>x</option> w celu zobaczenia dokładniejszej informacji na temat problemu urządzenia i podjęcia decyzji. Na przykład:</para>
<screen># <userinput>zpool status -x</userinput>
  pool: tank
 state: DEGRADED
status: One or more devices could not be opened.  Sufficient replicas exist for
        the pool continue functioning in a degraded state.
action: Attach the missing device and online it using 'zpool online'.
   see: http://www.sun.com/msg/ZFS-8000-D3
 scrub: resilver completed with 0 errors on Fri Mar 10 11:08:29 2006 
config:

        NAME        STATE    READ  WRITE  CKSUM
        tank         DEGRADED        0        0        0
          mirror      DEGRADED        0        0        0
            c0t1d0   UNAVAIL      0     0     0  cannot open
            c1t1d0      ONLINE        0        0        0</screen>
<para>You can see from this output that the missing device <literal>c0t1d0</literal> is not functioning. Jesli zostanie stwierdzone, ze urządzenie jest wadliwe, należy je wymienić.</para>
<para>Then, use the <command>zpool online</command> command to online the replaced device. Na przykład:</para>
<screen># <userinput>zpool online tank c0t1d0</userinput>
</screen>
<para>Confirm that the pool with the replaced device is healthy.</para>
<screen># <userinput>zpool status -x tank</userinput>
pool 'tank' is healthy</screen>
<sect2 xml:id="gbbxn">
<title>Physically Reattaching the Device</title>
<para>Exactly how a missing device is reattached depends on the device in
question. If the device is a network-attached drive, connectivity should be
restored. If the device is a USB or other removable media, it should be reattached
to the system. If the device is a local disk, a controller might have failed
such that the device is no longer visible to the system. In this case, the
controller should be replaced at which point the disks will again be available.
Other pathologies can exist and depend on the type of hardware and its configuration.
If a drive fails and it is no longer visible to the system (an unlikely event),
the device should be treated as a damaged device. Follow the procedures outlined
in <olink targetdoc="" remap="internal" targetptr="gbbvf">Naprawa uszkodzonego urządzenia</olink>.</para>
</sect2>
<sect2 xml:id="gbbyi">
<title>Notifying ZFS of Device Availability</title>
<para>Once a device is reattached to the system, ZFS might or might not automatically
detect its availability. If the pool was previously faulted, or the system
was rebooted as part of the attach procedure, then ZFS automatically rescans
all devices when it tries to open the pool. If the pool was degraded and the
device was replaced while the system was up, you must notify ZFS that the
device is now available and ready to be reopened by using the <command>zpool
online</command> command. For example:<indexterm xml:id="indexterm-537">
<primary>Pule pamięci ZFS</primary>
<secondary>notifying ZFS of reattached device (<command>zpool online</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-538">
<primary>notifying</primary>
<secondary>ZFS of reattached device (<command>zpool online</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-539">
<primary>usuwanie problemów</primary>
<secondary>notifying ZFS of reattached device (<command>zpool online</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool online tank c0t1d0</userinput>
</screen>
<para>For more information about bringing devices online, see <olink targetdoc="" remap="internal" targetptr="gazgk">Bringing a Device Online</olink>.</para>
</sect2>
</sect1>
<sect1 xml:id="gbbvf">
<title>Naprawa Uszodzonego Urządzenia</title>
<para>This section describes how to determine device failure types, clear
transient errors, and replace a device.</para>
<sect2 xml:id="gbbzs">
<title>Determining the Type of Device Failure</title>
<para>The term <emphasis>damaged device</emphasis> is rather vague, and can
describe a number of possible situations:<indexterm xml:id="indexterm-540">
<primary>Pule pamięci ZFS</primary>
<secondary>determining type of device failure</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-541">
<primary>determining</primary>
<secondary>type of device failure</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-542">
<primary>usuwanie problemów</primary>
<secondary>determining type of device failure</secondary>
<tertiary>opis</tertiary>
</indexterm>
</para>
<itemizedlist>
<listitem>
<para>
<emphasis role="strong">Bit rot</emphasis> – Over time, random events, such as magnetic influences and cosmic rays, can cause bits stored on disk to flip in unpredictable events. These events are relatively rare but common enough to cause potential data corruption in large or long-running systems. [fuzzy] Te błędy są podzielone na trzy kategorie:</para>
</listitem>
<listitem>
<para>
<emphasis role="strong">Misdirected reads or writes</emphasis> –
Firmware bugs or hardware faults can cause reads or writes of entire blocks
to reference the incorrect location on disk. These errors are typically transient,
though a large number might indicate a faulty drive.</para>
</listitem>
<listitem>
<para>
<emphasis role="strong">Administrator error</emphasis> – Administrators can unknowingly overwrite portions of the disk with bad data (such as copying <filename>/dev/zero</filename> over portions of the disk) that cause permanent corruption on disk. [fuzzy] [fuzzy] Te błędy są podzielone na trzy kategorie:</para>
</listitem>
<listitem>
<para>
<emphasis role="strong">Temporary outage</emphasis>–
A disk might become unavailable for a period time, causing I/Os to fail. This
situation is typically associated with network-attached devices, though local
disks can experience temporary outages as well. These errors might or might
not be transient.</para>
</listitem>
<listitem>
<para>
<emphasis role="strong">Bad or flaky hardware</emphasis> – This situation is a catch-all for the various problems that bad hardware exhibits. This could be consistent I/O errors, faulty transports causing random corruption, or any number of failures. Te błędy są zwykle trwałe.</para>
</listitem>
<listitem>
<para>
<emphasis role="strong">Offlined device</emphasis> –
If a device is offline, it is assumed that the administrator placed the device
in this state because it is presumed faulty. The administrator who placed
the device in this state can determine is this assumption is accurate.</para>
</listitem>
</itemizedlist>
<para>Determining exactly what is wrong can be a difficult process. The first
step is to examine the error counts in the <command>zpool status</command> output
as follows:</para>
<screen># <userinput>zpool status -v</userinput> <replaceable>pool</replaceable>
</screen>
<para>The errors are divided into I/O errors and checksum errors, both of which might indicate the possible failure type. Typical operation predicts a very small number of errors (just a few over long periods of time). If you are seeing large numbers of errors, then this situation probably indicates impending or complete device failure. However, the pathology for administrator error can result in large error counts. Innym źródłem informacji jest log systemowy. If the log shows a large number of SCSI or fibre channel driver messages, then this situation probably indicates serious hardware problems. If no syslog messages are generated, then the damage is likely transient.</para>
<para>The goal is to answer the following question:</para>
<para>
<emphasis>Is another error likely to occur on this device?</emphasis>
</para>
<para>Errors that happen only once are considered <emphasis>transient</emphasis>,
and do not indicate potential failure. Errors that are persistent or severe
enough to indicate potential hardware failure are considered “fatal.”
The act of determining the type of error is beyond the scope of any automated
software currently available with ZFS, and so much must be done manually by
you, the administrator. Once the determination is made, the appropriate action
can be taken. Either clear the transient errors or replace the device due
to fatal errors. These repair procedures are described in the next sections.</para>
<para>Even if the device errors are considered transient, it still may have
caused uncorrectable data errors within the pool. These errors require special
repair procedures, even if the underlying device is deemed healthy or otherwise
repaired. For more information on repairing data errors, see <olink targetdoc="" remap="internal" targetptr="gbbwl">Repairing Damaged Data</olink>.</para>
</sect2>
<sect2 xml:id="gbbzv">
<title>Clearing Transient Errors</title>
<para>If the device errors are deemed transient, in that they are unlikely
to effect the future health of the device, then the device errors can be safely
cleared to indicate that no fatal error occurred. To clear error counters
for RAID-Z or mirrored devices, use the <command>zpool clear</command> command.
For example:<indexterm xml:id="indexterm-543">
<primary>Pule pamięci ZFS</primary>
<secondary>clearing device errors (<command>zpool clear</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-544">
<primary>clearing</primary>
<secondary>device errors (<command>zpool clear</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-545">
<primary>usuwanie problemów</primary>
<secondary>clear device errors (<command>zpool clear</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool clear tank c1t0d0</userinput>
</screen>
<para>This syntax clears any errors associated with the device and clears
any data error counts associated with the device.</para>
<para>To clear all errors associated with the virtual devices in the pool,
and clear any data error counts associated with the pool, use the following
syntax:</para>
<screen># <userinput>zpool clear tank</userinput>
</screen>
<para>For more information about clearing pool errors, see <olink targetdoc="" remap="internal" targetptr="gazge">Clearing Storage Pool Devices</olink>.</para>
</sect2>
<sect2 xml:id="gbbzy">
<title>Replacing a Device in a ZFS Storage Pool</title>
<para>If device damage is permanent or future permanent damage is likely,
the device must be replaced. Whether the device can be replaced depends on
the configuration.</para>
<sect3 xml:id="gbcfb">
<title>Determining if a Device Can Be Replaced</title>
<para>For a device to be replaced, the pool must be in the <literal>ONLINE</literal> state.
The device must be part of a replicated configuration, or it must be healthy
(in the <literal>ONLINE</literal> state). If the disk is part of a replicated
configuration, sufficient replicas from which to retrieve good data must exist.
If two disks in a four-way mirror are faulted, then either disk can be replaced
because healthy replicas are available. However, if two disks in a four-way
RAID-Z device are faulted, then neither disk can be replaced because not enough
replicas from which to retrieve data exist. If the device is damaged but otherwise
online, it can be replaced as long as the pool is not in the <literal>FAULTED</literal> state.
However, any bad data on the device is copied to the new device unless there
are sufficient replicas with good data.<indexterm xml:id="indexterm-546">
<primary>Pule pamięci ZFS</primary>
<secondary>determining if a device can be replaced</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-547">
<primary>determining</primary>
<secondary>if a device can be replaced</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-548">
<primary>usuwanie problemów</primary>
<secondary>determining if a device can be replaced</secondary>
<tertiary>opis</tertiary>
</indexterm>
</para>
<para>In the following configuration, the disk <literal>c1t1d0</literal> can
be replaced, and any data in the pool is copied from the good replica, <literal>c1t0d0</literal>.</para>
<screen>[fuzzy] mirror      DEGRADED        0        0        0
    [fuzzy] c1t0d0      ONLINE        0        0        0
    [fuzzy] c1t1d0      ONLINE        0        0        0</screen>
<para>The disk <literal>c1t0d0</literal> can also be replaced, though no self-healing
of data can take place because no good replica is available.</para>
<para>In the following configuration, neither of the faulted disks can be
replaced. The <literal>ONLINE</literal> disks cannot be replaced either, because
the pool itself is faulted.</para>
<screen>[fuzzy] [fuzzy] c1t1d0      ONLINE        0        0        0
    [fuzzy] c1t0d0      ONLINE        0        0        0
    [fuzzy] [fuzzy] c1t1d0      ONLINE        0        0        0
    [fuzzy] [fuzzy] c1t1d0      ONLINE        0        0        0
    [fuzzy] [fuzzy] c1t0d0      ONLINE        0        0        0</screen>
<para>In the following configuration, either top-level disk can be replaced,
though any bad data present on the disk is copied to the new disk.</para>
<screen>[fuzzy] [fuzzy] c1t0d0      ONLINE        0        0        0</screen>
<para>If either disk were faulted, then no replacement could be performed
because the pool itself would be faulted.</para>
</sect3>
<sect3 xml:id="gbcdv">
<title>Unreplaceable Devices</title>
<para>If the loss of a device causes the pool to become faulted, or the device
contains too many data errors in an unreplicated configuration, then the device
cannot safely be replaced. Without sufficient replicas, no good data with
which to heal the damaged device exists. In this case, the only option is
to destroy the pool and re-create the configuration, restoring your data in
the process.</para>
<para>For more information about restoring an entire pool, see <olink targetdoc="" remap="internal" targetptr="gbctt">Repairing ZFS Storage Pool-Wide Damage</olink>.</para>
</sect3>
<sect3 xml:id="gbcet">
<title>[fuzzy] Naprawa Uszodzonego Urządzenia</title>
<para>Once you have determined that a device can be replaced, use the <command>zpool
replace</command> command to replace the device. If you are replacing the
damaged device with another different device, use the following command:<indexterm xml:id="indexterm-549">
<primary>Pule pamięci ZFS</primary>
<secondary>replacing a device (<command>zpool replace</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-550">
<primary>replacing</primary>
<secondary>a device (<command>zpool replace</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-551">
<primary>usuwanie problemów</primary>
<secondary>replacing a device (<command>zpool replace</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool replace tank c1t0d0 c2t0d0</userinput>
</screen>
<para>This command begins migrating data to the new device from the damaged device, or other devices in the pool if it is in a replicated configuration. When the command is finished, it detaches the damaged device from the configuration, at which point the device can be removed from the system. If you have already removed the device and replaced it with a new device in the same location, use the single device form of the command. Na przykład:</para>
<screen># <userinput>zpool replace tank c1t0d0</userinput>
</screen>
<para>This command takes an unformatted disk, formats it appropriately, and
then begins resilvering data from the rest of the configuration.</para>
<para>For more information about the <command>zpool replace</command> command,
see <olink targetdoc="" remap="internal" targetptr="gazgd">Replacing Devices in a Storage Pool</olink>.</para>
</sect3>
<sect3 xml:id="gbcus">
<title>Viewing Resilvering Status</title>
<para>The process of replacing a drive can take an extended period of time,
depending on the size of the drive and the amount of data in the pool. The
process of moving data from one device to another device is known as <emphasis>resilvering</emphasis>, and can be monitored by using the <command>zpool status</command> command.</para>
<para>Traditional file systems resilver data at the block level. Because ZFS
eliminates the artificial layering of the volume manager, it can perform resilvering
in a much more powerful and controlled manner. The two main advantages of
this feature are as follows:</para>
<itemizedlist>
<listitem>
<para>ZFS only resilvers the minimum amount of necessary data. In
the case of a short outage (as opposed to a complete device replacement),
the entire disk can be resilvered in a matter of minutes or seconds, rather
than resilvering the entire disk, or complicating matters with “dirty
region” logging that some volume managers support. When an entire disk
is replaced, the resilvering process takes time proportional to the amount
of data used on disk. Replacing a 500-Gbyte disk can take seconds if only
a few gigabytes of used space is in the pool.<indexterm xml:id="indexterm-552">
<primary>Pule pamięci ZFS</primary>
<secondary>viewing resilvering process</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-553">
<primary>replacing</primary>
<secondary>a device (<command>zpool replace</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-554">
<primary>usuwanie problemów</primary>
<secondary>replacing a device (<command>zpool replace</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
</listitem>
<listitem>
<para>Resilvering is interruptible and safe. If the system loses
power or is rebooted, the resilvering process resumes exactly where it left
off, without any need for manual intervention.</para>
</listitem>
</itemizedlist>
<para>Aby zobaczyć status jawnej kontroli danych, użyj komendy <command>zpool status</command> Na przykład:</para>
<screen># <userinput>zpool status tank</userinput>
  pool: tank
 state: DEGRADED
status: One or more devices is being resilvered.
action: Wait for the resilvering process to complete.
   see: http://www.sun.com/msg/ZFS-XXXX-08
 scrub: none requestes
config:
        NAME        STATE    READ  WRITE  CKSUM 
        tank         DEGRADED        0        0        0
          mirror      DEGRADED        0        0        0
            replacing         DEGRADED     0     0     0  52% resilvered
              c1t0d0      ONLINE        0        0        0
              c2t0d0      ONLINE        0        0        0  
            c1t1d0      ONLINE        0        0        0</screen>
<para>In this example, the disk <literal>c1t0d0</literal> is being replaced
by <literal>c2t0d0</literal>. This event is observed in the status output
by presence of the <emphasis>replacing</emphasis> virtual device in the configuration.
This device is not real, nor is it possible for you to create a pool by using
this virtual device type. The purpose of this device is solely to display
the resilvering process, and to identify exactly which device is being replaced. </para>
<para>Note that any pool currently undergoing resilvering is placed in the <literal>DEGRADED</literal> state, because the pool cannot provide the desired replication level until the resilvering process is complete. Resilvering proceeds as fast as possible, though the I/O is always scheduled with a lower priority than user-requested I/O, to minimize impact on the system. Once the resilvering is complete, the configuration reverts to the new, complete, configuration. Na przykład:</para>
<screen># <userinput>zpool status tank</userinput>
  pool: tank
 state: ONLINE
 scrub: scrub completed with 0 errors on Tue Mar 7 15:27:36 2006 config:

        NAME        STATE    READ  WRITE  CKSUM
        tank           ONLINE        0        0        0
          mirror        ONLINE        0        0        0
            [fuzzy] c2t0d0      ONLINE        0        0        0
            c1t1d0       ONLINE        0        0        0

errors: No known data errors</screen>
<para>The pool is once again <literal>ONLINE</literal>, and the original bad
disk (<literal>c1t0d0</literal>) has been removed from the configuration.</para>
</sect3>
</sect2>
</sect1>
<sect1 xml:id="gbbwl">
<title>Naprawa uszkodzonych danych</title>
<para>ZFS uses checksumming, replication, and self-healing data to minimize the chances of data corruption. Nonetheless, data corruption can occur if the pool isn't replicated, if corruption occurred while the pool was degraded, or an unlikely series of events conspired to corrupt multiple copies of a piece of data. Regardless of the source, the result is the same: The data is corrupted and therefore no longer accessible. The action taken depends on the type of data being corrupted, and its relative value. Dwa podstawowe typy danych mogą zostać uszkodzone:</para>
<itemizedlist>
<listitem>
<para>Pool metadata – ZFS requires a certain amount of data
to be parsed to open a pool and access datasets. If this data is corrupted,
the entire pool or complete portions of the dataset hierarchy will become
unavailable.</para>
</listitem>
<listitem>
<para>Object data – In this case, the corruption is within
a specific file or directory. This problem might result in a portion of the
file or directory being inaccessible, or this problem might cause the object
to be broken altogether.</para>
</listitem>
</itemizedlist>
<para>Dane są sprawdzane podczas normalnych operacji, ale również podczas sprawdzania weryfikacji danych. For more information about how to verify the integrity of pool data, see <olink targetdoc="" remap="internal" targetptr="gbbwa">Checking ZFS Data Integrity</olink>.</para>
<sect2 xml:id="gbcuz">
<title>Identyfikowanie rodzaju uszkodzenia danych</title>
<para>By default, the <command>zpool status</command> command shows only that
corruption has occurred, but not where this corruption occurred. For example:<indexterm xml:id="indexterm-555">
<primary>Pule pamięci ZFS</primary>
<secondary>identifying type of data corruption (<command>zpool status -v</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-556">
<primary>identifying</primary>
<secondary>type of data corruption (<command>zpool status -v</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-557">
<primary>usuwanie problemów</primary>
<secondary>determining type of data corruption (<command>zpool status -v</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool status tank -v</userinput>
   state: ONLINE
status: One or more devices has experienced an error resulting in data
                            corruption.   Applications may be affected.
	action: Restore the file in question if possible.  Otherwise restore the
                                                                                 entire pool from backup.
	   see: http://www.sun.com/msg/ZFS-8000-8A
	 scrub: none requested
	config:

		NAME         STATE     READ WRITE CKSUM
		tank         ONLINE       1     0     0
		  mirror     ONLINE       1     0     0
		    c2t0d0   ONLINE       2     0     0
		    c1t1d0   ONLINE       2     0     0

	errors: The following persistent errors have been detected:

		  DATASET  OBJECT  RANGE
		  tank     6       0-512</screen>
<para>Each error indicates only that an error occurred at the given point in time. Each error is not necessarily still present on the system. W zwykłych przypadkach ta sytuacja jest prawdziwa. Certain temporary outages might result in data corruption that is automatically repaired once the outage ends. A complete scrub of the pool is guaranteed to examine every active block in the pool, so the error log is reset whenever a scrub finishes. If you determine that the errors are no longer present, and you don't want to wait for a scrub to complete, reset all errors in the pool by using the <command>zpool online</command> command.</para>
<para>If the data corruption is in pool-wide metadata, the output is slightly different. Na przykład:</para>
<screen># <userinput>zpool status -v morpheus</userinput>
  pool: morpheus
    id: 1422736890544688191
 state: FAULTED
status: The pool metadata is corrupted.
action: The pool cannot be imported due to damaged devices/uszkodzone urządzenia or data.
   see: http://www.sun.com/msg/ZFS-8000-72

        morpheus    FAULTED   corrupted data
          c1t10d0   ONLINE</screen>
<para>In the case of pool-wide corruption, the pool is placed into the <literal>FAULTED</literal> state, because the pool cannot possibly provide the needed replication
level.</para>
</sect2>
<sect2 xml:id="gbctx">
<title>Naprawa uszkodzonego pliku lub katalogu</title>
<para>If a file or directory is corrupted, the system might still be able to function depending on the type of corruption. Uszkodzenie jest nie do naprawienia. Żadne zdrowe kopie danych nie istnieją w innych miejscach systemu. If the data is valuable, you have no choice but to restore the affected data from backup. Even so, you might be able to recover from this corruption without restoring the entire pool.<indexterm xml:id="indexterm-558"> <primary>Pule pamięci ZFS</primary> <secondary>repairing a corrupted file or directory</secondary> <tertiary>opis</tertiary> </indexterm> <indexterm xml:id="indexterm-559"> <primary>repairing</primary> <secondary>repairing a corrupted file or directory</secondary> <tertiary>opis</tertiary> </indexterm> <indexterm xml:id="indexterm-560"> <primary>usuwanie problemów</primary> <secondary>repairing a corrupted file or directory</secondary> <tertiary>opis</tertiary> </indexterm>
</para>
<para>If the damage is within a file data block, then the file can safely be removed, thereby clearing the error from the system. Pierwszym krokiem jest próba usunięcia pliku, używając komendy <command>rm</command>. If this command doesn't work, the corruption is within the file's metadata, and ZFS cannot determine which blocks belong to the file in order to remove the corruption.</para>
<para>If the corruption is within a directory or a file's metadata, the only
choice is to move the file elsewhere. You can safely move any file or directory
to a less convenient location, allowing the original object to be restored
in place.</para>
</sect2>
<sect2 xml:id="gbctt">
<title>Repairing ZFS Storage Pool-Wide Damage</title>
<para>If the damage is in pool metadata that damage prevents the pool from being opened, then you must restore the pool and all its data from backup. The mechanism you use varies widely by the pool configuration and backup strategy. Najpierw, zachowaj konfiguracje puli uzyskaną przy użyciu komendy <command>zpool status</command>, umożliwi to jej odtworzenie, kiedy zostanie zniszczona. Później użyj komendy <command>zpool destroy</command> <option>f</option> w celu zniszenia puli. Also, keep a file describing the layout of the datasets and the various locally set properties somewhere safe, as this information will become inaccessible if the pool is ever rendered inaccessible. With the pool configuration and dataset layout, you can reconstruct your complete configuration after destroying the pool. The data can then be populated by using whatever backup or restoration strategy you use.<indexterm xml:id="indexterm-561"> <primary>Pule pamięci ZFS</primary> <secondary>repairing pool-wide damage</secondary> <tertiary>opis</tertiary> </indexterm> <indexterm xml:id="indexterm-562"> <primary>repairing</primary> <secondary>pool-wide damage</secondary> <tertiary>opis</tertiary> </indexterm> <indexterm xml:id="indexterm-563"> <primary>usuwanie problemów</primary> <secondary>repairing pool-wide damage</secondary> <tertiary>opis</tertiary> </indexterm>
</para>
</sect2>
</sect1>
<sect1 xml:id="gbbwc">
<title>Naprawa nie ładującego sie systemu</title>
<para>ZFS is designed to be robust and stable despite errors. Even so, software
bugs or certain unexpected pathologies might cause the system to panic when
a pool is accessed. As part of the boot process, each pool must be opened,
which means that such failures will cause a system to enter into a panic-reboot
loop. In order to recover from this situation, ZFS must be informed not to
look for any pools on startup.<indexterm xml:id="indexterm-564">
<primary>Pule pamięci ZFS</primary>
<secondary>repairing an unbootable system</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-565">
<primary>repairing</primary>
<secondary>an unbootable system</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-566">
<primary>usuwanie problemów</primary>
<secondary>repairing an unbootable system</secondary>
<tertiary>opis</tertiary>
</indexterm>
</para>
<para>ZFS maintains an internal cache of available pools and their configurations
in <filename>/etc/zfs/zpool.cache</filename>. The location and contents of
this file are private and are subject to change. If the system becomes unbootable,
boot to the <literal>none</literal> milestone by using the <option>
m milestone=none</option> boot option. Once the system is up, remount your root file system
as writable and then remove <filename>/etc/zfs/zpool.cache</filename>. These
actions cause ZFS to forget that any pools exist on the system, preventing
it from trying to access the bad pool causing the problem. You can then proceed
to a normal system state by issuing the <command>svcadm milestone all</command> command.
You can use a similar process when booting from an alternate root to perform
repairs.</para>
<para>Once the system is up, you can attempt to import the pool by using the <command>zpool import</command> command. However, doing so will likely cause the same
error that occurred during boot, because the command uses the same mechanism
to access pools. If more than one pool is on the system and you want to import
a specific pool without accessing any other pools, you must re-initialize
the devices  in the damaged
pool, at which point you can safely import the good pool.</para>
</sect1>
</chapter>