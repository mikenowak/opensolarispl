<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML//EN" "docbook.dtd"
[
<!ENTITY % xinclude SYSTEM "xinclude.mod">
]>
<chapter xmlns="http://docbook.org/ns/docbook" version="5.0" xml:id="gavwg">



<title>Usuwanie problemów i odzyskiwanie danych w ZFS-ie</title>
<toc>
<para>Ten rozdział opisuje w jaki sposłób identyfikować i naprawiać błędy w ZFS-ie. Zawarto równierz informacje dotyczącą zapobieganiu powstawania błędow.</para>
<para>W tym rozdziale znajduja sa nastepujace sekcje:</para>
<itemizedlist>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gbbth">Rodzaje błędów w ZFS-ie</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gbbwa">Sprawdzanie integralności danych w ZFS-ie</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gbbuw">Identyfikowanie problemów w ZFS-ie</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gbbve">Naprawa uszkodzonej konfiguracji ZFS</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gbbvb">Naprawa brakującego urządzenia</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gbbvf">Naprawa brakującego urządzenia</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gbbwl">Naprawa uszkodzonych danych</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gbbwc">Naprawa nie ładującego sie systemu</olink>
</para>
</listitem>
</itemizedlist>
</toc>
<sect1 xml:id="gbbth">
<title>Rodzaje błędów w ZFS-ie</title>
<para>Jako połączenie systemu plików oraz menedżera wolumenu, ZFS może zgłosić wiele
rodzajów błędów. Niniejszy rozdział rozpoczyna się przeglądem rodzajów awarii,
w późniejszej części opisuje w jaki sposób je identyfikować na działającym systemie
Na koniec podjęta jest dyskusja na temat usuwania znalezionych błędów.
ZFS może napotkać trzy podstawowe typy błędów:
<indexterm xml:id="indexterm-493">
<primary>Pule pamięci ZFS</primary>
<secondary>rodzaje błędów</secondary>
</indexterm>
<indexterm xml:id="indexterm-494">
<primary>rodzaje błędów</primary>
</indexterm>
<indexterm xml:id="indexterm-495">
<primary>usuwanie błędów</primary>
<secondary>rodzaje błędów w ZFS-ie</secondary>
</indexterm>
</para>
<itemizedlist>
<listitem>
<para>Brakujące urządzenia</para>
</listitem>
<listitem>
<para>Uszkodzone urządzenia</para>
</listitem>
<listitem>
<para>Uszkodzone dane</para>
</listitem>
</itemizedlist>
<para>Proszę zwrócić uwagę, że pojedyńcza pula może zawierać wszystkie trzy rodzaje
błędów, dlatego pełna procedura naprawy pociąga za sobą znajdowanie i usuwanie
wszystkich typów awarii.
</para>
<sect2 xml:id="gbbxj">
<title>Brakujące urządzenie w puli pamięci ZFS</title>
<para>Jeśli z systemu usunięto urządzenie, ZFS odkrywa, że nie może być
użyte i oznacza je stanem <literal>FAULTED</literal>. W zależności od poziomu 
replikacji, może to spowodować niedostępność całej puli. W przypadku usunięcia z macierzy 
RAID-Z lub mirrorowanego dysku jednego urządzenia, pula pozostaje dostępna. Jeżeli usunięto 
wszystkie mirrorowane komponenty, jedno urządzenie z macierzy RAID-Z, lub pojedyńczy dysk 
najwyższego poziomu, pula zostanie oznaczona stanem <literal>FAULTED</literal>. Do czasu 
ponownego podłączenia brakujących urządzeń, dostęp do danych z puli pozostaje 
niedostępny.<indexterm xml:id="indexterm-496">
<primary>Pule pamięci ZFS</primary>
<secondary>brakujące (uszkodzone) urządzenia</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-497">
<primary>rodzaje błędów</primary>
<secondary>brakujące (uszkodzone) urządzenia</secondary>
</indexterm>
<indexterm xml:id="indexterm-498">
<primary>usuwanie problemów</primary>
<secondary>brakujące (uszkodzone) urządzenia</secondary>
</indexterm>
</para>
</sect2>
<sect2 xml:id="gbbym">
<title>Uszkodzone urządzenia w puli pamięci ZFS</title>
<para>Określenie “uszkodzone” dotyczy szeregu możliwych błędów, które
przestawiono w poniższych przykładach:<indexterm xml:id="indexterm-499">
<primary>Pule pamięci ZFS</primary>
<secondary>uszkodzone urządzenia</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-500">
<primary>rodzaje błędów</primary>
<secondary>uszkodzone urządzenia</secondary>
</indexterm>
<indexterm xml:id="indexterm-501">
<primary>usuwanie problemów</primary>
<secondary>uszkodzone urządzenia</secondary>
</indexterm>
</para>
<itemizedlist>
<listitem>
<para>Przejściowe błędy we/wy związane z błędami dysku lub kontrolera</para>
</listitem>
<listitem>
<para>Uszkodzenie danych związane z promieniowaniem kosmicznym</para>
</listitem>
<listitem>
<para>Błędy w sterownikach, powodujące nieprawidłowy transfer danych do lub ze złego miejsca</para>
</listitem>
<listitem>
<para>Inny użytkownik, przypadkowo nadpisujący porcje danych na fizycznym urządzeniu</para>
</listitem>
</itemizedlist>
<para>W niektórych przypadkach błędy te są tymczasowe, jak na przykład losowe błędy we/wy podczas problemów z kontrolerem. W reszcie przypadków uszkodzenia są trwałe, jak na przykład fizyczne uszkodzenie dysku. Nawet wtedy, kiedy uszkodzenia są trwałe, błędy niekoniecznie się powtórzą. Na przykład, jeżeli administrator przez przypadek nadpisze część dysku, nie wystąpił błąd sprzętowy i urządzenie nie musi zostać wymienione. Identyfikacja problemów dyskowych nie jest prostym zadaniem i jest opisana bardziej szczegółowo w późniejszej sekcji.</para>
</sect2>
<sect2 xml:id="gbbwx">
<title>Zepsute dane ZFS</title>
<para>Uszkodzenie danych występuje, kiedy jedno lub więcej urządzeń (wliczając brakujące lub zniszczone urządzenia) dotyczy wirtualnego urządzenia najwyższego poziomu. Na przykład połowa mirroru może doświadczyć tysięcy błędów dyskowych, bez utraty danych. Kiedy błąd zostanie napotkany w tej samej lokacji po drugiej strony mirroru, rezultatem będzie uszkodzenie danych. <indexterm xml:id="indexterm-502"><primary>Pule pamięci</primary><secondary>uszkodzone dane</secondary><tertiary>opis</tertiary></indexterm><indexterm xml:id="indexterm-503"><primary>rodzaje błędów</primary><secondary>uszkodzone dane</secondary></indexterm><indexterm xml:id="indexterm-504"><primary>dane</primary><secondary>uszkodzone</secondary></indexterm>
</para>
<para>Zepsucie danych jest zawsze trwałe i wymaga specjalnej uwagi podczas naprawy. Nawet kiedy odpowiednie urządzenia zostaną naprawione lub wymienione, oryginalne dane są utracone na zawsze. W większości przypadków, ten scenariusz wymaga odtworzenia danych z kopi zapasowych. Podczas napotkania, błędy danych są rejestrowane i mogą zostać skontrolowane poprzez regularną weryfikację danych dysku, co zostało wyjaśnione w kolejnej sekcji. Kiedy popsuty blok zostanie usunięty, kolejna weryfikacja danych rozpoznaje, że uszkodzenie zostało naprawione i usuwa wszelkie ślady błędu z systemu.</para>
</sect2>
</sect1>
<sect1 xml:id="gbbwa">
<title>Sprawdzanie integralności danych w ZFS-ie</title>
<para>Dla ZFS nie istnieje równoważne narzędzie jak <command>fsck</command>. To narzędzie było stosowane w celu naprawiania danych oraz sprawdzania ich poprawności.</para>
<sect2 xml:id="gbbyc">
<title>Naprawa danych</title>
<para>W tradycyjnych systemach plików, sposób w jaki dane są zapisywane jest podatny na uszkodzenia co powoduje błędy w regularności danych. Ponieważ tradycyjne systemy plików nie są transakcyjne, niepowiązane bloki, uszkodzone linki lub inne niespójne struktury danych są dozwolone.  Dodatkowo dzienniki (jouornal) rozwiązują niektóre z tych problemów, ale mogą powodować dodatkowe, kiedy dziennik nie może zostać odtworzony. Żaden z powyższych problemów nie istnieje dla ZFS. Jedynym sposobem na niespójne dane na dysku jest uszkodzenie sprzętowe (w tym wypadku pula powinna była zostać zreplikowana) lub błąd w oprogramowaniu ZFS.<indexterm xml:id="indexterm-505"><primary>Pule pamięci ZFS</primary><secondary>naprawa danych</secondary><tertiary>opis</tertiary></indexterm><indexterm xml:id="indexterm-506"><primary>sprawdzanie</primary><secondary>Spójność danych ZFS</secondary></indexterm><indexterm xml:id="indexterm-507"><primary>dane</primary><secondary>naprawa</secondary></indexterm>
</para>
<para>Biorąc pod uwagę, że narzędzie <command>fsck</command> zostało stworzone do naprawy znanych problemów specyficznych dla danych systemów plików, napisanie takiego narzędzia dla systemu plików, w którym problemy są nieznane nie jest możliwe. W przyszłości, problemy mogą okazać się na tyle wspólne, że stworzenie takiego narzędzia stanie się możliwe. Jednak problemy te mogą zostać zawsze ominięte przy użyciu replikowanych pul.</para>
<para>Jeśli pule nie są replikowane, szansa niedostępności danych jest zawsze możliwa.</para>
</sect2>
<sect2 xml:id="gbbyd">
<title>Sprawdzanie poprawności danych</title>
<para>Dodatkowo, oprócz naprawy danych, narzędzie <command>fsck</command> sprawdza dane na dysku pod względem ich poprawności. Tradycyjnie to zadanie odbywa się poprzez odmontowanie systemu plików oraz uruchomnienie narzędzia <command>fsck</command>. W tym przypadku może zaistnieć potrzeba uruchomienia systemu w trybie pojedyńczego użytkownika. Ten scenariusz powoduje przestój działania proporcjonalny do rozmiaru sprawdzanego systemu plików. Zamiast konieczności używania odpowiedniego narzędzia do sprawdzania danych, ZFS oferuje mechanizm sprawdzający je regularnie. Ta funkcjonalność jest znana jako <emphasis>weryfikacja danych (scrubbing)</emphasis> i jest w powrzechnym użyciu w pamięci podręcznej jako metoda wykrywania i zapobiegania błędom, zanim spowodują awarie sprzętową lub programową. <indexterm xml:id="indexterm-508"><primary>Pule pamięci ZFS</primary><secondary>sprawdzanie poprawności danych</secondary><tertiary>opis</tertiary></indexterm><indexterm xml:id="indexterm-509"><primary>dane</primary><secondary>weryfikacja danych (scrubbing)</secondary></indexterm><indexterm xml:id="indexterm-510"><primary>weryfikacja danych</primary><secondary>sprawdzanie poprawności danych</secondary></indexterm>
</para>
</sect2>
<sect2 xml:id="gbbxi">
<title>Kontrola weryfikacji danych ZFS</title>
<para>Ilekroć ZFS napotka błąd poprzez operację weryfikacji lub zwykłego żądania dostępu do pliku, błąd ten jest logowany wewnętrznie. Daje to możliwość szybkiego przeglądu wszystkich znanych błędów w puli. <indexterm xml:id="indexterm-511"><primary>Pule pamięci ZFS</primary><secondary>weryfikacja danych</secondary><tertiary>opis&gt;</tertiary></indexterm><indexterm xml:id="indexterm-512"><primary>kontrola</primary><secondary> sprawdzanie poprawności danych (weryfikacja danych)</secondary></indexterm>
</para>
<sect3 xml:id="gbbws">
<title>Jawna kontrola danych ZFS</title>
<para>Najprostrzą metodą na sprawdzenie integralności danych jest zainicjowanie jawnej kontroli wszystkich danych dla puli. Ta operacja sprawdza jednokrotnie wszystkie dane w puli i weryfikuje, czy wszystkie bloki mogą być odczytane. Weryfikacja danych przebiega z maksymalną dostępną dla urządzenia prędkością, aczkolwiek priorytet każdej szczątkowej operacji we/wy pozostaje poniżej normalnych operacji. Weryfikacja danych może mieć negatywny wpływ na wydajność, jednak system powinien pozostawać czuły podczas weryfikacji. Aby zainicjować jawną kontrolę danych, użyj komendy <command>zpool scrub</command> Na przykład:<indexterm xml:id="indexterm-513"><primary>Pule pamięci ZFS</primary><secondary>weryfikacja danych</secondary><tertiary>(przykład)</tertiary></indexterm><indexterm xml:id="indexterm-514"><primary>weryfikacja</primary><secondary>(przykład)</secondary></indexterm><indexterm xml:id="indexterm-515"><primary>dane</primary><secondary>weryfikacja</secondary><tertiary>(przykład)</tertiary></indexterm>
</para>
<screen># <userinput>zpool scrub tank</userinput>
</screen>
<para>Status aktualnej jawnej kontroli może zostać wyświetlony przy użyciu komendy <command>zpool status</command>. Na przykład:</para>
<screen># <userinput>zpool status -v tank</userinput>
  pool: tank
 state: ONLINE
 scrub: scrub completed with 0 errors on Tue Mar  7 15:27:36 2006
config:

        NAME         STATE     READ WRITE CKSUM
        tank         ONLINE       0     0     0
          mirror     ONLINE       0     0     0
            c1t0d0   ONLINE       0     0     0
            c1t1d0   ONLINE       0     0     0

errors: No known data errors</screen>
<para>Należy zwrócić uwagę, że jednocześnie możliwa jest tylko jedna operacja weryfikacji danych dla każdej z puli.</para>
<para>Wykonywanie regularnej weryfikacji gwarantuje ciągłość we/wy dla wszystkich dysków w systemie. Regularna weryfikacja danych posiada uboczny efekt, powodujący uniemożliwienie menadżerowi zasilania postawienia niepracujących dysków w stanie niskiego poboru energii. Jeśli system w większości przypadków korzysta ciągle z operacji we/wy lub zużycie energii nie jest brane pod uwagę, wtedy ten problem może zostać bezpiecznie zignorowany.</para>
<para>Więcej informacji na temat interpretowania wyniku komendy <command>zpool status</command> można znaleźć w <olink targetdoc="" remap="internal" targetptr="gaynp">Status puli ZFS</olink></para>
</sect3>
<sect3 xml:id="gbbya">
<title>Jawna kontrola danych ZFS i Resilvering</title>
<para>Kiedy urządzenie zostaje wymienione, operacja resilweringu jest inicjowana. Zdrowe dane są kopiowane na nowe urządzenie. Ta akcja jest formą weryfikacji danych dysku. Dlatego jednocześnie tylko jedna taka akcja może się odbywać w obrębie jednej puli. Jeśli operacja weryfikacji danych jest w toku, resilwering usypia weryfikacje i ją restartuje w momencie kiedy zostanie zakończony.<indexterm xml:id="indexterm-516"><primary>Pule pamięci ZFS</primary><secondary>weryfikacja danych i resilwering</secondary><tertiary>opis</tertiary></indexterm><indexterm xml:id="indexterm-517"><primary>resilwering i weryfikacja danych</primary><secondary>opis</secondary></indexterm><indexterm xml:id="indexterm-518"><primary>dane</primary><secondary>resilwering</secondary><tertiary>opis</tertiary></indexterm>
</para>
<para>Więcej informacji na temat resilweringu można znaleźć w <olink targetdoc="" remap="internal" targetptr="gbcus">Podgląd statusu resilweringu</olink></para>
</sect3>
</sect2>
</sect1>
<sect1 xml:id="gbbuw">
<title>Identyfikowanie problemów w ZFS-ie</title>
<para>Wykrywanie wszystkich usterek jest skupione wokół komendy <command>zpool status</command> Ta komenda analizuje różne przypadki błędów w systemie i identyfikuje najcięższe. Jako rezultat sugerowana akcja wraz z likami do odpowiednich artykułów zostaje zaprezentowana. Proszę zwrócić uwagę, że komenda ta identyfikuje pojedyńczy problem w puli, podczas gdy wiele problemów może istnieć. Na przykład, błędy uszkodzenia danych zawsze pociągają za sobą uszkodzenie jednego z urządzeń. Wymiana uszkodzonego urządzenia nie naprawia problemu uszkodzenia danych.</para>
<para>Dodatkowo, silnik diagnostyki ZFS zgłasza błędy puli oraz dysku. Błędy suma kontrolnej, we/wy, oraz puli związane z błędami urządzenia lub puli też są zgłaszane. Błędy ZFS zgłaszane przez <command>fmd</command> są wyświetlane zarówno na konsoli jak i w pliku wiadomości systemowych. W wielu przypadkach komunikat komendy <command>fmd</command> kieruje do instrukcji naprawy dla polecenia <command>zpool status</command>.<indexterm xml:id="indexterm-519"><primary>Pule pamięci ZFS</primary><secondary>identyfikowanie problemów</secondary><tertiary>opis</tertiary></indexterm><indexterm xml:id="indexterm-520"><primary>usuwanie problemów</primary><secondary>identyfikowanie problemów</secondary></indexterm>
</para>
<para>Podstawowy proces odtworzenia wygląda następująco:</para>
<itemizedlist>
<listitem>
<para>Indentyfikowanie błędów poprzez komunikaty komendy <command>fmd</command>, które są wyświetlane na konsoli lub w plikach <filename>/var/adm/messages</filename>.</para>
</listitem>
<listitem>
<para>Znajdz przyszłe instrukcje naprawy przy użyciu komendy <command>zpool status -x</command>.</para>
</listitem>
<listitem>
<para>Napraw następujące błędy:</para>
<itemizedlist>
<listitem>
<para>Wymień brakujące lub uszkodzone urządzenie i podnieś je do stanu online.</para>
</listitem>
<listitem>
<para>Odzyskaj brakujące dane lub błędną konfiguracje z kopi zapasowej.</para>
</listitem>
<listitem>
<para>Potwierdź odzyskanie danych używając komende <command>zpool status</command> <command>x</command>.</para>
</listitem>
<listitem>
<para>Stwórz kopie zapasową odzyskanej konfiguracji.</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para>Ten rozdział opisuje w jaki sposób interpretować komunikaty komendy <command>zpool status</command> w celu zdiagnozowania typu błędu i kieruje do odpowiedniej sekcji w celu naprawy problemu. Podczas gdy większość zadań jest wykonwyana automatycznie przez komende, jest ważne aby zrozumieć które błędy zostały zidentyfikowane aby zdiagnozować typ awarii.</para>
<sect2 xml:id="gbcwb">
<title>Sprawdzanie, czy problem istnieje w puli ZFS.</title>
<para>Najprostrzym sposobem na sprawdzenie, czy istnieje jakikolwiek znany błąd w systemie, jest użycie komendy <command>zpool status</command> <option>x</option>. Ta komenda opisuje wyłącznie problemy związane z pulami. Jeśli w systemie nie istnieją uszkodzone pule, komenda wyświetla prosą wiadomość:</para>
<screen># <userinput>zpool status -x</userinput>
all pools are healthy</screen>
<para>Bez flagi <option>x</option>, komenda wyświetla kompletny status dla wszystkich pul (lub dla określonej puli, jeśli została sprecyzowana w lini poleceń), nawet jeśli pule są zdrowe.<indexterm xml:id="indexterm-521"> <primary>Pule pamięci ZFS</primary> <secondary>ustalanie czy istnieją problemy (<command>zpool status</command> <option> x</option>)</secondary> <tertiary>opis</tertiary></indexterm> <indexterm xml:id="indexterm-522"> <primary>usuwanie problemów</primary> <secondary>ustalanie czy istnieją problemy  (<command>zpool status</command> <option> x</option>)</secondary></indexterm> 
</para>
<para>Więcej informacji na temat argumentów komendy <command>zpool status</command> można znaleźć w <olink targetdoc="" remap="internal" targetptr="gaynp">Zapytania Statusu Puli Pamęci ZFS</olink></para>
</sect2>
<sect2 xml:id="gbcve">
<title>Zrozumienie wyniku działania komendy <command>zpool status</command></title>
<para>Kompletny wynik komendy <command>zpool status</command> jest podobny do następującego:</para>
<screen># <userinput>zpool status tank</userinput>
  pool: tank
 state: DEGRADED
status: One or more devices has been taken offline by the administrator.
        Sufficient replicas exist for the pool to continue functioning in a
        degraded state.
action: Online the device using 'zpool online' or replace the device with
        'zpool replace'.
 scrub: none requested
 config:

        NAME         STATE     READ WRITE CKSUM
        tank         DEGRADED     0     0     0
          mirror     DEGRADED     0     0     0
            c1t0d0   ONLINE       0     0     0
            
errors: No known data errors</screen>
<para>Wynik jest podzielony na kilka sekcji:</para>
<sect3 xml:id="gbcvl">
<title>Ogólna Informacja Statusu Puli</title>
<para>Nagłówek sekcji komunikatu wyjściowego komendy <command>zpool status</command> zawiera następujące pola (niektóre z pól są wyświetlane wyłącznie dla pul zawierających błędy): <indexterm xml:id="indexterm-523"> <primary>Pule pamięci ZFS</primary> <secondary>ogólna informacja statusu puli dla rozwiązywania problemów </secondary> <tertiary>opis</tertiary></indexterm> <indexterm xml:id="indexterm-524"> <primary>usuwanie problemów</primary> <secondary>ogólna informacja statusu puli </secondary> <tertiary>opis</tertiary> </indexterm>
</para>
<variablelist>
<varlistentry>
<term>
<literal>pool</literal>
</term>
<listitem>
<para>Nazwa puli.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>state</literal>
</term>
<listitem>
<para>Aktualny stan zdrowia puli. Ta informacja odnosi się tylko do zdolności puli do zapewnienia odpowiedniego poziomu replikacji. Pule, które są w stanie <literal>ONLINE</literal>mogą ciągle posiadać urządzenia z wadami lub błędy danych.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>status</literal>
</term>
<listitem>
<para>Opis co jest złego z pulą. To pole jest pominięte, jeśli nie zostały znalezione żadne problemy.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>action</literal>
</term>
<listitem>
<para>Rekomendowane czynności związane z naprawieniem błędów. To pole jest skrócone dla użytkownika do odpowiednich odnośników. To pole jest pominięte, jeśli nie zostały znalezione żadne problemy.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>see</literal>
</term>
<listitem>
<para>Odnośnik do artykułów zawierających szczegółowe informacje na temat naprawy. Artykuły online są aktualizowane częściej niż ten dokument, dlatego znajdują się w nich najbardziej aktualne procedury naprawy. To pole jest pominięte, jeśli nie zostały znalezione żadne problemy.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>scrub</literal>
</term>
<listitem>
<para>Identyfikuje aktualny status operacji weryfikacji danych, która może zawierać datę oraz czas ostatniej operacji, postęp weryfikacji lub informację, że weryfikacja nie była wykonana.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>errors</literal>
</term>
<listitem>
<para>Indentyfikuje znane błędy danych lub ich brak.</para>
</listitem>
</varlistentry>
</variablelist>
</sect3>
<sect3 xml:id="gbcvv">
<title>Informacja o konfiguracji</title>
<para>Pole <literal>config</literal> w komunikacie komendy <command>zpool status</command> opisuje rozmieszczenie urządzeń zawartych w puli, jak również ich status i informacje o jakichkolwiek błędach generowanych przez urządzenia. Stan może być jednym z następujących:  <literal>ONLINE</literal>,  <literal>FAULTED</literal>, <literal>DEGRADED</literal>, <literal>UNAVAILABLE</literal>, lub <literal>OFFLINE</literal>. Jeśli status jest <literal>ONLINE</literal>, tolerancja uszkodzeń puli pozwala na jej dalsze działanie.</para>
<para>Drugia sekcja komunikatu konfiguracyjnego wyświetla statystykę błędów. Te błędy są podzielone na trzy kategorie:</para>
<itemizedlist>
<listitem>
<para>
<literal>READ</literal> – nastąpił błąd we/wy podczas operacji odczytu.</para>
</listitem>
<listitem>
<para>
<literal>WRITE</literal> – błąd we/wy podczas operacji zapisu. </para>
</listitem>
<listitem>
<para>
<literal>CKSUM</literal> – błąd sumy kontrolnej.  Urządzenie zwróciło uszkodzone dane jako rezultat operacji odczytu.</para>
</listitem>
</itemizedlist>
<para>Te błędy pozwalają określić czy uszkodzenie jest trwałe. Mała ilość błędów operacji we/wy może oznaczać tymczasową awarię, podczas gdy ich duża ilość może oznaczać stały problem z urządzeniem. Te błędy niekoniecznie odnoszą się  do błędów danych zgłaszanych przez aplikacje. Jeżeli urządzenie znajduje się w zbytecznej konfiguracji, dyski mogą pokazywać nienaprawialne błędy, jednak żadne błędy nie pojawiają się w mirrorze lub na poziomie macierzy RAID-Z. W przypadku takiego scenariusza, ZFS otrzymuje poprawne dane i próbuje naprawić uszkodzone z istniejących replik.</para>
<para>Więcej informacji na temat interpretowania błędów w celu ustalenia przyczyny uszkodzenia urządzenia, można znaleźć w  <olink targetdoc="" remap="internal" targetptr="gbbzs">Określanie Typu Uszkodzenia Urządzenia</olink>.</para>
<para>Finalnie, dodatkowa pomocnicza informacja jest wyświetlona w ostatniej kolumnie komunikatu  pochodzącego z komendy <command>zpool status</command>. Te informacje są rozwinięte w polu  <literal>state</literal>, pomagając diagnozować rodzaje błędów. Jeśli urządzenie jest w stanie <literal>FAULTED</literal>, to pole wskazuje niedostępność urządzenia lub uszkodzenie danych na nim. Jeśli urządzenie wykonuje operację resilweringu, to pole pokazuje aktualny postęp operacji.</para>
<para>Więcej informacji na temat monitoringu procesu resilweringu można znaleźć w  <olink targetdoc="" remap="internal" targetptr="gbcus">Podgląd Statusu Operacji Resilweringu</olink></para>
</sect3>
<sect3 xml:id="gbcvd">
<title>Status Weryfikacji Danych</title>
<para>Trzecia sekcja komunikatu komendy  <command>zpool status</command>, opisuje aktualny stan jakiejkolwiej operacji weryfikacji. Ta informacja jest odrębna od jakichkolwiek błędów znalezionych w systemie, aczkolwiek mozę być użyta do określenia błędności raportu uszkodzenia danych. Jeśli operacja weryfikacji danych została uokńczona niedawno, w większości przypadków wszystkie znane typy błędów danych zostały odkryte.</para>
<para>Więcej informacji na temat weryfikacji danych oraz interpretacji informacji można znaleźć w <olink targetdoc="" remap="internal" targetptr="gbbwa">Sprawdzanie Integralnośći Danych ZFS </olink>.</para>
</sect3>
<sect3 xml:id="gbcwe">
<title>Błędy uszkodzenia danych</title>
<para>Polecenie <command>zpool status</command> równierz pokazuje, czy jakiekolwiek znane błędy są powiązane z pulą. Te błędy mogą zostać odnalezione podczas operacji weryfikacji danych lub normalnego użytkowania. ZFS utrzymuje trwały log wszystkich znanych błędów danych związanych z pulą. Ten log jest odnawiany, kiedy kompletna operacja weryfikacji danych zostaje ukończona.</para>
<para>Błędy uszkodzenia danych są zawsze krytyczne. Ich istnienie wskazują, że conajmniej jedna aplikacja doświadczyła błędy we/wy związane z uszkodzeniem danych w puli. Błędy urządzeń w replikowanej puli nie powodują błędów danych i nie są zgłaszane w tym logu. Domyślnie, tylko znalezione błędy są wyświetlane. Kompletna lista błędów, wraz z ich specyfikacją, może zostać znaleciona używając komendy <command>zpool status</command> <option> v</option>. Na przykład: <indexterm xml:id="indexterm-525"> <primary>Pule pamięci ZFS</primary> <secondary>zidentyfikowane błędy danych(<command>zpool status</command> <option> v</option>)</secondary> <tertiary>(przykład)</tertiary> </indexterm> <indexterm xml:id="indexterm-526"> <primary>dane</primary> <secondary>identyfikowanie uszkodzenia(<command>zpool status</command><option>v</option>)</secondary> <tertiary>(przykład)</tertiary> </indexterm> <indexterm xml:id="indexterm-527"> <primary>usuwanie problemów</primary> <secondary>zidentyfikowane błędy danych  (<command>zpool status</command> <option> v</option>)</secondary> <tertiary>(przykład)</tertiary> </indexterm>
</para>
<screen># <userinput>zpool status -v</userinput>
  pool: tank
 state: DEGRADED
status: One or more devices has experienced an error resulting in data
        corruption.  Applications may be affected.
action: Restore the file in question if possible.  Otherwise restore the
        entire pool from backup.
   see: http://www.sun.com/msg/ZFS-8000-8A
 scrub: resilver completed with 1 errors on Fri Mar 17 15:42:18 2006
config:

        NAME         STATE     READ WRITE CKSUM
        tank         DEGRADED     0     0     1
          mirror     DEGRADED     0     0     1
            c1t0d0   ONLINE       0     0     2
            c1t1d0   UNAVAIL      0     0     0  corrupted data

errors: The following persistent errors have been detected:

          DATASET  OBJECT  RANGE
          5        0       lvl=4294967295 blkid=0</screen>
<para>Podobny komunikat jest wyświetlony przy użyciu komendy  <command>fmd</command> w konsoli systemowej oraz w pliku  <filename>/var/adm/messages</filename>. Ten komunikat może zostać również śledzony używając komendy <command>fmdump</command>.</para>
<para>Więcej informacji na temat interpretowania błędów danych można znaleźć w  <olink targetdoc="" remap="internal" targetptr="gbcuz">Identyfikowanie Typów Błędów Danych</olink>.</para>
</sect3>
</sect2>
<sect2 xml:id="gbcvk">
<title>Raport Systemowy Błędów ZFS</title>
<para>Jako uzupełnienie śledzenia błędów w puli, ZFS również zgłasza interesujące informacje w logu systemowym (syslog). Następujący scenariusze generują wiadomości, aby poinformować administratora: <indexterm xml:id="indexterm-528"> <primary>Pule pamięci ZFS</primary> <secondary>wiadomości błędów systemowych</secondary> <tertiary>opis</tertiary> </indexterm> <indexterm xml:id="indexterm-529"> <primary>wyświetlanie</primary> <secondary>log systemowy reportujący błędy ZFS</secondary> <tertiary>opis</tertiary> </indexterm> <indexterm xml:id="indexterm-530"> <primary>usuwanie problemów</primary> <secondary>log systemowy reportujący błędy ZFS</secondary> </indexterm>
</para>
<itemizedlist>
<listitem>
<para>
<emphasis role="strong">Device state transition</emphasis> – Jeśli urządzenie zostanie oznaczone stanem <literal>FAULTED</literal>, ZFS loguje wiadomość, przy czym tolerancja uszkodzenia całej puli może zostać wzięta pod uwagę.  Podobna wiadomość zostaje wysłana, jeżeli urządzenie jest później podniesione do stanu online, przywracając pulę do zdrowia.</para>
</listitem>
<listitem>
<para>
<emphasis role="strong">Data corruption</emphasis> –  Jeśli jakiekolwiek uszkodzenie danych jest wykryte, ZFS loguje wiadomość opisującą kiedy i gdzie uszkodzenie zostało odnalezione. Wiadomość jest logowana tylko przy pierwszym odnalezieniu błędu. Kolejny dostęp do uszkodzonych danych nie generuje wiadomości.</para>
</listitem>
<listitem>
<para>
<emphasis role="strong">Pool failures and device failures</emphasis> – Jeśli następuje błąd puli lub urządzenia, demon menadżera błędów zgłasza je poprzez log systemowy jak również poprzez polecenie  <command>fmdump</command>.</para>
</listitem>
</itemizedlist>
<para>Jeśli ZFS wykryje błąd urządzenia i automatycznie je naprawi, żadnen komunikat nie zostanie zgłoszony. Tego rodzaju błędy nie stanowią awarii w nadmiarowej puli lub integralności danych. Co więcej, tego typu błędy są zwykle rezultatem problemu sterownika, który generuje własne komunikaty.</para>
</sect2>
</sect1>
<sect1 xml:id="gbbve">
<title>Naprawa uszkodzonej konfiguracji ZFS</title>
<para>ZFS maintains a cache of active pools and their configuration on the root file system. If this file is corrupted or somehow becomes out of sync with what is stored on disk, the pool can no longer be opened. ZFS tries to avoid this situation, though arbitrary corruption is always possible given the qualities of the underlying file system and storage. Ta sytuacja zwykle powoduje zniknięcie puli z systemu, kiedy w przeciwnym przypadku powinna być dostępna. This situation can also manifest itself as a partial configuration that is missing an unknown number of top-level virtual devices. In either case, the configuration can be recovered by exporting the pool (if it is visible at all), and re-importing it.<indexterm xml:id="indexterm-531"> <primary>Pule pamięci ZFS</primary> <secondary>repairing a damaged ZFS configuration</secondary> </indexterm> <indexterm xml:id="indexterm-532"> <primary>repairing</primary> <secondary>a damaged ZFS configuration</secondary> <tertiary>opis</tertiary> </indexterm> <indexterm xml:id="indexterm-533"> <primary>usuwanie problemów</primary> <secondary>repairing a damaged ZFS configuration</secondary> </indexterm>
</para>
<para>For more information about importing and exporting pools, see <olink targetdoc="" remap="internal" targetptr="gbchy">Migrating ZFS Storage Pools/Pule pamięci ZFS</olink>.</para>
</sect1>
<sect1 xml:id="gbbvb">
<title>Naprawa brakującego urządzenia</title>
<para>If a device cannot be opened, it displays as <literal>UNAVAILABLE</literal> in
the <command>zpool status</command> output. This status means that ZFS was
unable to open the device when the pool was first accessed, or the device
has since become unavailable. If the device causes a top-level virtual device
to be unavailable, then nothing in the pool can be accessed. Otherwise, the
fault tolerance of the pool might be compromised. In either case, the device
simply needs to be reattached to the system to restore normal operation.<indexterm xml:id="indexterm-534">
<primary>Pule pamięci ZFS</primary>
<secondary>replacing a missing device</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-535">
<primary>replacing</primary>
<secondary>a missing device</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-536">
<primary>usuwanie problemów</primary>
<secondary>replacing a missing device</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<para>Na przykład, po awarii urządzenia, możesz zobaczyć podobny komunikat pochodzący z <command>fmd</command>:</para>
<screen>SUNW-MSG-ID: ZFS-8000-D3, TYPE: Fault, VER: 1, SEVERITY: Major
EVENT-TIME: Fri Mar 17 14:38:47 MST 2006
PLATFORM: SUNW,Ultra-60, CSN: -, HOSTNAME: neo
SOURCE: zfs-diagnosis, REV: 1.0
EVENT-ID: 043bb0dd-f0a5-4b8f-a52d-8809e2ce2e0a
DESC: A ZFS device failed.  Refer to http://sun.com/msg/ZFS-8000-D3 for more information.
AUTO-RESPONSE: No automated response will occur.
IMPACT: Fault tolerance of the pool may be compromised.
RE-ACTION: Run 'zpool status -x' and replace the bad device.</screen>
<para>Następnym krokiem jest uzycie komendy <command>zpool status</command> <option>x</option> w celu zobaczenia dokładniejszej informacji na temat problemu urządzenia i podjęcia decyzji. Na przykład:</para>
<screen># <userinput>zpool status -x</userinput>
  pool: tank
 state: DEGRADED
status: One or more devices could not be opened.  Sufficient replicas exist for
        the pool continue functioning in a degraded state.
action: Attach the missing device and online it using 'zpool online'.
   see: http://www.sun.com/msg/ZFS-8000-D3
 scrub: resilver completed with 0 errors on Fri Mar 10 11:08:29 2006 
config:

        NAME         STATE     READ WRITE CKSUM
        tank         DEGRADED     0     0     0
          mirror     DEGRADED     0     0     0
            c0t1d0   UNAVAIL      0     0     0  cannot open
            c1t1d0      ONLINE        0        0        0</screen>
<para>You can see from this output that the missing device <literal>c0t1d0</literal> is not functioning. Jesli zostanie stwierdzone, ze urządzenie jest wadliwe, należy je wymienić.</para>
<para>Then, use the <command>zpool online</command> command to online the replaced device. Na przykład:</para>
<screen># <userinput>zpool online tank c0t1d0</userinput>
</screen>
<para>Confirm that the pool with the replaced device is healthy.</para>
<screen># <userinput>zpool status -x tank</userinput>
pool 'tank' is healthy</screen>
<sect2 xml:id="gbbxn">
<title>Physically Reattaching the Device</title>
<para>Exactly how a missing device is reattached depends on the device in
question. If the device is a network-attached drive, connectivity should be
restored. If the device is a USB or other removable media, it should be reattached
to the system. If the device is a local disk, a controller might have failed
such that the device is no longer visible to the system. In this case, the
controller should be replaced at which point the disks will again be available.
Other pathologies can exist and depend on the type of hardware and its configuration.
If a drive fails and it is no longer visible to the system (an unlikely event),
the device should be treated as a damaged device. Follow the procedures outlined
in <olink targetdoc="" remap="internal" targetptr="gbbvf">Naprawa uszkodzonego urządzenia</olink>.</para>
</sect2>
<sect2 xml:id="gbbyi">
<title>Notifying ZFS of Device Availability</title>
<para>Once a device is reattached to the system, ZFS might or might not automatically
detect its availability. If the pool was previously faulted, or the system
was rebooted as part of the attach procedure, then ZFS automatically rescans
all devices when it tries to open the pool. If the pool was degraded and the
device was replaced while the system was up, you must notify ZFS that the
device is now available and ready to be reopened by using the <command>zpool
online</command> command. For example:<indexterm xml:id="indexterm-537">
<primary>Pule pamięci ZFS</primary>
<secondary>notifying ZFS of reattached device (<command>zpool online</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-538">
<primary>notifying</primary>
<secondary>ZFS of reattached device (<command>zpool online</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-539">
<primary>usuwanie problemów</primary>
<secondary>notifying ZFS of reattached device (<command>zpool online</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool online tank c0t1d0</userinput>
</screen>
<para>For more information about bringing devices online, see <olink targetdoc="" remap="internal" targetptr="gazgk">Bringing a Device Online</olink>.</para>
</sect2>
</sect1>
<sect1 xml:id="gbbvf">
<title>Naprawa Uszodzonego Urządzenia</title>
<para>This section describes how to determine device failure types, clear
transient errors, and replace a device.</para>
<sect2 xml:id="gbbzs">
<title>Determining the Type of Device Failure</title>
<para>The term <emphasis>damaged device</emphasis> is rather vague, and can
describe a number of possible situations:<indexterm xml:id="indexterm-540">
<primary>Pule pamięci ZFS</primary>
<secondary>determining type of device failure</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-541">
<primary>determining</primary>
<secondary>type of device failure</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-542">
<primary>usuwanie problemów</primary>
<secondary>determining type of device failure</secondary>
<tertiary>opis</tertiary>
</indexterm>
</para>
<itemizedlist>
<listitem>
<para>
<emphasis role="strong">Bit rot</emphasis> – Over time, random events, such as magnetic influences and cosmic rays, can cause bits stored on disk to flip in unpredictable events. These events are relatively rare but common enough to cause potential data corruption in large or long-running systems. [fuzzy] Te błędy są podzielone na trzy kategorie:</para>
</listitem>
<listitem>
<para>
<emphasis role="strong">Misdirected reads or writes</emphasis> –
Firmware bugs or hardware faults can cause reads or writes of entire blocks
to reference the incorrect location on disk. These errors are typically transient,
though a large number might indicate a faulty drive.</para>
</listitem>
<listitem>
<para>
<emphasis role="strong">Administrator error</emphasis> – Administrators can unknowingly overwrite portions of the disk with bad data (such as copying <filename>/dev/zero</filename> over portions of the disk) that cause permanent corruption on disk. [fuzzy] [fuzzy] Te błędy są podzielone na trzy kategorie:</para>
</listitem>
<listitem>
<para>
<emphasis role="strong">Temporary outage</emphasis>–
A disk might become unavailable for a period time, causing I/Os to fail. This
situation is typically associated with network-attached devices, though local
disks can experience temporary outages as well. These errors might or might
not be transient.</para>
</listitem>
<listitem>
<para>
<emphasis role="strong">Bad or flaky hardware</emphasis> – This situation is a catch-all for the various problems that bad hardware exhibits. This could be consistent I/O errors, faulty transports causing random corruption, or any number of failures. Te błędy są zwykle trwałe.</para>
</listitem>
<listitem>
<para>
<emphasis role="strong">Offlined device</emphasis> –
If a device is offline, it is assumed that the administrator placed the device
in this state because it is presumed faulty. The administrator who placed
the device in this state can determine is this assumption is accurate.</para>
</listitem>
</itemizedlist>
<para>Determining exactly what is wrong can be a difficult process. The first
step is to examine the error counts in the <command>zpool status</command> output
as follows:</para>
<screen># <userinput>zpool status -v</userinput> <replaceable>pool</replaceable>
</screen>
<para>The errors are divided into I/O errors and checksum errors, both of which might indicate the possible failure type. Typical operation predicts a very small number of errors (just a few over long periods of time). If you are seeing large numbers of errors, then this situation probably indicates impending or complete device failure. However, the pathology for administrator error can result in large error counts. Innym źródłem informacji jest log systemowy. If the log shows a large number of SCSI or fibre channel driver messages, then this situation probably indicates serious hardware problems. If no syslog messages are generated, then the damage is likely transient.</para>
<para>The goal is to answer the following question:</para>
<para>
<emphasis>Is another error likely to occur on this device?</emphasis>
</para>
<para>Errors that happen only once are considered <emphasis>transient</emphasis>,
and do not indicate potential failure. Errors that are persistent or severe
enough to indicate potential hardware failure are considered “fatal.”
The act of determining the type of error is beyond the scope of any automated
software currently available with ZFS, and so much must be done manually by
you, the administrator. Once the determination is made, the appropriate action
can be taken. Either clear the transient errors or replace the device due
to fatal errors. These repair procedures are described in the next sections.</para>
<para>Even if the device errors are considered transient, it still may have
caused uncorrectable data errors within the pool. These errors require special
repair procedures, even if the underlying device is deemed healthy or otherwise
repaired. For more information on repairing data errors, see <olink targetdoc="" remap="internal" targetptr="gbbwl">Repairing Damaged Data</olink>.</para>
</sect2>
<sect2 xml:id="gbbzv">
<title>Clearing Transient Errors</title>
<para>If the device errors are deemed transient, in that they are unlikely
to effect the future health of the device, then the device errors can be safely
cleared to indicate that no fatal error occurred. To clear error counters
for RAID-Z or mirrored devices, use the <command>zpool clear</command> command.
For example:<indexterm xml:id="indexterm-543">
<primary>Pule pamięci ZFS</primary>
<secondary>clearing device errors (<command>zpool clear</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-544">
<primary>clearing</primary>
<secondary>device errors (<command>zpool clear</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-545">
<primary>usuwanie problemów</primary>
<secondary>clear device errors (<command>zpool clear</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool clear tank c1t0d0</userinput>
</screen>
<para>This syntax clears any errors associated with the device and clears
any data error counts associated with the device.</para>
<para>To clear all errors associated with the virtual devices in the pool,
and clear any data error counts associated with the pool, use the following
syntax:</para>
<screen># <userinput>zpool clear tank</userinput>
</screen>
<para>For more information about clearing pool errors, see <olink targetdoc="" remap="internal" targetptr="gazge">Clearing Storage Pool Devices</olink>.</para>
</sect2>
<sect2 xml:id="gbbzy">
<title>Replacing a Device in a ZFS Storage Pool</title>
<para>If device damage is permanent or future permanent damage is likely,
the device must be replaced. Whether the device can be replaced depends on
the configuration.</para>
<sect3 xml:id="gbcfb">
<title>Determining if a Device Can Be Replaced</title>
<para>For a device to be replaced, the pool must be in the <literal>ONLINE</literal> state.
The device must be part of a replicated configuration, or it must be healthy
(in the <literal>ONLINE</literal> state). If the disk is part of a replicated
configuration, sufficient replicas from which to retrieve good data must exist.
If two disks in a four-way mirror are faulted, then either disk can be replaced
because healthy replicas are available. However, if two disks in a four-way
RAID-Z device are faulted, then neither disk can be replaced because not enough
replicas from which to retrieve data exist. If the device is damaged but otherwise
online, it can be replaced as long as the pool is not in the <literal>FAULTED</literal> state.
However, any bad data on the device is copied to the new device unless there
are sufficient replicas with good data.<indexterm xml:id="indexterm-546">
<primary>Pule pamięci ZFS</primary>
<secondary>determining if a device can be replaced</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-547">
<primary>determining</primary>
<secondary>if a device can be replaced</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-548">
<primary>usuwanie problemów</primary>
<secondary>determining if a device can be replaced</secondary>
<tertiary>opis</tertiary>
</indexterm>
</para>
<para>In the following configuration, the disk <literal>c1t1d0</literal> can
be replaced, and any data in the pool is copied from the good replica, <literal>c1t0d0</literal>.</para>
<screen>[fuzzy] mirror      DEGRADED        0        0        0
    [fuzzy] c1t0d0      ONLINE        0        0        0
    [fuzzy] c1t1d0      ONLINE        0        0        0</screen>
<para>The disk <literal>c1t0d0</literal> can also be replaced, though no self-healing
of data can take place because no good replica is available.</para>
<para>In the following configuration, neither of the faulted disks can be
replaced. The <literal>ONLINE</literal> disks cannot be replaced either, because
the pool itself is faulted.</para>
<screen>[fuzzy] [fuzzy] c1t1d0      ONLINE        0        0        0
    [fuzzy] c1t0d0      ONLINE        0        0        0
    [fuzzy] [fuzzy] c1t1d0      ONLINE        0        0        0
    [fuzzy] [fuzzy] c1t1d0      ONLINE        0        0        0
    [fuzzy] [fuzzy] c1t0d0      ONLINE        0        0        0</screen>
<para>In the following configuration, either top-level disk can be replaced,
though any bad data present on the disk is copied to the new disk.</para>
<screen>[fuzzy] [fuzzy] c1t0d0      ONLINE        0        0        0</screen>
<para>If either disk were faulted, then no replacement could be performed
because the pool itself would be faulted.</para>
</sect3>
<sect3 xml:id="gbcdv">
<title>Unreplaceable Devices</title>
<para>If the loss of a device causes the pool to become faulted, or the device
contains too many data errors in an unreplicated configuration, then the device
cannot safely be replaced. Without sufficient replicas, no good data with
which to heal the damaged device exists. In this case, the only option is
to destroy the pool and re-create the configuration, restoring your data in
the process.</para>
<para>For more information about restoring an entire pool, see <olink targetdoc="" remap="internal" targetptr="gbctt">Repairing ZFS Storage Pool-Wide Damage</olink>.</para>
</sect3>
<sect3 xml:id="gbcet">
<title>[fuzzy] Naprawa Uszodzonego Urządzenia</title>
<para>Once you have determined that a device can be replaced, use the <command>zpool
replace</command> command to replace the device. If you are replacing the
damaged device with another different device, use the following command:<indexterm xml:id="indexterm-549">
<primary>Pule pamięci ZFS</primary>
<secondary>replacing a device (<command>zpool replace</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-550">
<primary>replacing</primary>
<secondary>a device (<command>zpool replace</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-551">
<primary>usuwanie problemów</primary>
<secondary>replacing a device (<command>zpool replace</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool replace tank c1t0d0 c2t0d0</userinput>
</screen>
<para>This command begins migrating data to the new device from the damaged device, or other devices in the pool if it is in a replicated configuration. When the command is finished, it detaches the damaged device from the configuration, at which point the device can be removed from the system. If you have already removed the device and replaced it with a new device in the same location, use the single device form of the command. Na przykład:</para>
<screen># <userinput>zpool replace tank c1t0d0</userinput>
</screen>
<para>This command takes an unformatted disk, formats it appropriately, and
then begins resilvering data from the rest of the configuration.</para>
<para>For more information about the <command>zpool replace</command> command,
see <olink targetdoc="" remap="internal" targetptr="gazgd">Replacing Devices in a Storage Pool</olink>.</para>
</sect3>
<sect3 xml:id="gbcus">
<title>Viewing Resilvering Status</title>
<para>The process of replacing a drive can take an extended period of time,
depending on the size of the drive and the amount of data in the pool. The
process of moving data from one device to another device is known as <emphasis>resilvering</emphasis>, and can be monitored by using the <command>zpool status</command> command.</para>
<para>Traditional file systems resilver data at the block level. Because ZFS
eliminates the artificial layering of the volume manager, it can perform resilvering
in a much more powerful and controlled manner. The two main advantages of
this feature are as follows:</para>
<itemizedlist>
<listitem>
<para>ZFS only resilvers the minimum amount of necessary data. In
the case of a short outage (as opposed to a complete device replacement),
the entire disk can be resilvered in a matter of minutes or seconds, rather
than resilvering the entire disk, or complicating matters with “dirty
region” logging that some volume managers support. When an entire disk
is replaced, the resilvering process takes time proportional to the amount
of data used on disk. Replacing a 500-Gbyte disk can take seconds if only
a few gigabytes of used space is in the pool.<indexterm xml:id="indexterm-552">
<primary>Pule pamięci ZFS</primary>
<secondary>viewing resilvering process</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-553">
<primary>replacing</primary>
<secondary>a device (<command>zpool replace</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-554">
<primary>usuwanie problemów</primary>
<secondary>replacing a device (<command>zpool replace</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
</listitem>
<listitem>
<para>Resilvering is interruptible and safe. If the system loses
power or is rebooted, the resilvering process resumes exactly where it left
off, without any need for manual intervention.</para>
</listitem>
</itemizedlist>
<para>Aby zobaczyć status jawnej kontroli danych, użyj komendy <command>zpool status</command> Na przykład:</para>
<screen># <userinput>zpool status tank</userinput>
  pool: tank
 state: DEGRADED
status: One or more devices is being resilvered.
action: Wait for the resilvering process to complete.
   see: http://www.sun.com/msg/ZFS-XXXX-08
 scrub: none requestes
config:
        NAME        STATE    READ  WRITE  CKSUM 
        tank         DEGRADED        0        0        0
          mirror      DEGRADED        0        0        0
            replacing         DEGRADED     0     0     0  52% resilvered
              c1t0d0      ONLINE        0        0        0
              c2t0d0      ONLINE        0        0        0  
            c1t1d0      ONLINE        0        0        0</screen>
<para>In this example, the disk <literal>c1t0d0</literal> is being replaced
by <literal>c2t0d0</literal>. This event is observed in the status output
by presence of the <emphasis>replacing</emphasis> virtual device in the configuration.
This device is not real, nor is it possible for you to create a pool by using
this virtual device type. The purpose of this device is solely to display
the resilvering process, and to identify exactly which device is being replaced. </para>
<para>Note that any pool currently undergoing resilvering is placed in the <literal>DEGRADED</literal> state, because the pool cannot provide the desired replication level until the resilvering process is complete. Resilvering proceeds as fast as possible, though the I/O is always scheduled with a lower priority than user-requested I/O, to minimize impact on the system. Once the resilvering is complete, the configuration reverts to the new, complete, configuration. Na przykład:</para>
<screen># <userinput>zpool status tank</userinput>
  pool: tank
 state: ONLINE
 scrub: scrub completed with 0 errors on Tue Mar  7 15:27:36 2006
config:

        NAME         STATE     READ WRITE CKSUM
        tank         ONLINE       0     0     0
          mirror     ONLINE       0     0     0
            [fuzzy] c2t0d0      ONLINE        0        0        0
            c1t1d0   ONLINE       0     0     0

errors: No known data errors</screen>
<para>The pool is once again <literal>ONLINE</literal>, and the original bad
disk (<literal>c1t0d0</literal>) has been removed from the configuration.</para>
</sect3>
</sect2>
</sect1>
<sect1 xml:id="gbbwl">
<title>Naprawa uszkodzonych danych</title>
<para>ZFS uses checksumming, replication, and self-healing data to minimize the chances of data corruption. Nonetheless, data corruption can occur if the pool isn't replicated, if corruption occurred while the pool was degraded, or an unlikely series of events conspired to corrupt multiple copies of a piece of data. Regardless of the source, the result is the same: The data is corrupted and therefore no longer accessible. The action taken depends on the type of data being corrupted, and its relative value. Dwa podstawowe typy danych mogą zostać uszkodzone:</para>
<itemizedlist>
<listitem>
<para>Pool metadata – ZFS requires a certain amount of data
to be parsed to open a pool and access datasets. If this data is corrupted,
the entire pool or complete portions of the dataset hierarchy will become
unavailable.</para>
</listitem>
<listitem>
<para>Object data – In this case, the corruption is within
a specific file or directory. This problem might result in a portion of the
file or directory being inaccessible, or this problem might cause the object
to be broken altogether.</para>
</listitem>
</itemizedlist>
<para>Dane są sprawdzane podczas normalnych operacji, ale również podczas sprawdzania weryfikacji danych. For more information about how to verify the integrity of pool data, see <olink targetdoc="" remap="internal" targetptr="gbbwa">Checking ZFS Data Integrity</olink>.</para>
<sect2 xml:id="gbcuz">
<title>Identyfikowanie rodzaju uszkodzenia danych</title>
<para>By default, the <command>zpool status</command> command shows only that
corruption has occurred, but not where this corruption occurred. For example:<indexterm xml:id="indexterm-555">
<primary>Pule pamięci ZFS</primary>
<secondary>identifying type of data corruption (<command>zpool status -v</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-556">
<primary>identifying</primary>
<secondary>type of data corruption (<command>zpool status -v</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-557">
<primary>usuwanie problemów</primary>
<secondary>determining type of data corruption (<command>zpool status -v</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool status tank -v</userinput>
   state: ONLINE
status: One or more devices has experienced an error resulting in data
                            corruption.  Applications may be affected.
	action: Restore the file in question if possible.  Otherwise restore the
                                                                                 entire pool from backup.
	   see: http://www.sun.com/msg/ZFS-8000-8A
	 scrub: none requested
	config:

		NAME         STATE     READ WRITE CKSUM
		tank         ONLINE       1     0     0
		  mirror     ONLINE       1     0     0
		    c2t0d0   ONLINE       2     0     0
		    c1t1d0   ONLINE       2     0     0

	errors: The following persistent errors have been detected:

		  DATASET  OBJECT  RANGE
		  tank     6       0-512</screen>
<para>Each error indicates only that an error occurred at the given point in time. Each error is not necessarily still present on the system. W zwykłych przypadkach ta sytuacja jest prawdziwa. Certain temporary outages might result in data corruption that is automatically repaired once the outage ends. A complete scrub of the pool is guaranteed to examine every active block in the pool, so the error log is reset whenever a scrub finishes. If you determine that the errors are no longer present, and you don't want to wait for a scrub to complete, reset all errors in the pool by using the <command>zpool online</command> command.</para>
<para>If the data corruption is in pool-wide metadata, the output is slightly different. Na przykład:</para>
<screen># <userinput>zpool status -v morpheus</userinput>
  pool: morpheus
    id: 1422736890544688191
 state: FAULTED
status: The pool metadata is corrupted.
action: The pool cannot be imported due to damaged devices/uszkodzone urządzenia or data.
   see: http://www.sun.com/msg/ZFS-8000-72

        morpheus    FAULTED   corrupted data
          c1t10d0   ONLINE</screen>
<para>In the case of pool-wide corruption, the pool is placed into the <literal>FAULTED</literal> state, because the pool cannot possibly provide the needed replication
level.</para>
</sect2>
<sect2 xml:id="gbctx">
<title>Naprawa uszkodzonego pliku lub katalogu</title>
<para>If a file or directory is corrupted, the system might still be able to function depending on the type of corruption. Uszkodzenie jest nie do naprawienia. Żadne zdrowe kopie danych nie istnieją w innych miejscach systemu. If the data is valuable, you have no choice but to restore the affected data from backup. Even so, you might be able to recover from this corruption without restoring the entire pool.<indexterm xml:id="indexterm-558"> <primary>Pule pamięci ZFS</primary> <secondary>repairing a corrupted file or directory</secondary> <tertiary>opis</tertiary> </indexterm> <indexterm xml:id="indexterm-559"> <primary>repairing</primary> <secondary>repairing a corrupted file or directory</secondary> <tertiary>opis</tertiary> </indexterm> <indexterm xml:id="indexterm-560"> <primary>usuwanie problemów</primary> <secondary>repairing a corrupted file or directory</secondary> <tertiary>opis</tertiary> </indexterm>
</para>
<para>If the damage is within a file data block, then the file can safely be removed, thereby clearing the error from the system. Pierwszym krokiem jest próba usunięcia pliku, używając komendy <command>rm</command>. If this command doesn't work, the corruption is within the file's metadata, and ZFS cannot determine which blocks belong to the file in order to remove the corruption.</para>
<para>If the corruption is within a directory or a file's metadata, the only
choice is to move the file elsewhere. You can safely move any file or directory
to a less convenient location, allowing the original object to be restored
in place.</para>
</sect2>
<sect2 xml:id="gbctt">
<title>Naprawa szeroko uszkodzonej puli pamięci ZFS</title>
<para>Jeśli uszkodzenie wystąpiło w metadanych puli, uniemożliwia ono dostęp do niej. W takim przypadku pula wraz z danymi musi zostać odtworzona z kopii bezpieczeństwa. Użyty mechanizm znacząco różni się ze względu na konfiguracje puli oraz strategie kopii bezpieczeństwa. Najpierw, zachowaj konfiguracje puli uzyskaną przy użyciu komendy <command>zpool status</command>, umożliwi to jej odtworzenie, kiedy zostanie zniszczona. Później użyj komendy <command>zpool destroy</command> <option>f</option> w celu zniszenia puli. Również zachowaj w bezpiecznym miejscu plik opisujący rozmieszczenie danych i różnego rodzaju lokalne parametry, ponieważ te informacje staną się niedostępne, jeśli pula kiedykolwiek zostanie niedostępna. Posiadając konfiguracje puli oraz rozmieszczenie danych, możesz zrekonstruować kompletną konfiguracje po zniszczeniu puli. W takim przypadku dane mogą zostać ponownie zapisane w zależności od kopii zapasowej oraz użytej strategii.<indexterm xml:id="indexterm-561"><primary>Pule pamięci ZFS</primary><secondary>naprawa szeroko uszkodzonej puli</secondary><tertiary>opis</tertiary></indexterm><indexterm xml:id="indexterm-562"><primary>naprawa</primary><secondary>szeroko uszkodzona pula</secondary><tertiary>opis</tertiary></indexterm><indexterm xml:id="indexterm-563"><primary>usuwanie problemów</primary><secondary>naprawa szeroko uszkodzonej puli</secondary><tertiary>opis</tertiary></indexterm>
</para>
</sect2>
</sect1>
<sect1 xml:id="gbbwc">
<title>Naprawa nie ładującego sie systemu</title>
<para>ZFS jest zaprojektowany tak, aby był odporny i stabilny w przypadku napotkania błędów. Jednak różnego rodzaju nieprzewidziane błędy w oprogramowaniu lub inne patologie mogą doprowadzić do awarii systemu w trakcie operacji dostępu do puli. Jako część uruchamiania systemu, każda z pul musi zostać otwarta, co oznacza, że tego typu awaria zostanie zapętlona. Aby wyjść z tej sytuacji, ZFS musi zostać poinformowane, aby nie otwierać żadnej z pul podczas startu systemu. <indexterm xml:id="indexterm-564"><primary>Pule pamięci ZFS</primary><secondary>naprawa nie ładującego się systemu</secondary><tertiary>opis&gt;</tertiary></indexterm><indexterm xml:id="indexterm-565"><primary>naprawa</primary><secondary>nie ładujący się system</secondary><tertiary>opis</tertiary></indexterm><indexterm xml:id="indexterm-566"><primary>usuwanie problemów</primary><secondary>naprawa nie ładującego się systemu</secondary><tertiary>opis</tertiary></indexterm>
</para>
<para>ZFS zachowuje wewnętrzny bufor dostępnych puli i ich konfiguracji w pliku <filename>/etc/zfs/zpool.cache</filename> Lokalizacja pliku oraz jego zawartość są prywatne i mogą się zmienić. Jeśli system nie może zostać uruchomiony, uruchom system w trybie <literal>none</literal> używając opcji <option>milestone=none</option> podczas uruchamiania systemu. Kiedy system jest podniesiony, odmontuj system plików root i zamontuj ponownie jako system plików z możliwością zapisu i usuń plik <filename>/etc/zfs/zpool.cache</filename>. Te akcje spowodują, że ZFS zapomni o istniejących pulach w systemie, zapobiegając próbę dostępu do uszkodzonej puli powodującej problem. Można wtedy uruchomić system w normalnym stanie, używając komendy <command>svcadm milestone all</command>. Podobny proces może zostać użyty, w przypadku uruchomienia systemu z zastępczego systemu plików root w celu wykonania napraw.</para>
<para>Kiedy system jest podniesiony, można spróbować zaimportować pulę, używając komendy <command>zpool import</command>. Jednak to działanie najprawdopodobniej spowoduje wystąpienie tego samego błędu, który wystąpił podczas uruchamiania systemu, ponieważ komenda używa tego samego mechanizmu dostępu do puli. Jeśli w systemie jest więcej niż jedna pula i chcesz zaimportować konkretną, bez dostępu do innych puli, musisz zainicjować ponownie urządzenia w uszkodzonej puli, po czym można bezpiecznie zaimportować zdrową pule.</para>
</sect1>
</chapter>