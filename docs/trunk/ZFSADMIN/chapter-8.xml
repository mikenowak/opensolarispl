<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML//EN" "docbook.dtd"[
	<!ENTITY % xinclude SYSTEM "xinclude.mod">
	%xinclude;
]>

<chapter xmlns="http://docbook.org/ns/docbook" version="5.0" xml:id="ftyxh">



<title>ZFS - zagadnienia zaawansowane</title>
<toc>
<para>W tym rozdziale przedstawiono emulowane woluminy, używanie ZFS-a w systemie Solaris
z zainstalowanymi zonami, ZFS-owe alternatywne pule główne oraz ZFS-owe profile praw.</para>
<para>Rozdział składa się z następujących podrozdziałów:</para>
<itemizedlist>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gaypf">Emulowane woluminy</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gayov">Używanie ZFS-a w systemie Solaris z zainstalowanymi zonami</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gbcgl">ZFS-owe alternatywne pule główne</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gbfvq">ZFS-owe profile praw</olink>
</para>
</listitem>
</itemizedlist>
</toc>
<sect1 xml:id="gaypf">
<title>Emulowane woluminy</title>
<para><emphasis>Emulowany wolumin</emphasis> to zbiór danych (dataset) reprezentujący
urządzenie blokowe, którego można używać jak każdego innego urządzenia blokowego.  Woluminom tym odpowiadają
pliki urządzeń w katalogu <filename>/dev/zvol/{dsk,rdsk}/path</filename>.<indexterm xml:id="indexterm-461">
<primary>emulowany wolumin</primary>
<secondary>opis</secondary>
</indexterm>
<indexterm xml:id="indexterm-462">
<primary>tworzenie</primary>
<secondary>emulowanego woluminu</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-463">
<primary>systemy plików ZFS</primary>
<secondary>tworzenie emulowanego woluminu</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<para>Poniższy przykład ilustruje tworzenie ZFS-owego woluminu <filename>tank/vol</filename> o pojemności 5 GB:</para>
<screen># <userinput>zfs create -V 5gb tank/vol</userinput>
</screen>
<para>Podczas tworzenia woluminu, rezerwacja jest automatycznie ustawiana
na rozmiar równy początkowej wielkości woluminu.  Wielkość rezerwacji cały
czas jest równa wielkości woluminu, dzięki czemu nie występują nieprzewidziane
efekty.  Na przykład, jeśli wielkość woluminu zmniejsza się, może
nastąpić uszkodzenie danych.  Należy zachować ostrożność podczas zmniejszania
woluminu.</para>
<para>Jeśli używasz systemu Solaris z zainstalowanymi zonami, nie możesz tworzyć ani klonować ZFS-owego woluminu
w zonie innej niz globalna.  Jakakolwiek próba stworzenia lub sklonowania 
woluminu z zony innej niż globalna nie powiedzie się.  Więcej
informacji o używaniu ZFS-owych woluminów w globalnej zonie w
<olink targetdoc="" remap="internal" targetptr="gbebi">Dodawanie ZFS-owych woluminów do zon innych niz globalna</olink>.</para>
<sect2 xml:id="gbfvg">
<title>Emulowane woluminy jako urządzenia wymiany (swap) i urządzenia zrzutów awaryjnych</title>
<para>W celu skonfigurowania przestrzeni wymiany należy utworzyć ZFS-owy wolumin o ustalonym rozmiarze
i włączyć wymianę na to urządzenie. Nie należy używać jako urządzenia wymiany pliku w ZFS-owym systemie
plików. Konfiguracja z ZFS-owym plikiem wymiany nie jest obsługiwana.<indexterm xml:id="indexterm-464">
<primary>emulowany wolumin</primary>
<secondary>jako urządzenie wymiany</secondary>
</indexterm>
<indexterm xml:id="indexterm-465">
<primary>tworzenie</primary>
<secondary>emulowanego woluminu jako urządzenia wymiany</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-466">
<primary>ZFS-owe systemy plików</primary>
<secondary>tworzenie emulowanego woluminu jako urządzenia wymiany</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<para>Poniższy przykład ilustruje dodanie woluminu <filename>tank/vol</filename> wielkości 5 GB
jako urządzenia wymiany.</para>
<screen># <userinput>swap -a /dev/zvol/dsk/tank/vol</userinput>
# <userinput>swap -l</userinput>
swapfile                 dev  swaplo blocks   free
/dev/dsk/c0t0d0s1      32,33      16 1048688  1048688
/dev/zvol/dsk/tank/vol 254,1      16 10485744 10485744</screen>
<para>Używanie woluminu ZFS-a jako urządzenia zrzutów awaryjnych nie jest obsługiwane. W celu
skonfigurowania urządzenia zrzutów awaryjnych należy użyć komendy <command>dumpadm</command>.</para>
</sect2>
</sect1>
<sect1 xml:id="gayov">
<title>Używanie ZFS-a w systemie Solaris z zainstalowanymi zonami</title>
<para>Datasety ZFS-a można dodać do zony jako ogólny system plików lub jako wydelegowany dataset.</para>
<para>Dodanie systemu plików umożliwia nieglobalnej zonie współdzielenie przestrzeni z zoną globalną, aczkolwiek administrator zony nie może kontrolować właściwości lub tworzyć nowych systemów plików w hierarchii. Mechanizm ten działa identycznie jak podłączenie do zony dowolnego innego systemu pliów i powinien być wykorzystywany głównie do współdzielenia miejsca.</para>
<para>ZFS pozwala także na delegację datasetu do nieglobalnej zony, przekazując administratorowi zony całkowitą kontrolę nad datasetem i jego potomkami. Administrator zony może tworzyć i niszczyć systemy plików w tym datasecie i modyfikować właściwości datasetów. Nie może natomiast zmieniać właściwości datasetów, które nie zostały dodane do zony i nie może przekroczyć limitów najwyższego poziomu ustalonych na wyeksportowanym datasecie.<indexterm xml:id="indexterm-467">
<primary>zony</primary>
<secondary>używanie z systemami plików ZFS-a</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-468">
<primary>systemy plików ZFS</primary>
<secondary>używanie w systemie Solaris z zainstalowanymi zonami</secondary>
<tertiary>opis</tertiary>
</indexterm>
</para>
<para>Poniższy przypadek opisuje pracę z ZFS-em w systemie Solaris z zainstalowanymi zonami:</para>
<itemizedlist>
<listitem>
<para>Właściwość <command>mountpoint</command> systemu plików ZFS-a dodawanego do nieglobalnej zony musi mieć wartość legacy.</para>
</listitem>
<listitem>
<para>Kiedy źródłowa <literal>zonepath</literal> i docelowa <literal>zonepath</literal> znajdują się na ZFS-ie i są w tej samej puli, <command>zoneadm clone</command> automatycznie użyje ZFS clone do sklonowana zony. Komenda <command>zoneadm clone</command> zrzuci obraz ZFS źródłowej <literal>zonepath</literal> i ustawi docelową <literal>zonepath</literal>. Nie należy używać opcji zrzutu obrazu ZFS-a do klonowania zony. Więcej informacji w <olink targetdoc="819-2450" remap="external" targetptr="zone">Części II, <citetitle remap="chapter">Zony,</citetitle> w <citetitle remap="book">Przewodnik administracji systemu: kontenery Solarisa - zarządzanie zasobami i zonami Solarisa</citetitle>
</olink>.</para>
</listitem>
</itemizedlist>
<sect2 xml:id="gbbrq">
<title>Dodawanie systemu plików ZFS-a do nieglobalnej zony</title>
<para>Kiedy zakłada się głównie współdzielenie przestrzeni, można dodać system plików ZFS-a jako ogólny system plików. Właściwość <literal>mountpoint</literal> systemu plików ZFS-a dodawanego do nieglobalnej zony musi być ustawiona na legacy.</para>
<para>Dodawnie systemu plików ZFS-a do nieglobalnej zony wykonuje się subkomendą <literal>add fs</literal> komendy <command>zonecfg</command>. Na przykład:<indexterm xml:id="indexterm-469">
<primary>zony</primary>
<secondary>dodawanie systemu plików ZFS-a do nieglobalnej zony</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-470">
<primary>dodawanie</primary>
<secondary>systemu plików ZFS-a do nieglobalnej zony</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-471">
<primary>systemy plików ZFS</primary>
<secondary>dodawanie systemu plików ZFS-a do nieglobalnej zony</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<para>W poniższym przykładzie globalny administrator dodaje w zonie globalnej system plików ZFS-a do nieglobalnej zony.</para>
<screen># <userinput>zonecfg -z zion</userinput>
zion: No such zone configured
Use 'create' to begin configuring a new zone.
zonecfg:zion&gt; <userinput>create</userinput>
zonecfg:zion&gt; <userinput>add fs</userinput>
zonecfg:zion:fs&gt; <userinput>set type=zfs</userinput>
zonecfg:zion:fs&gt; <userinput>set special=tank/zone/zion</userinput>
zonecfg:zion:fs&gt; <userinput>set dir=/export/shared</userinput>
zonecfg:zion:fs&gt; <userinput>end</userinput>
</screen>
<para>Polecenie to dodaje system plików ZFS-a <filename>tank/zone/zion</filename> do zony <literal>zion</literal> i montuje go w <filename>/export/shared</filename>.
Właściwość <property>mountpoint</property> systemu plików musi być ustawiona na <property>legacy</property>, a sam system plików nie może być wcześniej zamontowany w innej lokalizacji. Administrator zony może tworzyć i niszczyć pliki w systemie plików. System plików nie może zostać przemontowany w inna lokalizację, a administrator zony nie może zmienić właściwości systemu plików: atime, readonly, compression itd. Odpowiedzialnym za ustalenie i kontrolę właściwości systemu plióków jest administrator globalnej zony.</para>
<para>Więcej informacji i komendzie <command>zonecfg</command> i konfiguracji typów zasobów za pomocą <command>zonecfg</command> w <olink targetdoc="819-2450" remap="external" targetptr="zone">Części II, <citetitle remap="chapter">Zony,</citetitle> in <citetitle remap="book">Przewodnik administracji systemu: kontenery Solarisa - zarządzanie zasobami i zonami Solarisa</citetitle>
</olink>.</para>
</sect2>
<sect2 xml:id="gbbst">
<title>Delegowanie datasetów do nieglobalnej zony</title>
<para>Jeśli głównym celem jest oddelegowanie administrowania nośnikami danych do zony, wtedy należy skorzystać z możliwości ZFS-a dodania dataestu do zony za pomocą komendy <command>zonecfg</command> i jej podkomenty <literal>add dataset</literal>.<indexterm xml:id="indexterm-472">
<primary>zony</primary>
<secondary>delegowanie datasetu do nieglobalnej zony</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-473">
<primary>delegowanie</primary>
<secondary>datasetu do nieglobalnej zony</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-474">
<primary>systemy plików ZFS</primary>
<secondary>delegowanie datasetu do nieglobalnej zony</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<para>W poniższym przykładzie system plików ZFS-a oddelegowany został do nieglobalnej zony przez globalnego administratora w globalnej zonie.</para>
<screen># <userinput>zonecfg -z zion</userinput>
zion: No such zone configured
Use 'create' to begin configuring a new zone.
zonecfg:zion&gt; <userinput>create</userinput>
zonecfg:zion&gt; <userinput>add dataset</userinput>
zonecfg:zion:dataset&gt; <userinput>set name=tank/zone/zion</userinput>
zonecfg:zion:dataset&gt; <userinput>end</userinput>
</screen>
<para>Inaczej niż w przypadku dodania systemu plików, polecenie spowoduje widoczność systemu plików ZFS-a <filename>tank/zone/zion</filename> w zonie <literal>zion</literal>. Administrator zony może modyfikować właściwości systemu plików i tworzyć potomków. Dodatkowo może wykonywać obrazy systemu plików, klonować i w inny sposób kontrolować całą hierarchię systemu plików.</para>
<para>Więcej informacji o działaniach dozwolonych w zonie w <olink targetdoc="" remap="internal" targetptr="gbbsn">Zarządzenie właściwościami w zonie</olink>.</para>
</sect2>
<sect2 xml:id="gbebi">
<title>Dodawanie woluminu ZFS-a do nieglobalnej zony</title>
<para>Emoluwane woluminy nie mogą być dodawane do nieglobalnej zony poleceniem <command>zonecfg</command> <literal>add dataset</literal>. Przy wykryciu próby dodania takiego datasetu zona nie wybootuje. Można jednak dodać woluminy do zony za pomocą komendy <command>zonecfg</command> i komendy <literal>add device</literal>.<indexterm xml:id="indexterm-475">
<primary>zony</primary>
<secondary>dodawanie woluminu ZFS-a do nieglobalnej zony</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-476">
<primary>dodawanie</primary>
<secondary>woluminu ZFS-a do nieglobalnej zony</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-477">
<primary>systemy plików ZFS</primary>
<secondary>dodawanie</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<para>W poniższym przykładzie globalny administrator dodaje do nieglobalnej zony emulowany wolumin ZFS-a:</para>
<screen># <userinput>zonecfg -z zion</userinput>
zion: No such zone configured
Use 'create' to begin configuring a new zone.
zonecfg:zion&gt; <userinput>create</userinput>
zonecfg:zion&gt; <userinput>add device</userinput>
zonecfg:zion:device&gt; <userinput>set match=/dev/zvol/dsk/tank/vol</userinput>
zonecfg:zion:device&gt; <userinput>end</userinput>
</screen>
<para>Powyższe polecenie eksportuje emulowany wolumin <literal>tank/vol</literal> do zony. Należy pamiętać, że dodawanie surowego (ang. raw) woluminu do zony niesie ze sobą zagrożenia bezpieczeństwa, nawet jeśli wolumin nie odpowiada fizycznemu urządzeniu. W szczególności administrator zony mógłby stworzyć uszkodzone systemy plików, które spowodowałyby panikę systemu przy próbie zamontowania. Więcej informacji o dodawaniu urządzeń do zon i związanych z tym zagrożeniach w <olink targetdoc="" remap="internal" targetptr="gbbre">Znaczenie właściwości <property>zoned</property></olink>.</para>
<para>Więcej informacji o dodawaniu urządzeń do zon w <olink targetdoc="819-2450" remap="external" targetptr="zone">Części II, <citetitle remap="chapter">Zonach,</citetitle> w <citetitle remap="book">Przewodnik administratora systemu: zarządzanie kontenerami i zasobami Solarisa oraz zony Solarisa</citetitle>
</olink>.</para>
</sect2>
<sect2 xml:id="gbbso">
<title>Używanie puli nośników danych w zonie</title>
<para>ZFS storage pools cannot be created or modified within a zone. The delegated
administration model centralizes control of physical storage devices within
the global zone and control of virtual storage to non-global zones. While
a pool-level dataset can be added to a zone, any command that modifies the
physical characteristics of the pool, such as creating, adding, or removing
devices, is not allowed from within a zone. Even if physical devices are added
to a zone by using the <command>zonecfg</command> command's <literal>add device</literal> subcommand,
or if files are used, the <command>zpool</command> command does not allow
the creation of any new pools within the zone.</para>
</sect2>
<sect2 xml:id="gbbsn">
<title>Property Management Within a Zone</title>
<para>Once a dataset is added to a zone, the zone administrator can control
specific dataset properties. When a dataset is added to a zone, all its ancestors
are visible as read-only datasets, while the dataset itself is writable as
are all its children. For example, consider the following configuration:<indexterm xml:id="indexterm-478">
<primary>zony</primary>
<secondary>ZFS property management within a zone</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-479">
<primary>ZFS properties</primary>
<secondary>management within a zone</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-480">
<primary>systemy plików ZFS</primary>
<secondary>property management within a zone</secondary>
<tertiary>opis</tertiary>
</indexterm>
</para>
<screen>global# <userinput>zfs list -Ho name</userinput>
tank
tank/home
tank/data
tank/data/matrix
tank/data/zion
tank/data/zion/home</screen>
<para>If <filename>tank/data/zion</filename> is added to a zone, each dataset
would have the following properties.</para>
<informaltable frame="topbot">
<tgroup cols="4" colsep="0" rowsep="0">
<colspec colwidth="25*"/>
<colspec colwidth="25*"/>
<colspec colwidth="25*"/>
<colspec colwidth="25*"/>
<thead>
<row rowsep="1">
<entry>
<para>Dataset</para>
</entry>
<entry>
<para>Visible</para>
</entry>
<entry>
<para>Writable</para>
</entry>
<entry>
<para>Immutable Properties</para>
</entry>
</row>
</thead>
<tbody>
<row>
<entry>
<para>
<filename>tank</filename>
</para>
</entry>
<entry>
<para>Yes</para>
</entry>
<entry>
<para>No</para>
</entry>
<entry>
<para>-</para>
</entry>
</row>
<row>
<entry>
<para>
<filename>tank/home</filename>
</para>
</entry>
<entry>
<para>No</para>
</entry>
<entry>
<para>-</para>
</entry>
<entry>
<para>-</para>
</entry>
</row>
<row>
<entry>
<para>
<filename>tank/data</filename>
</para>
</entry>
<entry>
<para>Yes</para>
</entry>
<entry>
<para>No</para>
</entry>
<entry>
<para>-</para>
</entry>
</row>
<row>
<entry>
<para>
<filename>tank/data/matrix</filename>
</para>
</entry>
<entry>
<para>No</para>
</entry>
<entry>
<para>-</para>
</entry>
<entry>
<para>-</para>
</entry>
</row>
<row>
<entry>
<para>
<filename>tank/data/zion</filename>
</para>
</entry>
<entry>
<para>Yes</para>
</entry>
<entry>
<para>Yes</para>
</entry>
<entry>
<para>
<property>sharenfs</property>, <property>zoned</property>, <property>quota</property>, <property>reservation</property>
</para>
</entry>
</row>
<row>
<entry>
<para>
<filename>tank/data/zion/home</filename>
</para>
</entry>
<entry>
<para>Yes</para>
</entry>
<entry>
<para>Yes</para>
</entry>
<entry>
<para>
<property>sharenfs</property>, <property>zoned</property>
</para>
</entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>Note that every parent of <filename>tank/zone/zion</filename> is visible
read-only, all children are writable, and datasets that are not part of the
parent hierarchy are not visible at all. The zone administrator cannot change
the <property>sharenfs</property> property, because non-global zones cannot
act as NFS servers. Neither can the zone administrator change the <literal>zoned</literal> property,
because doing so would expose a security risk as described in the next section. </para>
<para>Any other property can be changed, except for the added dataset itself,
where the <property>quota</property> and <property>reservation</property> properties
cannot be changed. This behavior allows the global zone administrator to control
the space consumption of all datasets used by the non-global zone.</para>
<para>In addition, the <property>sharenfs</property> and <property>mountpoint</property> properties
cannot be changed by the global zone administrator once a dataset has been
added to a non-global zone.</para>
</sect2>
<sect2 xml:id="gbbre">
<title>Understanding the <property>zoned</property> Property</title>
<para>When a dataset is added to a non-global zone, the dataset must be specially
marked so that certain properties are not interpreted within the context of
the global zone. Once a dataset has been added to a non-global zone under
the control of a zone administrator, its contents can no longer be trusted.
As with any file system, there might be setuid binaries, symbolic links, or
otherwise questionable contents that might adversely affect the security of
the global zone. In addition, the <property>mountpoint</property> property
cannot be interpreted in the context of the global zone. Otherwise, the zone
administrator could affect the global zone's namespace. To address the latter,
ZFS uses the <property>zoned</property> property to indicate that a dataset
has been delegated to a non-global zone at one point in time.<indexterm xml:id="indexterm-481">
<primary>zony</primary>
<secondary>
<property>zoned</property> property</secondary>
<tertiary>detailed description</tertiary>
</indexterm>
<indexterm xml:id="indexterm-482">
<primary>
<property>zoned</property> property</primary>
<secondary>detailed description</secondary>
</indexterm>
<indexterm xml:id="indexterm-483">
<primary>ZFS properties</primary>
<secondary>
<property>zoned</property> property</secondary>
<tertiary>detailed description</tertiary>
</indexterm>
</para>
<para>The <property>zoned</property> property is a boolean value that is automatically
turned on when a zone containing a ZFS dataset is first booted. A zone administrator
will not need to manually turn on this property. If the <property>zoned</property> property
is set, the dataset cannot be mounted or shared in the global zone, and is
ignored when the <command>zfs share</command> <option>
a</option> command or
the <command>zfs mount</command> <option>
a</option> command is executed. In
the following example, <filename>tank/zone/zion</filename> has been added
to a zone, while <filename>tank/zone/global</filename> has not:</para>
<screen># <userinput>zfs list -o name,zoned,mountpoint -r tank/zone</userinput>
NAME                  ZONED  MOUNTPOINT
tank/zone/global        off  /tank/zone/global
tank/zone/zion           on  /tank/zone/zion
# <userinput>zfs mount</userinput>
tank/zone/global           /tank/zone/global
tank/zone/zion             /export/zone/zion/root/tank/zone/zion</screen>
<para>Note the difference between the <property>mountpoint</property> property
and the directory where the <filename>tank/zone/zion</filename> dataset is
currently mounted. The <property>mountpoint</property> property reflects the
property as stored on disk, not where the dataset is currently mounted on
the system.</para>
<para>When a dataset is removed from a zone or a zone is destroyed, the <property>zoned</property> property is <emphasis role="strong">not</emphasis> automatically
cleared. This behavior is due to the inherent security risks associated with
these tasks. Because an untrusted user has had complete access to the dataset
and its children, the <property>mountpoint</property> property might be set
to bad values, or setuid binaries might exist on the file systems.</para>
<para>To prevent accidental security risks, the <property>zoned</property> property
must be manually cleared by the global administrator if you want to reuse
the dataset in any way. Before setting the <property>zoned</property> property
to <literal>off</literal>, make sure that the <property>mountpoint</property> property
for the dataset and all its children are set to reasonable values and that
no setuid binaries exist, or turn off the <property>setuid</property> property.</para>
<para>Once you have verified that no security vulnerabilities are left, the <property>zoned</property> property can be turned off by using the <command>zfs set</command> or <command>zfs inherit</command> commands. If the <property>zoned</property> property
is turned off while a dataset is in use within a zone, the system might behave
in unpredictable ways. Only change the property if you are sure the dataset
is no longer in use by a non-global zone.</para>
</sect2>
</sect1>
<sect1 xml:id="gbcgl">
<title>ZFS Alternate Root Pools</title>
<para>When a pool is created, the pool is intrinsically tied to the host system.
The host system maintains knowledge about the pool so that it can detect when
the pool is otherwise unavailable. While useful for normal operation, this
knowledge can prove a hindrance when booting from alternate media, or creating
a pool on removable media. To solve this problem, ZFS provides an <emphasis>alternate
root</emphasis> pool feature. An alternate root pool does not persist across
system reboots, and all mount points are modified to be relative to the root
of the pool.<indexterm xml:id="indexterm-484">
<primary>alternate root pools</primary>
<secondary>opis</secondary>
</indexterm>
<indexterm xml:id="indexterm-485">
<primary>ZFS storage pools</primary>
<secondary>alternate root pools</secondary>
</indexterm>
</para>
<sect2 xml:id="gbdaw">
<title>Creating ZFS Alternate Root Pools</title>
<para>The most common use for creating an alternate root pool is for use with
removable media. In these circumstances, users typically want a single file
system, and they want it to be mounted wherever they choose on the target
system. When an alternate root pool is created by using the <option>
R</option> option,
the mount point of the root file system is automatically set to <filename>/</filename>,
which is the equivalent of the alternate root itself.<indexterm xml:id="indexterm-486">
<primary>alternate root pools</primary>
<secondary>creating</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-487">
<primary>creating</primary>
<secondary>alternate root pools</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<para>In the following example, a pool called <literal>morpheus</literal> is
created with <filename>/mnt</filename> as the alternate root path:</para>
<screen># <userinput>zpool create -R /mnt morpheus c0t0d0</userinput>
# <userinput>zfs list morpheus</userinput>
NAME                   USED  AVAIL  REFER  MOUNTPOINT
morpheus              32.5K  33.5G     8K  /mnt/</screen>
<para>Note the single file system, <literal>morpheus</literal>, whose mount
point is the alternate root of the pool, <filename>/mnt</filename>. The mount
point that is stored on disk is <filename>/</filename> and the full path to <filename>/mnt</filename> is interpreted only in the context of the alternate root pool.
This file system can then be exported and imported under an arbitrary alternate
root pool on a different system.</para>
</sect2>
<sect2 xml:id="gbdbj">
<title>Importing Alternate Root Pools</title>
<para>Pools can also be imported using an alternate root. This feature allows
for recovery situations, where the mount points should not be interpreted
in context of the current root, but under some temporary directory where repairs
can be performed. This feature also can be used when mounting removable media
as described above.<indexterm xml:id="indexterm-488">
<primary>alternate root pools</primary>
<secondary>importing</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-489">
<primary>importing</primary>
<secondary>alternate root pools</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<para>In the following example, a pool called <literal>morpheus</literal> is
imported with <filename>/mnt</filename> as the alternate root path. This example
assumes that <literal>morpheus</literal> was previously exported.</para>
<screen># <userinput>zpool import -R /mnt morpheus</userinput>
# <userinput>zpool list morpheus</userinput>
NAME                    SIZE    USED   AVAIL    CAP  HEALTH     ALTROOT
morpheus               33.8G   68.0K   33.7G     0%  ONLINE     /mnt
# <userinput>zfs list morpheus</userinput>
NAME                   USED  AVAIL  REFER  MOUNTPOINT
morpheus              32.5K  33.5G     8K  /mnt/morpheus</screen>
</sect2>
</sect1>
<sect1 xml:id="gbfvq">
<title>ZFS Rights Profiles</title>
<para>If you want to perform ZFS management tasks without using the superuser
(root) account, you can assume a role with either of the following profiles
to perform ZFS administration tasks:<indexterm xml:id="indexterm-490">
<primary>systemy plików ZFS</primary>
<secondary>rights profiles</secondary>
</indexterm>
<indexterm xml:id="indexterm-491">
<primary>ZFS storage pools</primary>
<secondary>rights profiles</secondary>
</indexterm>
<indexterm xml:id="indexterm-492">
<primary>rights profiles</primary>
<secondary>for management of ZFS file systems and storage pools</secondary>
<tertiary>opis</tertiary>
</indexterm>
</para>
<itemizedlist>
<listitem>
<para>ZFS Storage Management – Provides the ability to create,
destroy, and manipulate devices within a ZFS storage pool</para>
</listitem>
<listitem>
<para>ZFS File system Management – Provides the ability to
create, destroy, and modify ZFS file systems</para>
</listitem>
</itemizedlist>
<para>For more information about creating or assigning roles, see <olink targetdoc="819-3321" remap="external">
<citetitle remap="book">System Administration Guide: Security
Services</citetitle>
</olink>.</para>
</sect1>
</chapter>
