<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML//EN" "docbook.dtd"[
	<!ENTITY % xinclude SYSTEM "xinclude.mod">
	%xinclude;
]>

<chapter xmlns="http://docbook.org/ns/docbook" version="5.0" xml:id="gavwn">



<title>Zarządzanie pulami nośników danych ZFS</title>
<toc>
<para>W tym rozdziale opisane jest tworzenie i zarządzanie pulami nośników 
danych ZFS.</para>
<para>Podrozdziały:</para>
<itemizedlist>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gcfog">Składniki puli ZFS
</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gaypw">Tworzenie i usuwanie puli
 ZFS</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gayrd">Zarządzanie urządzeniami 
w puli ZFS</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gaynp">Sprawdzanie statusu puli 
ZFS</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gbchy">Przenoszenie puli ZFS
</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gcikw">Powiększanie puli ZFS
</olink>
</para>
</listitem>
</itemizedlist>
</toc>
<sect1 xml:id="gcfog">
<title>Składniki puli nośników danych ZFS</title>
<para>Poniższe paragrafy dostarczają dokładnych informacji o następujących 
komponentach puli nośników danych:</para>
<itemizedlist>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazdp">Używanie dysków w pulach danych ZFS</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazcr">Używanie plików w pulach danych ZFS</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazca">Rozpoznawanie wirtualnych urządzeń w pulach danych ZFS</olink>
</para>
</listitem>
</itemizedlist>
<sect2 xml:id="gazdp">
<title>Używanie dysków w puli nośników danych ZFS</title>
<para>Najbardziej podstawowym elementem puli nośników danych jest fizyczny 
nośnik danych. Może nim być urządzenie blokowe o pojemności nie mniejszej niż 
128 MB. Najczęściej urządzeniem tym jest dysk twardy widoczny dla systemu 
w katalogu <filename>/dev/dsk</filename>.<indexterm xml:id="indexterm-81">
<primary>Pule nośników danych ZFS</primary>
<secondary>komponenty</secondary>
</indexterm>
<indexterm xml:id="indexterm-82">
<primary>komponenty</primary>
<secondary>pul nośników danych ZFS</secondary>
</indexterm>
</para>
<para>Nośnikiem danych może być cały dysk (<filename>c1t0d0</filename>), 
lub pojedynczy blok (ang. slice), (<filename>c0t0d0s7</filename>). Zaleca się 
używania całych dysków, w którym to przypadku dysk nie wymaga specjalnego 
formatowania. ZFS formatuje dysk używają etykiet EFI do przechowywania jednego 
bloku. W takim przypadku tablica partycji wyświetlana przez komendę <command>
format</command> wygląda podobnie:</para>
<screen>Current partition table (original):
Total disk sectors available: 71670953 + 16384 (reserved sectors)

Part      Tag    Flag     First Sector        Size        Last Sector
  0        usr    wm                34      34.18GB         71670953    
  1 unassigned    wm                 0          0              0    
  2 unassigned    wm                 0          0              0    
  3 unassigned    wm                 0          0              0    
  4 unassigned    wm                 0          0              0    
  5 unassigned    wm                 0          0              0    
  6 unassigned    wm                 0          0              0    
  7 unassigned    wm                 0          0              0    
  8   reserved    wm          71670954       8.00MB         71687337</screen>
<para>Aby użyć całego dysku, musi być nazwany zgodnie z konwencją Solarisa, na 
przykład <filename>/dev/dsk/cXtXdXsX</filename>. Niektóre sterowniki od 
niezależnych dostawców używają innych konwencji nazewniczych lub umieszczają 
dyski w innych katalogach, niż <filename>/dev/dsk</filename>. Aby użyć tych 
dysków, należy ręcznie je etykietować i udostępnić blok ZFS-owi.
<indexterm xml:id="indexterm-83">
<primary>Etykiety EFI</primary>
<secondary>opis</secondary>
</indexterm>
<indexterm xml:id="indexterm-84">
<primary>Etykiety EFI</primary>
<secondary>współpraca z ZFS</secondary>
</indexterm>
</para>
<para>ZFS nadaje etykietę EFI w momencie tworzenia puli przy użyciu całego 
dysku. Można też etykietować dyski tradycyjnym Solarisowym VTOC przy tworzeniu 
puli z użyciem bloku.</para>
<para>Bloki powinny być używane tylko w poniższych warunkach:</para>
<itemizedlist>
<listitem>
<para>Nazwa urządzenia jest niestandardowa.</para>
</listitem>
<listitem>
<para>Pojedynczy dysk jest współdzielony między system plików ZFS oraz inny 
system plików, na przykład UFS.</para>
</listitem>
<listitem>
<para>Dysk jest używany na potrzeby swap bądź zrzutów (dump).</para>
</listitem>
</itemizedlist>
<para>Dyski można wskazać zarówno poprzez użycie pełnej ścieżki, na przykład 
<filename>/dev/dsk/c1t0d0</filename>, lub przez nazwę urządzenia znajdującego 
się w katalogu<filename>/dev/dsk</filename>, na przykład <filename>c1t0d0
</filename>. Poniższe są prawidłowymi nazwami dysków:
<indexterm xml:id="indexterm-85">
<primary>pule nośników danych ZFS</primary>
<secondary>używanie całego dysku</secondary>
</indexterm>
<indexterm xml:id="indexterm-86">
<primary>całe dyski</primary>
<secondary>jako komponenty pul nośników danych ZFS</secondary>
</indexterm>
<indexterm xml:id="indexterm-87">
<primary>dyski</primary>
<secondary>jako komponenty pul nośników danych ZFS</secondary>
</indexterm>
</para>
<itemizedlist>
<listitem>
<para>
<filename>c1t0d0</filename>
</para>
</listitem>
<listitem>
<para>
<filename>/dev/dsk/c1t0d0</filename>
</para>
</listitem>
<listitem>
<para>
<filename>c0t0d6s2</filename>
</para>
</listitem>
<listitem>
<para>
<filename>/dev/foo/disk</filename>
</para>
</listitem>
</itemizedlist>
<para>Using whole physical disks is the simplest way to create ZFS storage
pools. ZFS configurations become progressively more complex, from management,
reliability, and performance perspectives, when you build pools from disk
slices, LUNs in hardware RAID arrays, or volumes presented by software-based
volume managers. The following considerations might help you determine how
to configure ZFS with other hardware or software storage solutions:</para>
<itemizedlist>
<listitem>
<para>If you construct ZFS configurations on top of LUNs from hardware
RAID arrays, you need to understand the relationship between ZFS redundancy
features and the redundancy features offered by  the array. Certain configurations
might provide adequate redundancy and performance, but other configurations
might not.</para>
</listitem>
<listitem>
<para>You can construct logical devices for ZFS using volumes presented
by software-based volume managers, such as <trademark>Solaris</trademark> Volume
Manager (SVM) or Veritas Volume Manager (VxVM).  However, these configurations
are not recommended. While ZFS functions properly on such devices, less-than-optimal
performance might be the result.</para>
</listitem>
</itemizedlist>
<para>For additional information about storage pool recommendations, see the
ZFS best practices site:</para>
<para>
<link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:type="url" xlink:href="http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide">http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide</link>
</para>
<para>Dyski identyfikuje się zarówno za pomocą ścieżki, jak i ich ID, jeśli 
jest dostępne. Metoda ta pozwala na rekonfigurowanie urządzeń w systemie bez 
uaktualniania stanu ZFS. Jeśli dysk zostanie przeniesiony z kontrolera 1., 
do kontrolera 2., ZFS użyje ID urządzenia do wykrycia go i ustalenia, że powinno
 być teraz używane za pośrednictwem kontrolera 2. ID urządzenia jest unikalne 
dla firmware'u. Wprawdzie mało prawdopodobna jest zmiana ID urządzenia, jednak 
niektóre uaktualnienia firmware'u dokonywały takich zmian. W takiej sytuacji, 
ZFS może użyć urządzenia za pośrednictwem jego ścieżki systemowej i 
automatycznie uaktualnić przechowywane ID urządzenia. Jeśli dokonano zarówno 
zmiany ścieżki jak i ID urządzenia, aby możliwe było użycie puli, należy 
wyeksportować pulę, a później ją zaimportować.</para>
</sect2>
<sect2 xml:id="gazcr">
<title>Używanie plików w pulach ZFS</title>
<para>ZFS umożliwia korzystanie z plików UFS jako urządzeń w puli. Opcja ta jest
 pomyślana głównie w celach testowych i do prostych eksperymentów, nie zaś do 
użycia w środowisku produkcyjnym. Powodem jest to, że <emphasis role="strong">
spójność plików zależy od systemu plików, na których pliki są przechowywane.
</emphasis>. Jeśli zostanie utworzona pula ZFS na bazie plików przechowywanych 
na systemie plików UFS, wtedy pośrednio żąda się gwarancji semantyki 
synchronicznej oraz poprawności od systemu plików UFS.
<indexterm xml:id="indexterm-88">
<primary>Pule nośników danych ZFS</primary>
<secondary>używanie plików</secondary>
</indexterm>
<indexterm xml:id="indexterm-89">
<primary>pliki</primary>
<secondary>jako komponenty pul nośników danych ZFS</secondary>
</indexterm>
</para>
<para>Pliki mogą się jednak okazać przydatne przy pierwszych próbach z ZFS-em, 
bądź przy eksperymentowaniu z bardziej skomplikowanymi układami, kiedy nie ma 
wystarczającej ilości prawdziwych nośników. Wszystkie pliki należy wskazywać za 
pomocą pełnej ścieżki dostępu i muszą mieć przynajmniej 128MB. Jeśli plik 
zostanie przeniesiony bądź zmieniona będzie jego nazwa, pula musi zostać 
wyeksportowana i zaimportowana, ponieważ plikom nie przypisuje się żadnego 
unikalnego ID, po którym mogłyby być znów odnalezione.</para>
</sect2>
<sect2 xml:id="gazca">
<title>Rozpoznawanie urządzeń wirtualne w pulach nośników danych</title>
<para>Każda pula nośników składa się z jednego lub więcej wirtualnych urządzeń. 
<emphasis>Urządzenie wirtualne</emphasis> to wewnętrzna reprezentacja puli 
nośników, opisująca układ fizycznych nośników i ich charakterystyki awaryjności.
 Wirtualne urządzenie jako takie reprezentuje urządzenia dyskowe lub pliki, 
używane do tworzenia puli. <indexterm xml:id="indexterm-90">
<primary>Pule nośników danych ZFS</primary>
<secondary>urządzenia wirtualne</secondary>
</indexterm>
<indexterm xml:id="indexterm-91">
<primary>urządzenia wirtualne</primary>
<secondary>jako komponenty pul nośników danych ZFS</secondary>
</indexterm>
</para>
<para>Dwa urządzenia wirtualne najwyższego poziomu (top level) zapewniają 
redundancję danych: mirror i RAID-Z. Urządzenia te składają się z dysków, 
bloków dysków lub plików. </para>
<para>Dyski, bloki dysków i pliki używane w pulach poza mirrorami i RAID-Z 
same funkcjonują jako urządzenia wirtualne najwyższego poziomu. </para>
<para>Pule nośników danych zawierają zazwyczaj wiele urządzeń wirtualnych 
najwyższego poziomu. ZFS dynamicznie rozdziela dane pomiędzy wszystkie 
urządzenia najwyższego poziomu w puli.</para>
</sect2>
</sect1>
<sect1 xml:id="gcfof">
<title>Opcje replikacji danych w pulach ZFS</title>
<para>ZFS zapewnia redundancję danych, jak i właściwości samonaprawiania
uszkodzonych danych w konfiguracji mirror oraz RAID-Z.<indexterm xml:id="indexterm-92">
<primary>opcje replikacji danych w ZFS</primary>
<secondary>mirror lub RAID-Z</secondary>
</indexterm>
</para>
<itemizedlist>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gamss">Konfiguracja puli nośników danych: mirror</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gamtu">Konfiguracja puli nośników danych RAID-Z</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazch">Samonaprawiające się dane w konfiguracjach z replikacją</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazdd">Dynamiczny striping w pulach danych</olink>
</para>
</listitem>
</itemizedlist>
<sect2 xml:id="gamss">
<title>Konfiguracja puli nośników danych: mirror</title>
<para>Konfiguracja lustrzana (mirrored) puli nośników danych wymaga użycia 
przynajmniej dwóch dysków, najlepiej na osobnych kontrolerach. W konfiguracji 
lustrzanej można używać wielu dysków. Dodatkowo można stworzyć więcej niż jedno 
lustro w puli. Prosta konfiguracja lustrzana wygląda podobnie do poniższej:
<indexterm xml:id="indexterm-93">
<primary>konfiguracja lustrzana</primary>
<secondary>opis</secondary>
</indexterm>
<indexterm xml:id="indexterm-94">
<primary>konfiguracja lustrzana</primary>
<secondary>opcja redundancji</secondary>
</indexterm>
<indexterm xml:id="indexterm-95">
<primary>konfiguracja lustrzana</primary>
<secondary>koncepcja</secondary>
</indexterm>
<indexterm xml:id="indexterm-96">
<primary>pule nośników danych ZFS</primary>
<secondary>konfiguracja lustrzana, opis</secondary>
</indexterm>
</para>
<screen>mirror c1t0d0 c2t0d0</screen>
<para>Bardziej skomplikowana konfiguracja będzie podobna do poniższej:</para>
<screen>mirror c1t0d0 c2t0d0 c3t0d0 mirror c4t0d0 c5t0d0 c6t0d0</screen>
<para>Więcej informacji o tworzeniu lustrzanej konfiguracji puli nośników 
danych w <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazhv">tworzeniu 
lustrzanych pul nośników danych</olink>.</para>
</sect2>
<sect2 xml:id="gamtu">
<title>Konfiguracja puli nośników danych RAID-Z</title>
<para>Oprócz konfiguracji lustrzanej puli nośników danych, ZFS umożliwia 
konfigurację RAID-Z z pojedynczą lub podwójną kontrolą parzystości.
RAID-Z z pojedynczą kontrolą parzystości jest podobny do RAID-5. RAID-Z
z podwójną kontrolą parzystości jest zbliżony do RAID-6.
<indexterm xml:id="indexterm-97">
<primary>konfiguracja RAID-Z</primary>
<secondary>pojedyncza parzystość, opis</secondary>
</indexterm>
<indexterm xml:id="indexterm-98">
<primary>konfiguracja RAID-Z</primary>
<secondary>podwójna parzystość, opis</secondary>
</indexterm>
<indexterm xml:id="indexterm-99">
<primary>konfiguracja RAID-Z</primary>
<secondary>opcja redundancji</secondary>
</indexterm>
<indexterm xml:id="indexterm-100">
<primary>konfiguracja RAID-Z</primary>
<secondary>koncepcja</secondary>
</indexterm>
<indexterm xml:id="indexterm-101">
<primary>pule nośników danych ZFS</primary>
<secondary>konfiguracja RAID-Z, opis</secondary>
</indexterm>
</para>
<para>Wszystkie tradycyjne algorytmy RAID-5 i podobne (RAID-4. RAID-5. RAID-6, 
RDP i PARZYSTE-NIEPARZYSTE na przykład) obarczone są problemem tak zwanej 
"luki zapisu RAID-5". Jeśli tylko fragment bloku danych RAID-5 zostanie zapisany
 i nastąpi awaria zasilania zanim wszystkie bloki zostaną zapisane na dysk, 
parzystość i dane będą rozsynchronizowane, a przez to bezużyteczne na zawsze, 
chyba że następny zapis pełnego bloku nadpisze ten fragment. W RAID-Z ZFS używa 
bloków o zmiennej długości, zatem każdy zapis zapis jest zapisem pełnego bloku. 
Jest to możliwe tylko dlatego, że ZFS integruje system plików i zarządzanie 
urządzeniami w taki sposób, że metadane systemu plików zawierają wystarczająco 
dużo informacji o modelu redundancji w niższej warstwie, aby poradzić sobie z 
blokami RAID o zmiennej długości. RAID-Z to pierwszy to pierwsze na świecie 
czysto programowe rozwiązanie problemu luki zapisu RAID-5.</para>
<para>Do konfiguracji RAID-Z wymagane są co najmniej dwa dyski. Obecnie RAID-Z 
zapewnia pojedynczą parzystość. Na przykład, jeśli w puli RAID-Z są trzy dyski, 
dane parzystości zajmują miejsce równe pojemności jednego z dysków.</para>
<para>A RAID-Z configuration with N disks of size X with P parity disks can
hold approximately (N-P)*X bytes and can withstand one device failing before
data integrity is compromised.  You need at least two disks for a single-parity
RAID-Z configuration and at least three disks for a double-parity RAID-Z configuration.
For example, if you have three disks in a single-parity RAID-Z configuration,
parity data occupies space equal to one of the three disks. Otherwise, no
special hardware is required to create a RAID-Z configuration. </para>
<para>Koncepcyjnie, konfiguracja RAID-Z z trzema dyskami wygląda jak poniżej:
</para>
<screen>raidz c1t0d0 c2t0d0 c3t0d0</screen>
<para>Bardziej skomplikowana konfiguracja RAID-Z wygląda podobnie do poniższej:
</para>
<screen remap="wide">raidz c1t0d0 c2t0d0 c3t0d0 c4t0d0 c5t0d0 c6t0d0 c7t0d0 raidz c8t0d0 c9t0d0 c10t0d0 c11t0d0
c12t0d0 c13t0d0 c14t0d0</screen>
<para>Jeśli tworzona jest konfiguracja z wieloma dyskami, jak w powyższym 
przykładzie, konfiguracja RAID-Z z 14 dyskami sprawdza się lepiej jako dwie 
konfiguracje RAID-Z z 7 dyskami każda. Konfiguracje RAID-Z grupami o 
jednocyfrowej liczbie dysków powinny mieć lepszą wydajność.</para>
<para>Więcej informacji o tworzeniu pul RAID-Z w 
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gcvjg">Tworzenie pul RAID-Z
</olink>.</para>
<para>For more information about choosing between a mirrored configuration
or a RAID-Z configuration based on performance and space considerations, see
the following blog:</para>
<para>
<link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:type="url" xlink:href="http://blogs.sun.com/roller/page/roch?entry=when_to_and_not_to">http://blogs.sun.com/roller/page/roch?entry=when_to_and_not_to</link>
</para>
<para>For additional information on RAID-Z storage pool recommendations, see
the ZFS best practices site:</para>
<para>
<link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:type="url" xlink:href="http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide">http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide</link>
</para>
</sect2>
<sect2 xml:id="gazch">
<title>Samonaprawiające się dane w konfiguracjach z redundancją</title>
<para>W konfiguracji lustrzanej lub RAID-Z ZFS zapewnia samonaprawianie się 
danych.</para>
<para>Kiedy wykryty zostanie blok z uszkodzonymi danymi, ZFS nie tylko pobiera 
poprawne dane z innej kopii, ale również naprawia uszkodzone dane zastępując je 
poprawną kopią.<indexterm xml:id="indexterm-102">
<primary>samonaprawiające się dane</primary>
<secondary>opis</secondary>
</indexterm>
</para>
</sect2>
<sect2 xml:id="gazdd">
<title>Dynamiczny striping w pulach danych</title>
<para>Dla każdego urządzenia wirtualnego dodanego do puli, ZFS dynamicznie 
zapisuje dane równomiernie na wszystkich dostępnych dyskach (stripe). Decyzja 
o tym, gdzie zapisać dane podejmowana jest w momencie zapisu, nie pojawiają się 
zatem bloki o stałej wielkości w momencie alokacji.
<indexterm xml:id="indexterm-103">
<primary>dynamiczny striping</primary>
<secondary>opis</secondary>
</indexterm>
<indexterm xml:id="indexterm-104">
<primary>dynamiczny striping</primary>
<secondary>możliwości puli nośników danych</secondary>
</indexterm>
<indexterm xml:id="indexterm-105">
<primary>Pule nośników danych ZFS</primary>
<secondary>dynamiczny striping</secondary>
</indexterm>
</para>
<para>Kiedy urządzenia wirtualne dodawane są do puli. ZFS stopniowo zapisuje 
dane na nowym urządzeniu w celu zapewnienia wydajności i zgodności z regułami 
alokacji przestrzeni dyskowej. Każde urządzenie wirtualne może być także lustrem
 lub urządzeniem RAID-Z zawierającym inne urządzenia dyskowe lub pliki. 
Konfiguracja taka pozwala na elastyczność w kontrolowaniu charakterystyk 
awaryjności w puli. Na przykład, można utworzyć następującą pulę z 4 dysków:
</para>
<itemizedlist>
<listitem>
<para>Cztery dyski używające dynamicznego stripingu</para>
</listitem>
<listitem>
<para>Jedna konfiguracja RAID-Z z czterema urządzeniami</para>
</listitem>
<listitem>
<para>Dwuurządzeniowy mirror z użyciem dynamicznego stripingu</para>
</listitem>
</itemizedlist>
<para>Wprawdzie ZFS umożliwia kombinowanie różnych typów urządzeń wirtualnych w 
jednej puli, jednak praktyka taka jest niezalecana. Można, na przykład, utworzyć
 pulę z dwuurządzeniowym mirrorem i trzyurządzeniowym RAID-Z. Odporność na 
awarie jest jednak tak dobra, jak dobre jest najgorsze urządzenie wirtualne, w 
tym przypadku RAID-Z. Zalecaną praktyką jest stosowanie takich samych urządzeń 
na tym samym poziomie redundancji w każdym urządzeniu.</para>
</sect2>
</sect1>
<sect1 xml:id="gaypw">
<title>Tworzenie i usuwanie pul nośników danych ZFS</title>
<para>The following sections describe different scenarios for creating and
destroying ZFS storage pools.</para>
<itemizedlist>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gaynr">Creating a ZFS Storage Pool</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazhs">Handling ZFS Storage Pool Creation Errors</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gammr">Destroying ZFS Storage Pools</olink>
</para>
</listitem>
</itemizedlist>
<para>Zgodnie z założeniami projektowymi, tworzenie i usuwanie pul jest proste i
 szybkie. Należy jednak zachować ostrożność przy wykonywaniu tych operacji. 
Wprawdzie ZFS stara się uniemożliwić włączenie już używanych urządzeń do nowej 
puli, nie zawsze jednak może wiedzieć, kiedy urządzenie jest już wykorzystywane 
w jakiejś puli. Usuwanie puli jest jeszcze prostsze. Komendy <command>zpool 
destroy</command> należy używać ostrożnie. To prosta komenda o poważnych 
konsekwencjach.<indexterm xml:id="indexterm-106">
<primary>tworzenie</primary>
<secondary>pul nośników danych ZFS</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-107">
<primary>usuwanie</primary>
<secondary>pul nośników danych ZFS</secondary>
<tertiary>opis</tertiary>
</indexterm>
</para>
<sect2 xml:id="gaynr">
<title>Tworzenie puli nośników danych ZFS</title>
<para>Do tworzenie puli nośników danych służy komenda <command>zpool create
</command>. Argumentami są: nazwa puli i dowolna liczba urządzeń wirtualnych. 
Nazwa puli musi spełniać wymogi nazewnicze wyjaśnione w 
<olink remap="external" targetdoc="chapter-1.xml" targetptr="gbcpt">Wymogi nazewnicze 
komponentów ZFS</olink>.<indexterm xml:id="indexterm-108">
<primary>tworzenie</primary>
<secondary>Puli nośników danych ZFS (<command>zpool create</command>)
</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-109">
<primary>
<command>zpool create</command>
</primary>
<secondary>prosta pula</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-110">
<primary>Pule nośników danych ZFS</primary>
<secondary>tworzenie (<command>zpool create</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<sect3 xml:id="gazgt">
<title>Tworzenie prostej puli nośników danych</title>
<para>Poniższa komenda tworzy nową pulę o nazwie <filename>tank</filename> 
składającą się z dysków <filename>c1t0d0</filename> i <filename>c1t1d0
</filename>:</para>
<screen># <userinput>zpool create tank c1t0d0 c1t1d0</userinput>
</screen>
<para>Dyski te znajdują się w katalogu <filename>/dev/dsk</filename> i zostaną 
zaetykietowane przez ZFS tak, aby zawierały jeden duży blok. Dane będą 
dynamicznie stripowane po wszystkich dyskach.</para>
</sect3>
<sect3 xml:id="gazhv">
<title>Tworzenie lustrzanej konfiguracji puli nośników danych</title>
<para>Do stworzenia lustrzanej puli służy słowo kluczowe <literal>mirror
</literal>, po którym następuje dowolna liczba dysków, z których będzie się 
składało lustro. Poprzez powtarzanie słowa kluczowego <literal>mirror</literal> 
można stworzyć wiele luster. Poniższa komenda tworzy pulę z dwoma 
dwuurządzeniowymi lustrami:</para>
<screen># <userinput>zpool create tank mirror c1d0 c2d0 mirror c3d0 c4d0
</userinput>
</screen>
<para>Drugie słowo kluczowe <literal>mirror</literal> wskazuje, że konfigurowane
 jest nowe urządzenie wirtualne najwyższej warstwy. Dane są dynamicznie 
stripowane na oba lustra z redundancją pomiędzy odpowiednimi dyskami.
<indexterm xml:id="indexterm-111">
<primary>tworzenie</primary>
<secondary>lustrzanej puli nośników danych ZFS (<command>zpool create</command>)
</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-112">
<primary>
<command>zpool create</command>
</primary>
<secondary>lustrzana konfiguracja puli nośników danych</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-113">
<primary>lustrzana konfiguracja puli nośników danych (<command>zpool create
</command>)</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-114">
<primary>Pule nośników danych ZFS</primary>
<secondary>tworzenie konfiguracji lustrzanej (<command>zpool create</command>)
</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<para>Currently, the following operations
are supported on a ZFS mirrored configuration:</para>
<itemizedlist>
<listitem>
<para>Adding another set of disks for an additional top-level <literal>vdev</literal> to an existing mirrored configuration. For more information,
see <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazgw">Adding Devices to a Storage Pool</olink>.</para>
</listitem>
<listitem>
<para>Attaching additional disks to an existing mirrored configuration.
Or, attaching additional disks to a non-replicated configuration to create
a mirrored configuration. For more information, see <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gcfhe">Attaching and Detaching Devices in a Storage Pool</olink>.</para>
</listitem>
<listitem>
<para>Replace a disk or disks in an existing mirrored configuration
as long as the replacement disks are greater than or equal to the device to
be replaced. For more information, see <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazgd">Replacing Devices in a Storage Pool</olink>.</para>
</listitem>
<listitem>
<para>Detach a disk or disk in a mirrored configuration as long
as the remaining devices provide adequate redundancy for the configuration.
For more information, see <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gcfhe">Attaching and Detaching Devices in a Storage Pool</olink>.</para>
</listitem>
</itemizedlist>
<para>Currently, the following operations are not supported
on a mirrored configuration:</para>
<itemizedlist>
<listitem>
<para>You cannot outright remove a device from a mirrored storage
pool. An RFE is filed for this feature.</para>
</listitem>
<listitem>
<para>You cannot split or break a mirror for backup purposes. An
RFE is filed for this feature.</para>
</listitem>
</itemizedlist>
</sect3>
<sect3 xml:id="gcvjg">
<title>Tworzenie pul nośników danych RAID-Z</title>
<para>Tworzenie puli RAID-Z z pojedynczą kontrolą parzystości jest identyczne jak tworzenie puli lustrzanej, z 
wyjątkiem słowa kluczowego <literal>raidz</literal> lub <literal>raidz1</literal> zamiast <literal>mirror
</literal>. Poniższy przykład pokazuje, jak stworzyć pulę z pojedynczym 
urządzeniem RAID-Z składającym się z pięciu dysków:
<indexterm xml:id="indexterm-115">
<primary>tworzenie</primary>
<secondary>pule nośników danych RAID-Z z pojedynczą parzystością(<command>zpool create</command>)
</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-116">
<primary>konfiguracja RAID-Z</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-117">
<primary>
<command>zpool create</command>
</primary>
<secondary>pula nośników danych RAID-Z</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-118">
<primary>Pule nośników danych ZFS</primary>
<secondary>tworzenie konfiguracji RAID-Z (<command>zpool create</command>)
</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool create tank raidz c1t0d0 c2t0d0 c3t0d0 c4t0d0 /dev/dsk/c5t0d0</userinput>
</screen>
<para>Przykład ten pokazuje, że dyski można wskazywać ich pełnymi ścieżkami. 
Urządzenie <filename>/dev/dsk/c5t0d0</filename> jest identyczne z urządzeniem 
<filename>c5t0d0</filename>.</para>
<para>Podobna konfiguracja może zostać stworzona przy pomocy bloków dysków. Na 
przykład:</para>
<screen># <userinput>zpool create tank raidz c1t0d0s0 c2t0d0s0 c3t0d0s0 c4t0d0s0 c5t0d0s0</userinput>
</screen>
<para>Dyski te muszą jednak być odpowiednio sformatowane, aby mieć właściwą 
wielkość bloku zero.</para>
<para>You can create a double-parity RAID-Z
configuration by using the <literal>raidz2</literal> keyword when the pool
is created. For example:<indexterm xml:id="indexterm-119">
<primary>creating</primary>
<secondary>double-parity RAID-Z storage pool (<command>zpool create</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool create tank raidz2 c1t0d0 c2t0d0 c3t0d0</userinput>
# <userinput>zpool status -v tank</userinput>
  pool: tank
 state: ONLINE
 scrub: none requested
config:

        NAME          STATE     READ WRITE CKSUM
        tank          ONLINE       0     0     0
          raidz2      ONLINE       0     0     0
            c1t0d0    ONLINE       0     0     0
            c2t0d0    ONLINE       0     0     0
            c3t0d0    ONLINE       0     0     0

errors: No known data errors</screen>
<para>Currently, the following operations
are supported on a ZFS RAID-Z configuration:</para>
<itemizedlist>
<listitem>
<para>Add another set of disks for an additional top-level <literal>vdev</literal> to an existing RAID-Z configuration. For more information, see <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazgw">Adding Devices to a Storage Pool</olink>.</para>
</listitem>
<listitem>
<para>Replace a disk or disks in an existing RAID-Z configuration
as long as the replacement disks are greater than or equal to the device to
be replaced. For more information, see <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazgd">Replacing Devices in a Storage Pool</olink>.</para>
</listitem>
</itemizedlist>
<para>Currently, the following operations are not supported
on a RAID-Z configuration:</para>
<itemizedlist>
<listitem>
<para>Attach an additional disk to an existing RAID-Z configuration.</para>
</listitem>
<listitem>
<para>Detach a disk from a RAID-Z configuration.</para>
</listitem>
<listitem>
<para>You cannot outright remove a device from a RAID-Z configuration.
An RFE is filed for this feature.</para>
</listitem>
</itemizedlist>
<para>Więcej informacji o konfiguracji RAID-Z w <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gamtu">Konfiguracji RAID-Z pul nośników danych</olink>.</para>
</sect3>
</sect2>
<sect2 xml:id="gazhs">
<title>Obsługa błędów przy tworzeniu pul nośników danych ZFS</title>
<para>Błędy przy tworzeniu puli nośników mogą wystąpić z wielu powodów. Część z 
nich jest oczywista, na przykład brak urządzenia, jednak inne błędy są bardziej 
subtelne.</para>
<sect3 xml:id="gazht">
<title>Wykrywanie używanych urządzeń</title>
<para>Przed sformatowaniem dysku, ZFS sprawdza, czy dysk jest używany przez ZFS 
lub inną część systemu operacyjnego. Jeśli dysk jest używany, może wystąpić błąd
 podobny do poniższego:</para>
<screen># <userinput>zpool create tank c1t0d0 c1t1d0</userinput>
invalid vdev specification
use '-f' to override the following errors:
/dev/dsk/c1t0d0s0 is currently mounted on /. Please see umount(1M).
/dev/dsk/c1t0d0s1 is currently mounted on swap. Please see swap(1M).
/dev/dsk/c1t1d0s0 is part of active ZFS pool zeepool. Please see zpool(1M).</screen>
<para>Niektóre z tych błędów można obejść przy użyciu opcji <option>
f</option>, ale większości się nie da. Poniższych użyć nie uda się obejść za 
pomocą opcji <option>
f</option> i należy usunąć je ręcznie:<indexterm xml:id="indexterm-120">
<primary>wykrywanie</primary>
<secondary>używanych urządzeń</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-121">
<primary>używane urządzenia</primary>
<secondary>wykrywanie</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<variablelist>
<varlistentry>
<term>
<emphasis role="strong">Zamontowane systemy plików</emphasis>
</term>
<listitem>
<para>Dysk lub jeden z bloków zawiera system plików zamontowany w systemie. Błąd
 ten należy poprawić komendą <command>umount</command>.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong">System plików w /etc/vfstab</emphasis>
</term>
<listitem>
<para>Dysk zawiera system plików obecny w pliku <filename>/etc/vfstab</filename>
, ale nie jest on zamontowany. Błąd ten można poprawić usuwając lub oznaczając 
jako komentarz linię w pliku <filename>/etc/vfstab</filename>.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong">Urządzenie przeznaczone na zrzuty pamięci (dump)
</emphasis>
</term>
<listitem>
<para>Dysk jest używany jako urządzenie przeznaczone na zrzuty pamięci (dump). 
Błąd ten można poprawić komendą <command>dumpadm</command>.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong">Część puli ZFS</emphasis>
</term>
<listitem>
<para>Dysk lub plik jest częścią puli ZFS. Błąd ten można naprawić niszcząc pulę
 przy użyciu komendy <command>zpool</command>.</para>
</listitem>
</varlistentry>
</variablelist>
<para>Następujące testy na użycie urządzeń służą tylko jako pomocne ostrzeżenia 
i można je obejść opcją <option>
f</option> przy tworzeniu puli:</para>
<variablelist>
<varlistentry>
<term>
<emphasis role="strong">Zawiera system plików</emphasis>
</term>
<listitem>
<para>Dysk zawiera system plików, ale nie jest zamontowany i wydaje się być 
nieużywany.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong">Część woluminu</emphasis>
</term>
<listitem>
<para>Dysk jest częścią woluminu SVM.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong">Live upgrade</emphasis>
</term>
<listitem>
<para>Dysk jest używany jako alternatywne otoczenie startowe dla Solaris Live 
Upgrade.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong">Część wyeksportowanej puli ZFS</emphasis>
</term>
<listitem>
<para>Dysk jest częścią puli nośników wyeksportowanej lub ręcznie usuniętej z 
systemu. W drugim przypadku pulę przedstawia się jako <literal>potencjalnie 
aktywną</literal>, ponieważ dysk może, ale nie musi być dyskiem podłączonym 
przez sieć i używanym przez inny system. Obchodzenie potencjalnie aktywnej puli 
należy wykonywać bardzo ostrożnie.</para>
</listitem>
</varlistentry>
</variablelist>
<para>Poniższy przykład demonstruje użycie opcji <option>f</option>:</para>
<screen># <userinput>zpool create tank c1t0d0</userinput>
invalid vdev specification
use '-f' to override the following errors:
/dev/dsk/c1t0d0s0 contains a ufs filesystem.
# <userinput>zpool create -f tank c1t0d0</userinput>
</screen>
<para>Najlepiej jest poprawiać błędy, niż je obchodzić przy pomocy opcji 
<option>f</option>.</para>
</sect3>
<sect3 xml:id="gazgc">
<title>Niepasujące poziomy replikacji</title>
<para>Tworzenie pul z urządzeniami wirtualnymi o różnych poziomach replikacji 
jest niezalecane. Komenda <command>zpool</command> próbuje zapobiec utworzeniu 
puli z niepasującymi poziomami redundancji. Przy próbie utworzenia takiej 
konfiguracji puli, pojawią się błędy podobne do poniższych:
<indexterm xml:id="indexterm-122">
<primary>wykrywanie</primary>
<secondary>niepasujących poziomów replikacji</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-123">
<primary>niepasujące poziomy replikacji</primary>
<secondary>wykrywanie</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool create tank c1t0d0 mirror c2t0d0 c3t0d0</userinput>
invalid vdev specification 
use '-f' to override the following errors:
mismatched replication level: both disk and mirror vdevs are present
# <userinput>zpool create tank mirror c1t0d0 c2t0d0 mirror c3t0d0 c4t0d0 c5t0d0</userinput>
invalid vdev specification
use '-f' to override the following errors:
mismatched replication level: 2-way mirror and 3-way mirror vdevs are present
</screen>
<para>Błędy te można obejść za pomocą opcji <option>f</option>, ale postępowanie
 takie jest niezalecane. Polecenie ostrzeże również o tworzeniu puli mirror lub 
RAID-Z z użyciem urządzeń o różnych rozmiarach. Konfiguracja taka jest wprawdzie
 dozwolona, niepasujące poziomy redundancji powodują marnowanie przestrzeni na 
większym urządzeniu i wymaga użycia opcji <option>f</option>, aby wyłączyć 
ostrzeżenie.</para>
</sect3>
<sect3 xml:id="gazhd">
<title>Suchy przebieg tworzenia puli nośników danych</title>
<para>Tworzenie puli może się niespodziewanie nie udać na wiele sposobów, a 
formatowanie dysków może być szkodliwą czynnością, polecenie <command>zpool
create</command> ma dodatkową opcję <option>
n</option>, symulującą tworzenie puli bez zapisu danych na dysku. Opcja ta 
wykonuje testy na użyciu urządzeń i poprawność poziomów replikacji i informuje o
 wszelkich błędach. Jeśli nie ma błędów, pojawi się wynik podobny do poniższego:
<indexterm xml:id="indexterm-124">
<primary>suchy przebieg</primary>
<secondary>tworzenie puli nośników danych ZFS (<command>zpool create</command> 
<option>n</option>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-125">
<primary>
<command>zpool create</command> <option>n</option>
</primary>
<secondary>suchy przebieg</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-126">
<primary>Pule nośników danych ZFS</primary>
<secondary>wykonywanie suchego przebiegu (<command>zpool create</command> 
<option>n</option>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool create -n tank mirror c1t0d0 c1t1d0</userinput>
would create 'tank' with the following layout:

        tank
          mirror
            c1t0d0
            c1t1d0</screen>
<para>Niektórych błędów nie można wykryć bez rzeczywistego tworzenia puli. 
Najczęstszym przykładem jest dwukrotne wskazanie tego samego urządzenia 
dwukrotnie w tej samej konfiguracji. Błędu tego nie można wykryć bez próby 
zapisu danych, dlatego komenda <command>create -n</command> może poinformować o 
sukcesie, ale faktyczne utworzenie puli może się nie udać.</para>
</sect3>
<sect3 xml:id="gbeef">
<title>Domyślny punkt montowania pul</title>
<para>Po utworzeniu puli, domyślnym punktem montowania datasetu jest katalog 
<replaceable>/nazwa-puli</replaceable>. Tego katalogu nie powinno być, a jeśli 
jest, powinien być pusty. Jeśli nie istnieje, jest tworzony automatycznie. 
Jeśli istnieje, dataset jest automatycznie montowany w istniejącym katalogu. 
Aby utworzyć pulę z innym domyślnym punktem montowania należy użyć opcji 
<option>m</option> przy wykonywaniu komendy <command>zpool create</command>:
<indexterm xml:id="indexterm-127">
<primary>punkt montowania</primary>
<secondary>domyślny dla puli nośników danych ZFS</secondary>
</indexterm>
<indexterm xml:id="indexterm-128">
<primary>pule nośników danych ZFS</primary>
<secondary>domyślny punkt montowania</secondary>
</indexterm>
</para>
<screen># <userinput>zpool create home c1t0d0</userinput>
default mountpoint '/home' exists and is not empty
use '-m' option to specify a different default
# <userinput>zpool create -m /export/zfs home c1t0d0</userinput>
</screen>
<screen># <userinput>zpool create home c1t0d0</userinput>
default mountpoint '/home' exists and is not empty
use '-m' option to provide a different default
# <userinput>zpool create -m /export/zfs home c1t0d0</userinput>
</screen>
<para>Komenda ta tworzy nową pulę <literal>home</literal> i dataset <literal>
home</literal> z punktem montowania w <filename>/export/zfs</filename>.</para>
<para>Więcej informacji o punktach montowania w 
<olink remap="external" targetdoc="chapter-5.xml" targetptr="gaztn">Zarządzaniu punktami 
montowania ZFS</olink>.</para>
</sect3>
</sect2>
<sect2 xml:id="gammr">
<title>Niszczenie puli nośników danych ZFS</title>
<para>Pule niszczy się przy użyciu komendy <command>zpool destroy</command>. 
Niszczy ona pulę nawet wtedy, kiedy zawiera ona datasety.
<indexterm xml:id="indexterm-129">
<primary>niszczenie</primary>
<secondary>pul nośników danych ZFS (<command>zpool destroy</command>)
</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-130">
<primary>
<command>zpool destroy</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-131">
<primary>pule nośników danych ZFS</primary>
<secondary>niszczenie (<command>zpool destroy</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool destroy tank</userinput>
</screen>
<caution>

<para>Przy niszczeniu pul należy zachować daleko idącą ostrożność. Należy 
upewnić się, że niszczona jest właściwa pula i że istnieją kopie zapasowe 
danych. Po przypadkowym zniszczeniu niewłaściwej puli można spróbować ją 
odzyskać. 
Więcej informacji w <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gcfhw">
Odzyskiwaniu zniszczonych pul nośników danych ZFS</olink>.</para>
</caution>
<sect3 xml:id="gazhm">
<title>Niszczenie puli z uszkodzonym urządzeniem</title>
<para>Niszczenie puli wymaga zapisu na dysku danych wskazujących, że pula 
została zniszczona. Dane te zapewniają, że przy imporcie urządzenia nie będą 
przestawiały się jako potencjalna pula. Jeśli jedno lub więcej urządzeń jest 
niedostępnych, nadal można zniszczyć pulę. Dane o usunięciu puli nie będą jednak
 zapisane na tych urządzeniach.</para>
<para>Urządzenia te po naprawieniu będą przedstawiane jako <emphasis>
potencjalnie aktywne</emphasis> przy tworzeniu nowej puli i przedstawią się jako
 prawidłowe urządzenia przy importowaniu puli. Jeśli w puli jest wystarczająco 
uszkodzonych urządzeń aby cała pula była uszkodzona (co oznacza, że urządzenie 
wirtualne najwyższego poziomu jest uszkodzone), wtedy komenda zgłosi błąd i nie
 wykona się bez użycia opcji <option>f</option>. Opcja ta jest konieczna, 
ponieważ nie można odtworzyć puli i nie można ustalić, czy są w niej 
przechowywane dane. Na przykład:</para>
<screen># <userinput>zpool destroy tank</userinput>
cannot destroy 'tank': pool is faulted
use '-f' to force destruction anyway
# <userinput>zpool destroy -f tank</userinput>
</screen>
<para>Więcej informacji o zdrowiu pul i urządzeń w 
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gamno">Określanie zdrowia pul nośników 
danych ZFS</olink>.</para>
<para>Więcej informacji o importowaniu pul w 
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazuf">Importowaniu pul nośników
 danych ZFS</olink>.</para>
</sect3>
</sect2>
</sect1>
<sect1 xml:id="gayrd">
<title>Zarządzanie urządzeniami w pulach nośników danych ZFS</title>
<para>Większość podstawowych informacji dotyczących urządzeń znajduje się w
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gcfog">Komponentach 
puli nośników danych ZFS</olink>. Po utworzeniu puli możliwych jest kilka 
akcji służących zarządzaniu fizycznym urządzeniami w puli.</para>
<itemizedlist>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazgw">Adding Devices to a Storage Pool</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gcfhe">Attaching and Detaching Devices in a Storage Pool</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazgm">Onlining and Offlining Devices in a Storage Pool</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazge">Clearing Storage Pool Devices</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazgd">Replacing Devices in a Storage Pool</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gcvcw">Designating Hot Spares in Your Storage Pool</olink>
</para>
</listitem>
</itemizedlist>
<sect2 xml:id="gazgw">
<title>Dodawanie urządzeń do puli nośników danych</title>
<para>Poprzez dodanie do puli nowego wirtualnego urządzenia najwyższego poziomu,
 możliwe jest dynamiczne powiększenie przestrzeni dostępnej w puli. Przestrzeń 
ta jest automatycznie dostępna dla wszystkich datasetów w puli. Nowe urządzenie 
wirtualne dodaje się do puli za pomocą komendy <command>zpool add</command>.
Na przykład:<indexterm xml:id="indexterm-132">
<primary>dodawanie</primary>
<secondary>urządzeń do puli nośników danych ZFS (<command>zpool add</command>)
</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-133">
<primary>
<command>zpool add</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-134">
<primary>pule nośników danych ZFS</primary>
<secondary>dodawanie urządzeń (<command>zpool add</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool add zeepool mirror c2t1d0 c2t2d0</userinput>
</screen>
<para>Format urządzeń wirtualnych jest taki sam jak w przypadku komendy 
<command>zpool create</command> i dotyczą go te same zasady. Wykonywane są 
testy, czy urządzenia są używane i nie można zmienić poziomu redundancji 
bez użycia opcji <option>f</option>. Komenda umożliwia skorzystanie z opcji 
<option>n</option> umożliwiającej suchy przebieg. Na przykład:</para>
<screen># <userinput>zpool add -n zeepool mirror c3t1d0 c3t2d0</userinput>
would update 'zeepool' to the following configuration:
      zeepool
        mirror
            c1t0d0
            c1t1d0
        mirror
            c2t1d0
            c2t2d0
        mirror
            c3t1d0
            c3t2d0</screen>
<para>Komenda ta dodałaby dwa mirrorowane urządzenia <literal>c3t1d0</literal> 
i <literal>c3t2d0</literal> do istniejącej konfiguracji <filename>zeepool
</filename>.</para>
<para>Więcej informacji o sprawdzaniu urządzeń wirtualnych 
w <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazht">Wykrywaniu 
używanych urządzeń</olink>.</para>
<example xml:id="gevok">

<title>Adding Disks to a RAID-Z Configuration</title>
<para>Additional disks can be added similarly to a RAID-Z configuration. The
following example shows how to convert a storage pool with one RAID–Z
device comprised of 3 disks to a storage pool with two RAID-Z devices comprised
of 3 disks.</para>
<screen># <userinput>zpool status</userinput>
  pool: rpool
 state: ONLINE
 scrub: none requested
config:
        NAME         STATE     READ WRITE CKSUM
        rpool        ONLINE       0     0     0
          raidz1     ONLINE       0     0     0
            c1t2d0   ONLINE       0     0     0
            c1t3d0   ONLINE       0     0     0
            c1t4d0   ONLINE       0     0     0

errors: No known data errors
# <userinput>zpool add rpool raidz c2t2d0 c2t3d0 c2t4d0</userinput>
# <userinput>zpool status</userinput>
  pool: rpool
 state: ONLINE
 scrub: none requested
config:
        NAME         STATE     READ WRITE CKSUM
        rpool        ONLINE       0     0     0
          raidz1     ONLINE       0     0     0
            c1t2d0   ONLINE       0     0     0
            c1t3d0   ONLINE       0     0     0
            c1t4d0   ONLINE       0     0     0
          raidz1     ONLINE       0     0     0
            c1t2d0   ONLINE       0     0     0
            c1t3d0   ONLINE       0     0     0
            c1t4d0   ONLINE       0     0     0

errors: No known data errors</screen>
</example>
</sect2>
<sect2 xml:id="gcfhe">
<title>Dołączanie i odłączanie urządzeń z puli nośników danych</title>
<para>Dodatkowo, oprócz komendy <command>zpool add</command>, istnieje komenda 
<command>zpool attach</command>, pozwalająca na dołączenie urządzenia do 
istniejącego mirrorowanego bądź niemirrorwanego urządzenia.<indexterm xml:id="indexterm-135">
<primary>dołączanie</primary>
<secondary>urządzeń do puli nośników danych ZFS (<command>zpool attach
</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-136">
<primary>
<command>zpool attach</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-137">
<primary>pule nośników danych ZFS</primary>
<secondary>dołączanie urządzeń (<command>zpool attach</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<example xml:id="gevnu">

<title>Przekształcenie podwójnego mirrora w potrójny.</title>
<para>W tym przykładzie pula <literal>zeepool</literal> jest istniejącym podwójnym mirrorem,
który zostanie przekształcony w potrójny mirror poprzez dołączenie nowego urządzenia
<literal>c2t1d0</literal> do istniejącego <literal>c1t1d0</literal>.</para>
<screen># <userinput>zpool status</userinput>
  pool: zeepool
 state: ONLINE
 scrub: none requested
config:
        NAME        STATE     READ WRITE CKSUM
        zeepool     ONLINE       0     0     0
          mirror    ONLINE       0     0     0
            c0t1d0  ONLINE       0     0     0
            c1t1d0  ONLINE       0     0     0
errors: No known data errors
# <userinput>zpool attach zeepool c1t1d0 c2t1d0</userinput>
# <userinput>zpool status</userinput>
  pool: zeepool
 state: ONLINE
 scrub: resilver completed with 0 errors on Fri Jan 12 14:47:36 2007
config:

        NAME        STATE     READ WRITE CKSUM
        zeepool     ONLINE       0     0     0
          mirror    ONLINE       0     0     0
            c0t1d0  ONLINE       0     0     0
            c1t1d0  ONLINE       0     0     0
            c2t1d0  ONLINE       0     0     0</screen>
<para>Jeśli istniejące urządzenie jest podwójnym mirrorem, przyłączenie nowego 
urządzenia spowoduje przekształcenie w potrójny mirror i tak dalej. W każdym 
razie nowe urządzenie natychmiast zaczyna resilverować.</para>
</example>
<example xml:id="gevpf">

<title>Converting a Non-Redundant ZFS Storage Pool to
a Mirrored ZFS Storage Pool</title>
<para>In addition, you can convert a non-redundant storage pool into a redundant
storage pool by using the <command>zpool attach</command> command. For example:</para>
<screen># <userinput>zpool create tank c0t1d0</userinput>
# <userinput>zpool status</userinput>
  pool: tank
 state: ONLINE
 scrub: none requested
config:
        NAME        STATE     READ WRITE CKSUM
        tank        ONLINE       0     0     0
          c0t1d0    ONLINE       0     0     0

errors: No known data errors
# <userinput>zpool attach tank c0t1d0 c1t1d0</userinput>
# <userinput>zpool status</userinput>
  pool: tank
 state: ONLINE
 scrub: resilver completed with 0 errors on Fri Jan 12 14:55:48 2007
config:
        NAME        STATE     READ WRITE CKSUM
        tank        ONLINE       0     0     0
          mirror    ONLINE       0     0     0
            c0t1d0  ONLINE       0     0     0
            c1t1d0  ONLINE       0     0     0</screen>
</example>
<para>Do odłączenia urządzenia z mirrorowanej puli można użyć komendy <command>zpool detach
</command>. Na przykład:</para>
<screen># <userinput>zpool detach zeepool c2t1d0</userinput>
</screen>
<para>ZFS nie wykona tej operacji, jeśli nie istnieje żadna prawidłowa kopia 
danych. Na przykład:<indexterm xml:id="indexterm-138">
<primary>odłączania</primary>
<secondary>urządzeń z puli nośników danych ZFS (<command>zpool detach</command>)
</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-139">
<primary>
<command>zpool detach</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-140">
<primary>pule nośników danych ZFS</primary>
<secondary>odłączanie urządzeń (<command>zpool detach</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool detach newpool c1t2d0</userinput>
cannot detach c1t2d0: only applicable to mirror and replacing vdevs</screen>
</sect2>
<sect2 xml:id="gazgm">
<title>Włączanie i wyłączanie urządzeń w puli nośników danych</title>
<para>ZFS umożliwia włączanie i wyłączanie pojedynczych urządzeń. Jeśli sprzęt 
zachowuje się niestabilnie lub niewłaściwie, ZFS nadal zapisuje i odczytuje zeń 
dane, zakładając, że usterka jest chwilowa. Jeśli tak nie jest, można nakazać 
ZFS ignorować urządzenie przez wyłączenie go. ZFS nie wysyła żadnych zapytań 
do wyłączonego urządzenia.<indexterm xml:id="indexterm-141">
<primary>włączanie i wyłączanie urządzeń</primary>
<secondary>pula nośników danych ZFS</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-142">
<primary>pula nośników danych ZFS</primary>
<secondary>włączanie i wyłączanie urządzeń</secondary>
<tertiary>opis</tertiary>
</indexterm>
</para>
<note>
<para>Nie trzeba wyłączać urządzeń, aby je wymienić.</para>
</note>
<para>Jeśli zachodzi konieczność tymczasowego odłączenia nośników, można 
użyć komendy <command>offline</command>. Na przykład, jeśli zachodzi konieczność
 odłączenia macierzy od układu przełączników Fibre Channel i podłączenia jej 
do innego układu przełączników, można wyłączyć LUN-y z macierzy używanej 
w pulach ZFS. Podłączoną i działającą już na nowych przełącznikach macierz można 
aktywować włączając te same LUN-y. Dane, które pojawiły się w puli w czasie, 
kiedy LUN-y były wyłączone, automatycznie zaczną się resilverować, kiedy tylko 
LUN-y zostaną włączone.</para>
<para>Scenariusz taki jest możliwy przy założeniu, że omawiane systemy widzą 
nośniki danych po podłączeniu ich do nowych przełączników, możliwe że przez 
inne niż poprzednio kontrolery, oraz że pule są skonfigurowane jako RAID-Z 
lub mirror.</para>
<sect3 xml:id="gazfy">
<title>Wyłączanie urządzenia</title>
<para>Urządzenie można wyłączyć używając komendy <command>zpool offline
</command>.
Urządzenie można wskazać przez ścieżkę lub skrót, jeśli jest dyskiem. Na 
przykład:</para>
<screen># <userinput>zpool offline tank c1t0d0</userinput>
bringing device c1t0d0 offline</screen>
<para>Keep the following points in mind when taking a device offline:</para>
<itemizedlist>
<listitem>
<para>Niemożliwe jest takie wyłączanie urządzeń, aby uszkodzić pulę. Nie można 
na przykład wyłączyć dwóch urządzeń w RAID-Z ani wyłączyć wirtualnego urządzenia
 najwyższego poziomu. <indexterm xml:id="indexterm-143">
<primary>wyłączanie urządzenia (<command>zpool offline</command>)</primary>
<secondary>pula nośników danych ZFS</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-144">
<primary>
<command>zpool offline</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-145">
<primary>pule nośników danych ZFS</primary>
<secondary>wyłączanie urządzenia (<command>zpool offline</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool offline tank c1t0d0</userinput>
cannot offline c1t0d0: no valid replicas</screen>
</listitem>
<listitem>
<para>Domyślnie wyłączenie jest trwałe. Urządzenie pozostaje wyłączone po restarcie systemu.</para>
<para>Tymczasowego wyłączenia można dokonać przy użyciu <command>zpool offline</command> <option>
t</option>. Na przykład:</para>
<screen># <userinput>zpool offline -t tank c1t0d0</userinput>
 bringing device 'c1t0d0' offline</screen>
<para>Po restarcie systemu urządzenie to zostanie automatycznie przywrócone do stanu online <literal>ONLINE</literal>.</para>
</listitem>
<listitem>
<para>When a device is taken offline, it is not detached from the
storage pool. If you attempt to use the offlined device in another pool, even
after the original pool is destroyed, you will see a message similar to the
following:</para>
<screen>
<replaceable>device</replaceable> is part of exported or potentially active ZFS <replaceable>pool</replaceable>. Please see zpool(1M)</screen>
<para>If you want to use the offlined device in another storage pool after
destroying the original storage pool, first bring the device back online,
then destroy the original storage pool.</para>
<para>Another way to use a device
from another storage pool if you want to keep the original storage pool is
to replace the existing device in the original storage pool with another comparable
device. For information about replacing devices, see <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazgd">Replacing Devices in a Storage Pool</olink>.</para>
</listitem>
</itemizedlist>
<para>Offlined devices show up in the <literal>OFFLINE</literal> state when
you query pool status. For information about querying pool status, see <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gaynp">Querying ZFS Storage Pool Status</olink>.</para>
<para>For more information on device health, see <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gamno">Determining the Health Status of ZFS Storage Pools</olink>.</para>
</sect3>
<sect3 xml:id="gazgk">
<title>Włączanie urządzenia</title>
<para>Po wyłączeniu urządzenia można je włączyć komendą <command>zpool online</command>:</para>
<screen># <userinput>zpool online tank c1t0d0</userinput>
bringing device c1t0d0 online</screen>
<para>Po włączeniu urządzenia, wszelkie dane, które pojawiły się w puli są synchronizowane z nowym urządzeniem. Nie można używać wyłączania do wymiany dysków. Próba wymiany wyłączonego urządzenia spowoduje oflagowanie go jako uszkodzone przy włączaniu.<indexterm xml:id="indexterm-146">
<primary>włączanie urządzenia</primary>
<secondary>pula nośników danych ZFS (<command>zpool online</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-147">
<primary>
<command>zpool online</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-148">
<primary>pule nośników danych ZFS (<command>zpool online</command>)</primary>
<secondary>włączanie urządzenia</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<para>Przy próbie włączenia uszkodzonego urządzenia, przy użyciu komendy <command>fmd</command> pojawi się komunikat podobny do poniższego:</para>
<screen># <userinput>zpool online tank c1t0d0</userinput>
Bringing device c1t0d0 online
# 
SUNW-MSG-ID: ZFS-8000-D3, TYPE: Fault, VER: 1, SEVERITY: Major
EVENT-TIME: Thu Aug 31 11:13:59 MDT 2006
PLATFORM: SUNW,Ultra-60, CSN: -, HOSTNAME: neo
SOURCE: zfs-diagnosis, REV: 1.0
EVENT-ID: e11d8245-d76a-e152-80c6-e63763ed7e4f
DESC: A ZFS device failed.  Refer to http://sun.com/msg/ZFS-8000-D3 for more information.
AUTO-RESPONSE: No automated response will occur.
IMPACT: Fault tolerance of the pool may be compromised.
REC-ACTION: Run 'zpool status -x' and replace the bad device.</screen>
<para>Więcej informacji o wymianie uszkodzonych urządzeń w <olink remap="external" targetdoc="chapter-9.xml" targetptr="gbbvb">Naprawianiu brakujących urządzeń</olink>.</para>
</sect3>
</sect2>
<sect2 xml:id="gazge">
<title>Czyszczenie pul nośników danych</title>
<para>Jeśli urządzenie zostało wyłączone z powodu uszkodzenia, które powodowało wyświetlanie się błędów w wyniku komendy <command>zpool status</command>, można wyczyścić licznik błędów komendą <command>zpool clear</command>.<indexterm xml:id="indexterm-149">
<primary>czyszczenie</primary>
<secondary>urządzenie w puli nośników danych ZFS (<command>zpool clear</command>)</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-150">
<primary>
<command>zpool clear</command>
</primary>
<secondary>opis</secondary>
</indexterm>
</para>
<para>Komenda ta wydana bez argumentów czyści wszystkie błędy wszystkich urządzeń w puli. Na przykład:</para>
<screen># <userinput>zpool clear tank</userinput>
</screen>
<para>Jeśli jako argumenty podano jedno lub więcej urządzeń, komenda wyczyści błędy powiązane tylko z tymi urządzeniami. Na przykład:<indexterm xml:id="indexterm-151">
<primary>czyszczenie urządzeń</primary>
<secondary>puna nośników danych ZFS</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-152">
<primary>
<command>zpool clear</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-153">
<primary>pule nośników danych ZFS</primary>
<secondary>czyszczenie urządzenia</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool clear tank c1t0d0</userinput>
</screen>
<para>Więcej informacji o czyszczeniu błędów <command>zpool</command> w <olink remap="external" targetdoc="chapter-9.xml" targetptr="gbbzv">Czyszczeniu tymczasowych błędów</olink>.</para>
</sect2>
<sect2 xml:id="gazgd">
<title>Wymiana urządzeń w puli nośników danych</title>
<para>Urządzenie w puli można wymienić komendą <command>zpool
replace</command>.<indexterm xml:id="indexterm-154">
<primary>wymiana</primary>
<secondary>urządzenia (<command>zpool replace</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-155">
<primary>
<command>zpool replace</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-156">
<primary>pule nośników danych ZFS</primary>
<secondary>wymiana urządzenia (<command>zpool replace</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool replace tank c1t1d0 c1t2d0</userinput>
</screen>
<para>W tym przykładzie poprzednie urządzenie, <literal>c1t1d0</literal>, zostało zastąpione urządzeniem <literal>c1t2d0</literal>.</para>
<para>Pojemność urządzenia dołączanego do puli musi być nie mniejsza niż najmniejsza ze wszystkich pojemności urządzeń w konfiguracji mirror lub RAID-Z. Jeśli dołączone urządzenie jest większe, pojemność puli w konfiguracji innej niż RAID-Z i mirror zostanie powiększona po zakończeniu wymiany urządzenia.</para>
<para>Więcej informacji o wymianie urządzeń w <olink remap="external" targetdoc="chapter-9.xml" targetptr="gbbvb">Naprawianiu brakującego urządzenia</olink> i <olink remap="external" targetdoc="chapter-9.xml" targetptr="gbbvf">Naprawianiu uszkodzonego urządzenia</olink>.</para>
</sect2>
<sect2 xml:id="gcvcw">
<title>Designating Hot Spares in Your Storage Pool</title>
<para>The hot spares feature enables you to identify disks that could be used
to replace a failed or faulted device in one or more storage pools. Designating
a device as a <emphasis>hot spare</emphasis> means that the device is not
an active device in a pool, but if an active device in the pool fails, the
hot spare automatically replaces the failed device.<indexterm xml:id="indexterm-157">
<primary>hot spares</primary>
<secondary>creating</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<para>Devices can be designated as hot spares in the following ways:</para>
<itemizedlist>
<listitem>
<para>When the pool is created with the <command>zpool create</command> command</para>
</listitem>
<listitem>
<para>After the pool is created with the <command>zpool add</command> command</para>
</listitem>
<listitem>
<para>Hot spare devices can be shared between multiple pools</para>
</listitem>
</itemizedlist>
<para>Designate devices as hot spares when the pool is created. For example:<indexterm xml:id="indexterm-158">
<primary>hot spares</primary>
<secondary>description of</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool create zeepool mirror c1t1d0 c2t1d0 spare c1t2d0 c2t2d0</userinput>
# <userinput>zpool status zeepool</userinput>
pool: zeepool
 state: ONLINE
 scrub: none requested
config:

        NAME         STATE     READ WRITE CKSUM
        zeepool      ONLINE       0     0     0
          mirror     ONLINE       0     0     0
            c1t1d0   ONLINE       0     0     0
            c2t1d0   ONLINE       0     0     0
        spares
          c1t2d0     AVAIL
          c2t2d0     AVAIL   </screen>
<para>Designate hot spares by adding them to a pool after the pool is created.
For example:</para>
<screen># <userinput>zpool add -f zeepool spare c1t3d0 c2t3d0</userinput>
# <userinput>zpool status zeepool</userinput>
pool: zeepool
 state: ONLINE
 scrub: none requested
config:

        NAME         STATE     READ WRITE CKSUM
        zeepool      ONLINE       0     0     0
          mirror     ONLINE       0     0     0
            c1t1d0   ONLINE       0     0     0
            c2t1d0   ONLINE       0     0     0
        spares
          c1t3d0     AVAIL
          c2t3d0     AVAIL   </screen>
<para>Multiple pools can share devices that are designated as hot spares.
For example:</para>
<screen># <userinput>zpool create zeepool mirror c1t1d0 c2t1d0 spare c1t2d0 c2t2d0</userinput>
# <userinput>zpool create tank raidz c3t1d0 c4t1d0 spare c1t2d0 c2t2d0</userinput>
</screen>
<para>Hot spares can be removed from a storage pool by using the <command>zpool
remove</command> command. For example:</para>
<screen># <userinput>zpool remove zeepool c1t2d0</userinput>
# <userinput>zpool status zeepool</userinput>
pool: zeepool
 state: ONLINE
 scrub: none requested
config:

        NAME         STATE     READ WRITE CKSUM
        zeepool      ONLINE       0     0     0
          mirror     ONLINE       0     0     0
            c1t1d0   ONLINE       0     0     0
            c2t1d0   ONLINE       0     0     0
        spares
          c1t3d0     AVAIL</screen>
<para>A hot spare cannot be removed if it is currently used by the storage
pool.</para>
<para>Keep
the following points in mind when using ZFS hot spares:</para>
<itemizedlist>
<listitem>
<para>Currently, the <command>zpool remove</command> command can
only be used to remove hot spares.</para>
</listitem>
<listitem>
<para>Add a disk as a spare that is equal to or larger than the
size of the largest disk in the pool. Adding a smaller disk as a spare to
a pool is allowed. However, when the smaller spare disk is activated, either
automatically or with the <command>zpool replace</command> command, the operation
fails with an error similar to the following:</para>
<screen>cannot replace <replaceable>disk3</replaceable> with <replaceable>disk4</replaceable>: device is too small</screen>
</listitem>
</itemizedlist>
<sect3 xml:id="gcvdi">
<title>Activating and Deactivating Hot Spares in Your Storage
Pool</title>
<para>Hot spares are activated in the following ways:</para>
<itemizedlist>
<listitem>
<para>Manually replacement – Replace a failed device in a
storage pool with a hot spare by using the <command>zpool replace</command> command.</para>
</listitem>
<listitem>
<para>Automatic replacement – When a fault is received, an
FMA agent examines the pool to see if it has any available hot spares. If
so, it replaces the faulted device with an available spare.</para>
<para>If
a hot spare that is currently in use fails, the agent detaches the spare and
thereby cancels the replacement. The agent then attempts to replace the device
with another hot spare, if one is available. This feature is currently limited
by the fact that the ZFS diagnosis engine only emits faults when a device
disappears from the system. </para>
<para>Currently, no automated response
is available to bring the original device back online. You must explicitly
take one of the actions described in the example below. A future enhancement
will allow ZFS to subscribe to hotplug events and automatically replace the
affected device when it is replaced on the system.</para>
</listitem>
</itemizedlist>
<para>Manually replace a device with a hot spare by using the <command>zpool
replace</command> command. For example:</para>
<screen># <userinput>zpool replace zeepool c2t1d0 c2t3d0</userinput>
# <userinput>zpool status zeepool</userinput>
  pool: zeepool
 state: ONLINE
 scrub: resilver completed with 0 errors on Fri Jun  2 13:44:40 2006
config:

        NAME            STATE     READ WRITE CKSUM
        zeepool         ONLINE       0     0     0
          mirror        ONLINE       0     0     0
            c1t2d0      ONLINE       0     0     0
            spare       ONLINE       0     0     0
              c2t1d0    ONLINE       0     0     0
              c2t3d0    ONLINE       0     0     0
        spares
          c1t3d0        AVAIL
          c2t3d0        INUSE     currently in use

errors: No known data errors</screen>
<para>A faulted device is automatically replaced if a hot spare is available.
For example:</para>
<screen># <userinput>zpool status -x</userinput>
  pool: zeepool
 state: DEGRADED
status: One or more devices could not be opened.  Sufficient replicas exist for
        the pool to continue functioning in a degraded state.
action: Attach the missing device and online it using 'zpool online'.
   see: http://www.sun.com/msg/ZFS-8000-D3
 scrub: resilver completed with 0 errors on Fri Jun  2 13:56:49 2006
config:

        NAME                 STATE     READ WRITE CKSUM
        zeepool              DEGRADED     0     0     0
          mirror             DEGRADED     0     0     0
            c1t2d0           ONLINE       0     0     0
            spare            DEGRADED     0     0     0
              c2t1d0         UNAVAIL      0     0     0  cannot open
              c2t3d0         ONLINE       0     0     0
        spares
          c1t3d0             AVAIL
          c2t3d0             INUSE     currently in use

errors: No known data errors</screen>
<para>Currently, three ways to deactivate hot spares are available:</para>
<itemizedlist>
<listitem>
<para>Canceling the hot spare by removing it from the storage pool</para>
</listitem>
<listitem>
<para>Replacing the original device with a hot spare</para>
</listitem>
<listitem>
<para>Permanently swapping in the hot spare</para>
</listitem>
</itemizedlist>
<para>After the faulted device is replaced, use the <command>zpool detach</command> command
to return the hot spare back to the spare set. For example:</para>
<screen># <userinput>zpool detach zeepool c2t3d0</userinput>
# <userinput>zpool status zeepool</userinput>
  pool: zeepool
 state: ONLINE
 scrub: resilver completed with 0 errors on Fri Jun  2 13:58:35 2006
config:

        NAME               STATE     READ WRITE CKSUM
        zeepool            ONLINE       0     0     0
          mirror           ONLINE       0     0     0
            c1t2d0         ONLINE       0     0     0
            c2t1d0         ONLINE       0     0     0
        spares
          c1t3d0           AVAIL
          c2t3d0           AVAIL

errors: No known data errors</screen>
</sect3>
</sect2>
</sect1>
<sect1 xml:id="gaynp">
<title>Sprawdzanie stanu puli nośników danych ZFS</title>
<para>Komenda <command>zpool list</command> pozwala na sprawdzanie stanu puli na wiele sposobów. Dostępne w ten sposób informacje należą do trzech kategorii: podstawowe użycie, statystyki we/wy i zdrowie urządzeń. Wszystkie trzy są omawiane w poniższym podrozdziale.<indexterm xml:id="indexterm-159">
<primary>lista</primary>
<secondary>pule nośników danych ZFS</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-160">
<primary>
<command>zpool list</command>
</primary>
<secondary>opis</secondary>
</indexterm>
</para>
<itemizedlist>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gamml">Displaying Basic ZFS Storage Pool Information</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gammt">Viewing ZFS Storage Pool I/O Statistics</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gamno">Determining the Health Status of ZFS Storage Pools</olink>
</para>
</listitem>
</itemizedlist>
<sect2 xml:id="gamml">
<title>Wyświetlanie podstawowych informacji o pulach nośników danych ZFS</title>
<para>Podstawowe informacje o pulach można wyświetlić za pomocą komendy <command>zpool list</command>.</para>
<sect3 xml:id="gazij">
<title>Informacje o wszystkich dostępnych pulach</title>
<para>Jeśli nie podano argumentów, komenda wyświetli wszystkie pola o wszystkich pulach w systemie. Na przykład:</para>
<screen># <userinput>zpool list</userinput>
NAME                    SIZE    USED   AVAIL    CAP  HEALTH     ALTROOT
tank                   80.0G   22.3G   47.7G    28%  ONLINE     -
dozer                   1.2T    384G    816G    32%  ONLINE     -</screen>
<para>Powyższy wydruk pokazuje następujące informacje:</para>
<variablelist>
<varlistentry>
<term>
<literal>NAME</literal>
</term>
<listitem>
<para>Nazwa puli.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>SIZE</literal>
</term>
<listitem>
<para>Całkowita pojemność puli, równa sumie pojemności wszystkich urządzeń wirtualnych najwyższego poziomu.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>USED</literal>
</term>
<listitem>
<para>Miejsce alokowane przez wszystkie datasety i wewnętrzne metadane. Należy zauważyć, że wielkości raportowane w tym polu różnią się od wielkości raportowanych na poziomie systemu plików.</para>
<para>Więcej o ustalaniu dostępnej przestrzeni w systemie plików w <olink remap="external" targetdoc="chapter-3.xml" targetptr="gbchp">Obliczaniu miejsca w ZFS</olink>.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>AVAILABLE</literal>
</term>
<listitem>
<para>Wielkość niealokowanej przestrzeni w puli.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>CAPACITY</literal> (<literal>CAP</literal>)</term>
<listitem>
<para>Wielkość używanej przestrzeni, raportowana jako procent całkowitej pojemności.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>HEALTH</literal>
</term>
<listitem>
<para>Obecny poziom zdrowia puli.</para>
<para>Więcej o zdrowiu puli w <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gamno">Określaniu zdrowia pul nośników danych ZFS</olink>.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>ALTROOT</literal>
</term>
<listitem>
<para>Alternatywny główny katalog puli, jeśli jest.</para>
<para>Więcej informacji o alternatywnych głównych katalogach pul w <olink remap="external" targetdoc="chapter-8.xml" targetptr="gbcgl">Alternatywnych głównych katalogach pul ZFS</olink>.</para>
</listitem>
</varlistentry>
</variablelist>
<para>Można również odczytać statystyki konkretnej puli przez podanie jej w argumencie komendy. Na przykład:<indexterm xml:id="indexterm-161">
<primary>lista</primary>
<secondary>pule nośników danych ZFS</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-162">
<primary>
<command>zpool list</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-163">
<primary>pule nośników danych ZFS</primary>
<secondary>lista</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool list tank</userinput>
NAME                    SIZE    USED   AVAIL    CAP  HEALTH     ALTROOT
tank                   80.0G   22.3G   47.7G    28%  ONLINE     -</screen>
</sect3>
<sect3 xml:id="gazil">
<title>Konkretne statystyki puli nośników danych</title>
<para>Konkretne statystyki można odczytać używając opcji <option>
o</option>.
Opcja ta umożliwia tworzenie własnych raportów lub szybkie dostęp do pożądanych informacji. Aby, na przykład, wydrukować tylko nazwę i pojemność pul, należy użyć następującej składni:</para>
<screen># <userinput>zpool list -o name,size</userinput>
NAME                    SIZE
tank                   80.0G
dozer                   1.2T</screen>
<para>Nazwy kolumn odpowiadają właściwościom wydrukowanym w <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazij">Drukowaniu informacji o wszystkich pulach nośników danych</olink>.</para>
</sect3>
<sect3 xml:id="gazje">
<title>Skryptowe przetwarzanie wyjścia pul nośników danych ZFS</title>
<para>Domyślny format wyjścia komendy <command>zpool list</command> jest przede wszystkim czytelny i niełatwo użyć go w skryptach powłoki. Ułatwieniem w zastosowaniach programowych jest użycie opcji <option>
H</option>, która wyłącza drukowanie nazw kolumn i oddzielanie pól tabulatorami zamiast spacji. Aby uzyskać, na przykład, prostą listę nazw wszystkich pul w systemie:<indexterm xml:id="indexterm-164">
<primary>przetwarzanie skryptowe</primary>
<secondary>wyjście pul nośników danych ZFS</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-165">
<primary>
<command>zpool list -Ho name</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-166">
<primary>pule nośników danych ZFS</primary>
<secondary>skryptowe przetwarzanie wyjścia pul nośników danych</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool list -Ho name</userinput>
tank
dozer</screen>
<para>Poniżej jest następny przykład:</para>
<screen># <userinput>zpool list -H -o name,size</userinput>
tank   80.0G
dozer  1.2T</screen>
</sect3>
</sect2>
<sect2 xml:id="gammt">
<title>Podgląd statystyk We/Wy pul nośników danych ZFS</title>
<para>Statystyki We/Wy dla pul lub konkretnych urządzeń wirtualnych można uzyskać za pomocą komendy <command>zpool iostat</command>. Podobnie jak komenda <command>iostat</command>, ta komenda może wydrukować statyczny obraz sumarycznego We/Wy oraz aktualne statystyki dla wybranego okresu. Zwracane są następujące statystyki:<indexterm xml:id="indexterm-1647">
<primary>drukowanie</primary>
<secondary>statystyk We/Wy pul nośników danych ZFS</secondary>
<tertiary>opis</tertiary>
</indexterm>
</para>
<variablelist>
<varlistentry>
<term>
<literal>USED CAPACITY</literal>
</term>
<listitem>
<para>Ilość danych aktualnie przechowywanych w puli lub na urządzeniu. Liczba ta różni się nieznacznie od ilości miejsca faktycznie dostępnej systemom plików ze względu na wewnętrzne szczegóły implementacji.</para>
<para>Więcej informacji o różnicach w pojemności pul i datasetów w <olink remap="external" targetdoc="chapter-3.xml" targetptr="gbchp">Obliczaniu pojemności ZFS</olink>.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>AVAILABLE CAPACITY</literal>
</term>
<listitem>
<para>Ilość miejsca dostępna w puli lub na urządzeniu. Podobnie jak w przypadku pola <literal>used</literal>, różni się nieznacznie od miejsca dostępnego datasetom.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>READ OPERATIONS</literal>
</term>
<listitem>
<para>Liczba operacji odczytania We/Wy wysłanych do puli lub urządzenia, włącznie z żądaniami metadanych.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>WRITE OPERATIONS</literal>
</term>
<listitem>
<para>Liczba operacji zapisu We/Wy wysłanych do puli lub urządzenia.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>READ BANDWIDTH</literal>
</term>
<listitem>
<para>Przepustowość wszystkich operacji odczytu (włącznie z metadanymi) wyrażona w jednostkach na sekundę.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>WRITE BANDWIDTH</literal>
</term>
<listitem>
<para>Przepustowość wszystkich operacji zapisu wyrażona w jednostkach na sekundę.</para>
</listitem>
</varlistentry>
</variablelist>
<sect3 xml:id="gazng">
<title>Drukowanie statystyk dla wszystkich pul</title>
<para>Jeśli nie podano żadnych opcji, komenda <command>zpool iostat</command> drukuje skumulowane statystyki od momentu rozruchu systemu dla wszystkich pul w systemie. Na przykład:<indexterm xml:id="indexterm-168">
<primary>drukowanie</primary>
<secondary>statystyki We/Wy dla pul nośników danych ZFS</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-169">
<primary>
<command>zpool iostat</command>
</primary>
<secondary>pool-wide (przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-170">
<primary>pule nośników danych ZFS</primary>
<secondary>statystyki We/Wy dla pul nośników danych</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool iostat</userinput>
               capacity     operations    bandwidth
pool         used  avail   read  write   read  write
----------  -----  -----  -----  -----  -----  -----
tank         100G  20.0G   1.2M   102K   1.2M  3.45K
dozer       12.3G  67.7G   132K  15.2K  32.1K  1.20K</screen>
<para>Ze względu na fakt, że statystyki te są kumulatywne od momentu rozruchu systemu, przepustowość może wydawać się niska, jeśli pula jest względnie nieaktywna. Bliższe rzeczywistości statystyki można uzyskać wskazując przedział czasu, dla którego mają zostać wydrukowane dane. Na przykład:</para>
<screen># <userinput>zpool iostat tank 2</userinput>
               capacity     operations    bandwidth
pool         used  avail   read  write   read  write
----------  -----  -----  -----  -----  -----  -----
tank         100G  20.0G   1.2M   102K   1.2M  3.45K
tank         100G  20.0G    134      0  1.34K      0
tank         100G  20.0G     94    342  1.06K   4.1M</screen>
<para>W tym przykładzie komenda drukuje statystyki użycia puli <literal>tank</literal> co dwie sekundy, aż do zatrzymania procesu klawiszami Ctrl-C.
Można dodatkowo zdefiniować ilość powtórzeń wykonania komendy za pomocą parametru <literal>count</literal>. Na przykład <command>zpool iostat 2 3</command> wydrukuje podsumowanie trzy razy co dwie sekundy, łącznie sześć sekund. Jeśli w systemie jest tylko jedna pula, statystyki będą drukowane w kolejnych liniach. Jeśli istnieje więcej pul, co każdą iterację dodawane jest linia z myślnikiem, w celu wizualnego oddzielenia każdego wykonania.</para>
</sect3>
<sect3 xml:id="gazne">
<title>Drukowanie statystyk urządzeń wirtualnych</title>
<para>Oprócz statystyk We/Wy dla pul, komenda <command>zpool iostat</command> może wydrukować statystyki dla konkretnych urządzeń wirtualnych. Można nią zidentyfikować zbyt wolne urządzenia lub po prostu obserwować rozłożenie We/Wy generowanego przez ZFS. Dokładny układ urządzenia oraz jego statystyki We/Wy można wydrukować komendą <command>zpool iostat -v</command>.
Na przykład:<indexterm xml:id="indexterm-171">
<primary>drukowanie</primary>
<secondary>Statystyki We/Wy urządzeń wirtualnych w pulach nośników danych ZFS</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-172">
<primary>
<command>zpool iostat -v</command>
</primary>
<secondary>urządzenia wirtualne (przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-173">
<primary>pule nośników danych ZFS</primary>
<secondary>statystyki We/Wy urządzeń wirtualnych</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool iostat -v</userinput>
               capacity     operations    bandwidth
tank         used  avail   read  write   read  write
----------  -----  -----  -----  -----  -----  -----
mirror      20.4G  59.6G      0     22      0  6.00K
  c1t0d0        -      -      1    295  11.2K   148K
  c1t1d0        -      -      1    299  11.2K   148K
----------  -----  -----  -----  -----  -----  -----
total       24.5K   149M      0     22      0  6.00K</screen>
<para>Podczas przeglądania statystyk We/Wy urządzeń wirtualnych należy pamiętać o dwóch rzeczach.</para>
<itemizedlist>
<listitem>
<para>Po pierwsze, zużycie miejsca jest dostępne tylko urządzeniom najwyższego poziomu. Sposób alokacji pomiędzy mirrorami i urządzeniami RAID-Z jest charakterystyczna dla implementacji i trudna do wyrażenia w pojedynczej liczbie.</para>
</listitem>
<listitem>
<para>Po drugie, liczby mogą nie sumować się w sposób, jakiego można się spodziewać. W szczególności operacje na urządzeniach RAID-Z i morrorach nie będą równe. Różnica jest szczególnie zauważalna zaraz po utworzeniu puli, ponieważ większość We/Wy wykonywana jest bezpośrednio na dyskach jako część procesu tworzenia puli, nie jest zaś rejestrowana na poziomie mirrora. Z czasem liczby te powinny znacznie się do siebie zbliżyć, aczkolwiek zepsute, milczące lub wyłączone urządzenia mogą zmienić tę symetrię.</para>
</listitem>
</itemizedlist>
<para>Przy przeglądaniu statystyk urządzeń wirtualnych dostępne są te same opcje (interwał i liczba powtórzeń) co przy przeglądaniu statystyk pul.</para>
</sect3>
</sect2>
<sect2 xml:id="gamno">
<title>Określanie zdrowia pul nośników danych ZFS</title>
<para>ZFS zapewnia zintegrowaną metodę sprawdzania zdrowia pul i urządzeń. Zdrowie puli zależy od zdrowia poszczególnych urządzeń, które się na nią składają. Informacje te można wydrukować komendą <command>zpool status</command>. Dodatkowo potencjalne uszkodzenia pul i urządzeń są raportowane przez <command>fmd</command> i wyświetlane na konsoli systemowej i logowane w pliku <command>/VAT/ADM/messa ges</command>.
W tym podrozdziale przedstawione są sposoby ustalania zdrowia pul i urządzeń. Nie przedstawiamy sposobów radzenia sobie z  uszkodzeniami pul i urządzeń. Informacje o rozwiązywaniu problemów i odzyskiwaniu danych są w <olink remap="external" targetdoc="chapter-9.xml" targetptr="gavwg">Rozdziale 9, Rozwiązywanie problemów i odzyskiwanie danych na ZFS</olink>.<indexterm xml:id="indexterm-174">
<primary>drukowanie</primary>
<secondary>zdrowie pul nośników danych</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-175">
<primary>pule nośników danych ZFS</primary>
<secondary>drukowanie zdrowia</secondary>
</indexterm>
</para>
<para>Każde z urządzeń może mieć jeden z poniższych stanów:</para>
<variablelist>
<varlistentry>
<term>
<literal>ONLINE</literal>
</term>
<listitem>
<para>Urządzenie pracuje normalnie. Mogą pojawiać się chwilowe błędy, urządzenie ogólnie jest sprawne.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>DEGRADED</literal>
</term>
<listitem>
<para>Na urządzeniu wirtualnym zdarzyły się jakieś błędy, ale nadal może funkcjonować. Stan ten pojawia się najczęściej, gdy mirror lub RAID-Z utraciło jedno lub więcej ciągłych urządzeń. Odporność puli na uszkodzenia i błędy może być obniżona, ponieważ po uszkodzeniu następnego urządzenia pula może być trwale uszkodzona.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>FAULTED</literal>
</term>
<listitem>
<para>Urządzenie wirtualne jest niedostępne. Stan ten wskazuje najczęściej na uszkodzenie urządzenia uniemożliwiające korzystanie z niego, na przykład takie, w którym ZFS nie może ani wysyłać ani odczytywać z niego danych. Jeśli urządzenie wirtualne najwyższego poziomu jest w takim stanie, cała pula jest niedostępna.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>OFFLINE</literal>
</term>
<listitem>
<para>Urządzenie zostało ręcznie wyłączone przez administratora.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>UNAVAILABLE</literal>
</term>
<listitem>
<para>Nie można otworzyć urządzenia bądź urządzenia wirtualnego. W niektórych przypadkach pule z urządzeniami w stanie <literal>UNAVAILABLE</literal> oznaczone są jako <literal>DEGRADED</literal>. Jeśli urządzenie wirtualne najwyższego poziomu jest niedostępne, wszystkie urządzenia w puli są niedostępne.</para>
</listitem>
</varlistentry>
</variablelist>
<para>Zdrowie puli określane jest na podstawie zdrowia wszystkich jej urządzeń wirtualnych najwyższego poziomu. Jeśli wszystkie urządzenia wirtualne są w stanie <literal>ONLINE</literal>, to pula również określana jest jako <literal>ONLINE</literal>. Jeśli jedno z urządzeń wirtualnych jest w stanie <literal>DEGRADED</literal> lub <literal>UNAVAILABLE</literal>, to pula jest również w stanie <literal>DEGRADED</literal>. Jeśli urządzenie wirtualne najwyższego poziomu oznaczone jest jako <literal>FAULTED</literal> lub <literal>OFFLINE</literal>, to cała pula jest również oznaczana jako <literal>FAULTED</literal>. Pula w stanie <literal>FAULTED</literal> jest całkowicie niedostępna. Nie można odzyskać żadnych danych dopóki konieczne urządzenia nie zostaną dołączone lub naprawione. Pula w stanie <literal>DEGRADED</literal> nadal działa, ale odpowiedni poziom redundancji danych lub przepustowości może być nieosiągalny.</para>
<sect3 xml:id="gazqw">
<title>Podstawowe dane o zdrowiu pul nośników danych</title>
<para>Najszybszym sposobem wydrukowania przeglądu zdrowia pul jest użycie komendy <command>zpool status</command>:<indexterm xml:id="indexterm-176">
<primary>drukowanie</primary>
<secondary>zdrowie pul nośników danych ZFS</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-177">
<primary>
<command>zpool status -x</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-178">
<primary>pule nośników danych ZFS</primary>
<secondary>drukowanie stanu zdrowia</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool status -x</userinput>
all pools are healthy</screen>
<para>Konkretne pule można sprawdzić przez podanie nazwy puli komendzie. Każda pula, której stan jest inni niż <literal>ONLINE</literal> powinna zostać dokładnie sprawdzona w sposób przedstawiony w następnym podrozdziale.</para>
</sect3>
<sect3 xml:id="gazqf">
<title>Dokładne dane o zdrowiu</title>
<para>Dokładniejsze dane o zdrowiu można uzyskać przy użyciu opcji <option>
v</option>.
Na przykład:</para>
<screen># <userinput>zpool status -v tank</userinput>
  pool: tank
 state: DEGRADED
status: One or more devices could not be opened.  Sufficient replicas exist 
        for the pool to continue functioning in a degraded state.
action: Attach the missing device and online it using 'zpool online'.
   see: http://www.sun.com/msg/ZFS-8000-2Q
 scrub: none requested
config:

        NAME                STATE     READ WRITE CKSUM
        tank                DEGRADED     0     0     0
          mirror            DEGRADED     0     0     0
            c1t0d0          FAULTED      0     0     0  cannot open
            c1t1d0          ONLINE       0     0     0
errors: No known data errors</screen>
<para>Powyższy wynik podaje dokładne powody, dla których pula jest w obecnym stanie, włącznie z czytelnym opisem i odnośnikiem do artykułu w bazie wiedzy dotyczącym danego problemu. Każdy artykuł w bazie wiedzy zawiera najświeższe informacje o najlepszych sposobach naprawiana danego błędu. Przy użyciu dokładnych danych o konfiguracji powinno dać się ustalić, które urządzenie jest uszkodzone i jak naprawić pulę.<indexterm xml:id="indexterm-175">
<primary>drukowanie</primary>
<secondary>szczegółowe informacje o zdrowiu puli nośników danych ZFS</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-176">
<primary>
<command>zpool status -v</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-177">
<primary>pule nośników danych ZFS</primary>
<secondary>drukowanie szczegółowych informacji o zdrowiu</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<para>W powyższym przykładzie uszkodzone urządzenie powinno zostać wymienione. Po wymianie urządzenia należy użyć komendy <command>zpool online</command>, aby włączyć urządzenie. Na przykład:</para>
<screen># <userinput>zpool online tank c1t0d0</userinput>
Bringing device c1t0d0 online
# <userinput>zpool status -x</userinput>
all pools are healthy</screen>
<para>Jeśli w puli jest wyłączone urządzenie, wyjście komendy wskazuje pulę. Na przykład:</para>
<screen># <userinput>zpool status -x</userinput>
  pool: tank
 state: DEGRADED
status: One or more devices could not be opened.  Sufficient replicas exist for
        the pool to continue functioning in a degraded state.
action: Attach the missing device and online it using 'zpool online'.
   see: http://www.sun.com/msg/ZFS-8000-D3
 scrub: resilver completed with 0 errors on Fri Mar 17 14:38:47 2006
config:

        NAME         STATE     READ WRITE CKSUM
        tank         DEGRADED     0     0     0
          mirror     DEGRADED     0     0     0
            c1t0d0   UNAVAIL      0     0     0  cannot open
            c1t1d0   ONLINE       0     0     0</screen>
<para>Kolumny <literal>READ</literal> i <literal>WRITE</literal> podają liczbę błędów We/Wy, które pojawiły się na danych urządzeniu, a kolumna <literal>CKSUM</literal> podaje liczbę błędów sum kontrolnych, które pojawiły się na danych urządzeniu. Obydwa liczniki wskazują prawdopodobnie potencjalne uszkodzenie urządzenia i należy podjąć działania naprawcze. Jeśli wartości różne od zera podawane są dla urządzeń wirtualnych najwyższego poziomu, część danych może być niedostępna. Liczniki błędów rejestrują wszystkie znane błędy danych.</para>
<para>W powyższym przykładzie wyłączone urządzenie nie powoduje żadnych błędów w danych.</para>
<para>Więcej informacji o diagnozowaniu i reperowaniu uszkodzonych pul i danych w <olink targetdoc="" remap="internal" targetptr="gavwg">Rozdziale 9, Rozwiązywanie problemów i odzyskiwanie danych na ZFS</olink>.</para>
</sect3>
</sect2>
</sect1>
<sect1 xml:id="gbchy">
<title>Migracja pul nośników danych ZFS</title>
<para>Zdarza się, że konieczne jest przeniesienie puli między komputerami. Aby to zrobić, urządzenia należy odłączyć od oryginalnego komputera i podłączyć je w docelowym. Można to zrobić przez fizyczne przełączenie okablowania lub przez urządzenia wieloportowe, jak urządzenia w SAN. ZFS umożliwia eksport puli z maszyny i import jej na inną, nawet jeśli komputery mają inny porządek bitów (ang. endianness). Więcej informacji o replikacji lub migracji systemów plików między różnymi pulami nośników danych, które mogą znajdować się na różnych maszynach w <olink targetdoc="" remap="internal" targetptr="gbchx">Zapisywanie i odzyskiwanie danych ZFS</olink>.<indexterm xml:id="indexterm-178">
<primary>migracja pul nośników danych ZFS</primary>
<secondary>opis</secondary>
</indexterm>
<indexterm xml:id="indexterm-179">
<primary>pule nośników danych ZFS</primary>
<secondary>migracja</secondary>
<tertiary>opis</tertiary>
</indexterm>
</para>
<sect2 xml:id="gazre">
<title>Przygotowanie do migracji puli nośników danych ZFS</title>
<para>Pule nośników danych muszą być ręcznie eksportowane, aby zaznaczyć, że są gotowe do migracji. Operacja ta zapisuje wszelkie niezapisane wcześniej dane, nanosi dane wskazujące, że wykonano eksport i usuwa wszelką znajomość puli z systemu.</para>
<para>Jeśli pula nie zostanie wyeksportowana, ale dyski zostaną po prostu usunięte z systemu, wciąż można zaimportować pulę w innym systemie. Należy jednak liczyć się z utratą kilku ostatnich sekund transakcji a pula będzie oznaczona jako uszkodzona na oryginalnym komputerze, ponieważ urządzenia są niedostępne. Domyślnie komputer docelowy odmawia importu puli, która nie była wyeksportowana. Jest to konieczne, aby zapobiec przypadkowemu importowi aktywnej puli składającej się z sieciowych nośników danych, które mogą być używane przez inny system.</para>
</sect2>
<sect2 xml:id="gazqr">
<title>Eksportowanie puli nośników danych ZFS</title>
<para>Eksport puli wykonuje się za pomocą komendy <command>zpool export</command>. Na przykład:</para>
<screen># <userinput>zpool export tank</userinput>
</screen>
<para>Po wykonaniu komendy pula <literal>tank</literal> jest niewidoczna dla systemu. Komenda próbuje odmontować wszelkie systemy plików w puli zamontowane w systemie. Jeśli odmontowanie któregokolwiek się nie uda, można odmontować je siłą przy użyciu opcji <option>f</option>.
Na przykład:</para>
<screen># <userinput>zpool export tank</userinput>
cannot unmount '/export/home/eschrock': Device busy
# <userinput>zpool export -f tank</userinput>
</screen>
<para>Jeśli w trakcie eksportu urządzenia są niedostępne, nie można oznaczyć dysków jako wyeksportowane. Jeśli jeden z tych dysków zostanie później podłączony do systemu bez żadnych działających urządzeń, będzie oznaczony jako potencjalnie aktywny. Jeśli emulowane urządzenia w puli są używane, puli nie można wyeksportować nawet z użyciem opcji <option>
f</option>. Aby wyeksportować pulę z emulowanym urządzeniem, najpierw należy upewnić się, że wszyscy użytkownicy wolumenu już go nie używają.<indexterm xml:id="indexterm-180">
<primary>eksportowanie</primary>
<secondary>puli nośników danych ZFS</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-181">
<primary>
<command>zpool export</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-182">
<primary>pule nośników danych ZFS</primary>
<secondary>eksportowanie</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<para>Więcej informacji o emulowanych wolumenach w <olink targetdoc="" remap="internal" targetptr="gaypf">Emulowanych wolumenach</olink>.</para>
</sect2>
<sect2 xml:id="gazru">
<title>Określanie pul nośników danych gotowych do importu</title>
<para>Po usunięciu puli z systemu (przez wyeksportowanie lub usunięcie dysków), należy podłączyć urządzenia do docelowego systemu. Wprawdzie ZFS potrafi poradzić sobie w niektórych sytuacjach, kiedy tylko część urządzeń jest dostępna, wszystkie urządzenia w puli muszą zostać przeniesione między systemami. Urządzenia nie muszą zostać podłączone z tymi samymi nazwami. ZFS wykrywa wszelkie urządzenia o zmienionych nazwach i poprawia odpowiednio swoją konfigurację. Do wykrywania dostępnych pul służy komenda <command>zpool import</command> bez żadnych opcji. Na przykład:</para>
<screen># <userinput>zpool import</userinput>
  pool: tank
    id: 3778921145927357706
 state: ONLINE
action: The pool can be imported using its name or numeric identifier.
config:

        tank        ONLINE
          mirror    ONLINE
            c1t0d0  ONLINE
            c1t1d0  ONLINE</screen>
<para>W tym przykładzie pula <literal>tank</literal> jest dostępna do zaimportowania w systemie docelowym. Każda pula jest identyfikowana po nazwie oraz po unikalnym identyfikatorze liczbowym. Jeśli dostępnych jest kilka pul o tej samej nazwie, można je rozróżnić po identyfikatorach liczbowych.<indexterm xml:id="indexterm-183">
<primary>identyfikowanie</primary>
<secondary>puli nośników danych ZFS do importu (<command>zpool import -a</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-184">
<primary>
<command>zpool import -a</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-185">
<primary>pule nośników danych ZFS</primary>
<secondary>identyfikowanie do importu (<command>zpool import -a</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<para>Podobnie jak komenda <command>zpool status</command>, polecenie <command>zpool
import</command> odsyła do artykułu w bazie wiedzy zawierającego najaktualniejsze informacje odnośnie procedur naprawiania dla danego problemu. W tym przypadku, użytkownik może zmusić system do zaimportowania puli. Import puli podłączonej przez sieć i wykorzystywanej przez inny system może spowodować uszkodzenia danych i panikę jądra, ponieważ oba systemy będą usiłowały zapisywać na tych samych nośnikach. Jeśli nie wszystkie urządzenia w puli są dostępne, lecz zachowana jest wystarczająca redundancja aby pula była użyteczna, stan jej określany jest jako <literal>DEGRADED</literal>. Na przykład:</para>
<screen># <userinput>zpool import</userinput>
  pool: tank
    id: 3778921145927357706
 state: DEGRADED
status: One or more devices are missing from the system.
action: The pool can be imported despite missing or damaged devices.  The
        fault tolerance of the pool may be compromised if imported.
   see: http://www.sun.com/msg/ZFS-8000-2Q
config:

        tank         DEGRADED
          mirror     DEGRADED
            c1t0d0   UNAVAIL   cannot open
            c1t1d0   ONLINE</screen>
<para>W tym przykładzie pierwszy dysk jest uszkodzony lub w ogóle go nie ma, wciąż jednak można zaimportować pulę, ponieważ mirrorowane dane nadal są dostępne. Jeśli zbyt wiele urządzeń jest niedostępnych lub uszkodzonych, import puli nie powiedzie się. Na przykład:</para>
<screen># <userinput>zpool import</userinput>
  pool: dozer
    id: 12090808386336829175
 state: FAULTED
action: The pool cannot be imported. Attach the missing
        devices and try again.
   see: http://www.sun.com/msg/ZFS-8000-6X
config:
        raidz               FAULTED
          c1t0d0    ONLINE
          c1t1d0    FAULTED
          c1t2d0    ONLINE
          c1t3d0    FAULTED</screen>
<para>W tym przykładzie brakuje dwóch dysków w wirtualnym urządzeniu RAID-Z, co oznacza brak wystarczającej replikacji danych i niemożność odtworzenia puli. W tym przypadku ZFS nie wie, które jeszcze urządzenia są częścią puli, aczkolwiek stara się dostarczyć jak najwięcej informacji. Na przykład:</para>
<screen># <userinput>zpool import</userinput>
pool: dozer
    id: 12090808386336829175
 state: FAULTED
status: One or more devices are missing from the system.
action: The pool cannot be imported. Attach the missing
        devices and try again.
   see: http://www.sun.com/msg/ZFS-8000-6X
config:
        dozer          FAULTED   missing device
          raidz       ONLINE
            c1t0d0    ONLINE
            c1t1d0    ONLINE
            c1t2d0    ONLINE
            c1t3d0    ONLINE
        Additional devices are known to be part of this pool, though their
        exact configuration cannot be determined.</screen>
</sect2>
<sect2 xml:id="gaztk">
<title>Szukanie pul nośników danych ZFS w alternatywnych katalogach</title>
<para>Domyślnie komenda <command>zpool import</command> szuka urządzeń tylko w katalogu <filename>/dev/dsk</filename>. Jeśli urządzenie istnieje w innym katalogu lub używana jest pula zbudowana z plików, należy użyć opcji <option>d</option> w celu przeszukania innych katalogów. Na przykład:</para>
<screen># <userinput>zpool create dozer /file/a /file/b</userinput>
# <userinput>zpool export dozer</userinput>
# <userinput>zpool import</userinput>
no pools available
# <userinput>zpool import -d /file</userinput>
  pool: dozer
    id: 672153753596386982
 state: ONLINE
action: The pool can be imported using its name or numeric identifier.
config:

        dozer       ONLINE
          /file/a   ONLINE
          /file/b   ONLINE
# <userinput>zpool import -d /file dozer</userinput>
</screen>
<para>Jeśli urządzenia znajdują się w kliku katalogach można podać kilka opcji <option>d</option>.<indexterm xml:id="indexterm-186">
<primary>importowanie</primary>
<secondary>puli nośników danych ZFS z alternatywnych katalogów (<command>zpool import -d</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-187">
<primary>
<command>zpool import -d</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-188">
<primary>pule nośników danych ZFS</primary>
<secondary>importowanie z alternatywnych katalogów (<command>zpool import -d</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
</sect2>
<sect2 xml:id="gazuf">
<title>Importowanie pul nośników danych ZFS</title>
<para>Po zidentyfikowaniu puli do importu, można ją zaimportować przez wskazanie nazwy bądź liczbowego identyfikatora komendzie <command>zpool import</command>. Na przykład:</para>
<screen># <userinput>zpool import tank</userinput>
</screen>
<para>Jeśli kilka pul ma tę samą nazwę, można wskazać konkretną za pomocą liczbowego identyfikatora. Na przykład:</para>
<screen># <userinput>zpool import</userinput>
  pool: dozer
    id: 2704475622193776801
 state: ONLINE
action: The pool can be imported using its name or numeric identifier.
config:

        dozer       ONLINE
          c1t9d0    ONLINE

  pool: dozer
    id: 6223921996155991199
 state: ONLINE
action: The pool can be imported using its name or numeric identifier.
config:

        dozer       ONLINE
          c1t8d0    ONLINE
# <userinput>zpool import dozer</userinput>
cannot import 'dozer': more than one matching pool
import by numeric ID instead
# <userinput>zpool import 6223921996155991199</userinput>
</screen>
<para>Jeśli nazwa puli pokrywa się z nazwą już istniejącej puli, można zaimportować ją pod inną nazwą. Na przykład:</para>
<screen># <userinput>zpool import dozer zeepool</userinput>
</screen>
<para>Powyższa komenda zaimportowała pulę wyeksportowaną jako<literal>dozer</literal> przy użyciu nowej nazwy <literal>zeepool</literal>. Jeśli pula nie została czysto wyeksportowana, ZFS będzie wymagał flagi <option>f</option>, aby uniknąć przypadkowego zaimportowania puli, która jest nadal używana przez inny system. Na przykład:</para>
<screen># <userinput>zpool import dozer</userinput>
cannot import 'dozer': pool may be in use on another system
use '-f' to import anyway
# <userinput>zpool import -f dozer</userinput>
</screen>
<para>Pule można też importować z alternatywnym głównym katalogiem przy pomocy opcji <option>R</option> .
Więcej informacji o alternatywnych katalogach głównych w <olink targetdoc="" remap="internal" targetptr="gbcgl">Alternatywne katalogi główne pul nośników danych ZFS</olink>.<indexterm xml:id="indexterm-189">
<primary>importowanie</primary>
<secondary>puli nośników danych ZFS</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-190">
<primary>
<command>zpool import</command> <replaceable>name</replaceable>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-191">
<primary>pule nośników danych ZFS</primary>
<secondary>importowanie</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
</sect2>
<sect2 xml:id="gcfhw">
<title>Odzyskiwanie zniszczonych pul nośników danych ZFS</title>
<para>Zniszczoną pulę można odzyskać za pomocą komendy <command>zpool import</command> <option>
D</option>. Na przykład:</para>
<screen># <userinput>zpool destroy tank</userinput>
# <userinput>zpool import -D</userinput>
pool: tank
    id: 3778921145927357706
 state: ONLINE (DESTROYED)
action: The pool can be imported using its name or numeric identifier.  The
        pool was destroyed, but can be imported using the '-Df' flags.
config:

        tank        ONLINE
          mirror    ONLINE
            c1t0d0  ONLINE
            c1t1d0  ONLINE</screen>
<para>W powyższym wyniku komendy <command>zpool import</command> można zidentyfikować pulę jako zniszczoną ze względu na następującą informację o stanie:</para>
<screen>state: ONLINE (DESTROYED)</screen>
<para>Aby odzyskać zniszczoną pulę, należy wykonać komendę <command>zpool import</command> <option>D</option> i z flagą <option>f</option>.
Na przykład:</para>
<screen># <userinput>zpool import -Df tank</userinput>
# <userinput>zpool status tank</userinput>
  pool: tank
 state: ONLINE
 scrub: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        tank        ONLINE       0     0     0
          mirror    ONLINE       0     0     0
            c1t0d0  ONLINE       0     0     0
            c1t1d0  ONLINE       0     0     0

errors: No known data errors</screen>
<para>Jeśli jedno z urządzeń w zniszczonej puli jest niedostępne lub uszkodzone, odzyskanie zniszczonej puli może być niemożliwe. W takim przypadku należy najpierw zaimportować zdegradowana pulę, a następnie spróbować naprawić uszkodzenie. Na przykład:<indexterm xml:id="indexterm-192">
<primary>odzyskiwanie</primary>
<secondary>zniszczonej puli nośników danych ZFS</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-193">
<primary>
<command>zpool import -D</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-194">
<primary>pule nośników danych ZFS</primary>
<secondary>odzyskiwanie uszkodzonej puli</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool destroy dozer</userinput>
# <userinput>zpool import -D</userinput>
pool: dozer
    id: 
 state: DEGRADED (DESTROYED)
status: One or more devices are missing from the system.
action: The pool can be imported despite missing or damaged devices.  The
        fault tolerance of the pool may be compromised if imported.  The
        pool was destroyed, but can be imported using the '-Df' flags.
   see: http://www.sun.com/msg/ZFS-8000-2Q
config:

        dozer        DEGRADED
           raidz      ONLINE
            c1t0d0    ONLINE
            c1t1d0    ONLINE
            c1t2d0    UNAVAIL  cannot open
            c1t3d0    ONLINE
# <userinput>zpool import -Df dozer</userinput>
# <userinput>zpool status -x</userinput>
  pool: dozer
 state: DEGRADED
status: One or more devices could not be opened.  Sufficient replicas exist for
        the pool to continue functioning in a degraded state.
action: Attach the missing device and online it using 'zpool online'.
   see: http://www.sun.com/msg/ZFS-8000-D3
 scrub: resilver completed with 0 errors on Fri Mar 17 16:11:35 2006
config:

        NAME                     STATE     READ WRITE CKSUM
        dozer                    DEGRADED     0     0     0
          raidz                  ONLINE       0     0     0
            c1t0d0               ONLINE       0     0     0
            c1t1d0               ONLINE       0     0     0
            c1t2d0               UNAVAIL      0     0     0  cannot open
            c1t3d0               ONLINE       0     0     0

errors: No known data errors
# <userinput>zpool online dozer c1t2d0</userinput>
Bringing device c1t2d0 online
# <userinput>zpool status -x</userinput>
all pools are healthy</screen>
</sect2>
<sect2 xml:id="gcikw">
<title>Uaktualnianie pul nośników danych ZFS</title>
<para>W przyszłych wersjach ZFS może okazać się konieczne uaktualnienie pul, aby możliwe było skorzystanie z wszystkich opcji nowej wersji ZFS. 
Komenda <command>zpool upgrade</command> służy do przeprowadzenia tego procesu. Komenda <command>zpool status</command> dodatkowo informuje, jeśli pule pracują pod kontrolą starszej wersji ZFS. Na przykład:<indexterm xml:id="indexterm-195">
<primary>uaktualnianie</primary>
<secondary>pula nośników danych ZFS</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-196">
<primary>
<command>zpool upgrade</command>
</primary>
</indexterm>
<indexterm xml:id="indexterm-197">
<primary>pule nośników danych ZFS</primary>
<secondary>uaktualnianie</secondary>
<tertiary>opis</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool status</userinput>
  pool: test
 state: ONLINE
status: The pool is formatted using an older on-disk format.  The pool can
        still be used, but some features are unavailable.
action: Upgrade the pool using 'zpool upgrade'.  Once this is done, the
        pool will no longer be accessible on older software versions.
 scrub: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        test        ONLINE       0     0     0
          c1t27d0   ONLINE       0     0     0

errors: No known data errors</screen>
<para>W tym wydaniu ZFS wydanie polecenia <command>zpool upgrade</command> do uaktualnienia puli nie powinno być konieczne. Obecnie komenda ta drukuje informacje o wersji ZFS.</para>
<screen># <userinput>zpool upgrade</userinput>
This system is currently running ZFS version 1.

All pools are formatted using this version.</screen>
<para>W przyszłych wydaniach ZFS będzie można użyć poniższej składni do uzyskania dodatkowych informacji o wersji i obsługiwanych wydaniach.</para>
<screen># <userinput>zpool upgrade -v</userinput>
This system is currently running ZFS version 1.

The following versions are supported:

VER  DESCRIPTION
---  --------------------------------------------------------
 1   Initial ZFS version.

For more information on a particular version, including supported releases, see:

http://www.opensolaris.org/os/community/zfs/version/N

Where 'N' is the version number.</screen>
<para>Więcej informacji o procesie uaktualniania puli będzie w przyszłych wydaniach tego dokumentu.</para>
</sect2>
</sect1>
</chapter>
