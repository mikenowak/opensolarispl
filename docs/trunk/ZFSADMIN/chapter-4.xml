<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML//EN" "docbook.dtd"[
	<!ENTITY % xinclude SYSTEM "xinclude.mod">
	%xinclude;
]>

<chapter xmlns="http://docbook.org/ns/docbook" version="5.0" xml:id="gavwn">



<title>Zarządzanie pulami nośników danych ZFS</title>
<toc>
<para>W tym rozdziale opisane jest tworzenie i zarządzanie pulami nośników 
danych ZFS.</para>
<para>Podrozdziały:</para>
<itemizedlist>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gcfog">Składniki puli ZFS
</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gaypw">Tworzenie i usuwanie puli
 ZFS</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gayrd">Zarządzanie urządzeniami 
w puli ZFS</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gaynp">Sprawdzanie statusu puli 
ZFS</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gbchy">Przenoszenie puli ZFS
</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gcikw">Uaktualnianie puli ZFS
</olink>
</para>
</listitem>
</itemizedlist>
</toc>
<sect1 xml:id="gcfog">
<title>Składniki puli nośników danych ZFS</title>
<para>Poniższe paragrafy dostarczają dokładnych informacji o następujących 
komponentach puli nośników danych:</para>
<itemizedlist>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazdp">Używanie dysków w pulach danych ZFS</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazcr">Używanie plików w pulach danych ZFS</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazca">Rozpoznawanie wirtualnych urządzeń w pulach danych ZFS</olink>
</para>
</listitem>
</itemizedlist>
<sect2 xml:id="gazdp">
<title>Używanie dysków w puli nośników danych ZFS</title>
<para>Najbardziej podstawowym elementem puli nośników danych jest fizyczny 
nośnik danych. Może nim być urządzenie blokowe o pojemności nie mniejszej niż 
128 MB. Najczęściej urządzeniem tym jest dysk twardy widoczny dla systemu 
w katalogu <filename>/dev/dsk</filename>.<indexterm xml:id="indexterm-81">
<primary>Pule nośników danych ZFS</primary>
<secondary>komponenty</secondary>
</indexterm>
<indexterm xml:id="indexterm-82">
<primary>komponenty</primary>
<secondary>pul nośników danych ZFS</secondary>
</indexterm>
</para>
<para>Nośnikiem danych może być cały dysk (<filename>c1t0d0</filename>), 
lub pojedynczy blok (ang. slice), (<filename>c0t0d0s7</filename>). Zaleca się 
używania całych dysków, w którym to przypadku dysk nie wymaga specjalnego 
formatowania. ZFS formatuje dysk używają etykiet EFI do przechowywania jednego 
bloku. W takim przypadku tablica partycji wyświetlana przez komendę <command>
format</command> wygląda podobnie:</para>
<screen>Current partition table (original):
Total disk sectors available: 71670953 + 16384 (reserved sectors)

Part      Tag    Flag     First Sector        Size        Last Sector
  0        usr    wm                34      34.18GB         71670953    
  1 unassigned    wm                 0          0              0    
  2 unassigned    wm                 0          0              0    
  3 unassigned    wm                 0          0              0    
  4 unassigned    wm                 0          0              0    
  5 unassigned    wm                 0          0              0    
  6 unassigned    wm                 0          0              0    
  7 unassigned    wm                 0          0              0    
  8   reserved    wm          71670954       8.00MB         71687337</screen>
<para>Aby użyć całego dysku, musi być nazwany zgodnie z konwencją Solarisa, na 
przykład <filename>/dev/dsk/cXtXdXsX</filename>. Niektóre sterowniki od 
niezależnych dostawców używają innych konwencji nazewniczych lub umieszczają 
dyski w innych katalogach, niż <filename>/dev/dsk</filename>. Aby użyć tych 
dysków, należy ręcznie je etykietować i udostępnić blok ZFS-owi.
<indexterm xml:id="indexterm-83">
<primary>Etykiety EFI</primary>
<secondary>opis</secondary>
</indexterm>
<indexterm xml:id="indexterm-84">
<primary>Etykiety EFI</primary>
<secondary>współpraca z ZFS</secondary>
</indexterm>
</para>
<para>ZFS nadaje etykietę EFI w momencie tworzenia puli przy użyciu całego 
dysku. Można też etykietować dyski tradycyjnym Solarisowym VTOC przy tworzeniu 
puli z użyciem bloku.</para>
<para>Bloki powinny być używane tylko w poniższych warunkach:</para>
<itemizedlist>
<listitem>
<para>Nazwa urządzenia jest niestandardowa.</para>
</listitem>
<listitem>
<para>Pojedynczy dysk jest współdzielony między system plików ZFS oraz inny 
system plików, na przykład UFS.</para>
</listitem>
<listitem>
<para>Dysk jest używany na potrzeby swap bądź zrzutów (dump).</para>
</listitem>
</itemizedlist>
<para>Dyski można wskazać zarówno poprzez użycie pełnej ścieżki, na przykład 
<filename>/dev/dsk/c1t0d0</filename>, lub przez nazwę urządzenia znajdującego 
się w katalogu<filename>/dev/dsk</filename>, na przykład <filename>c1t0d0
</filename>. Poniższe są prawidłowymi nazwami dysków:
<indexterm xml:id="indexterm-85">
<primary>pule nośników danych ZFS</primary>
<secondary>używanie całego dysku</secondary>
</indexterm>
<indexterm xml:id="indexterm-86">
<primary>całe dyski</primary>
<secondary>jako komponenty pul nośników danych ZFS</secondary>
</indexterm>
<indexterm xml:id="indexterm-87">
<primary>dyski</primary>
<secondary>jako komponenty pul nośników danych ZFS</secondary>
</indexterm>
</para>
<itemizedlist>
<listitem>
<para>
<filename>c1t0d0</filename>
</para>
</listitem>
<listitem>
<para>
<filename>/dev/dsk/c1t0d0</filename>
</para>
</listitem>
<listitem>
<para>
<filename>c0t0d6s2</filename>
</para>
</listitem>
<listitem>
<para>
<filename>/dev/foo/disk</filename>
</para>
</listitem>
</itemizedlist>
<para>Użycie całych dysków jest najprosztszą metodą tworzenia pul nośników danych ZFS. Konfiguracje ZFS - z punktu widzenia zarządzalności, stabilności i wydajności - stają się bardziej skomplikowane przy tworzeniu pul z bloków, LUN-ów, macierzy sprzętowego RAID-u lub wolumenów eksportowanych przez programowych zarządców wolumenów. Poniższe uwagi mogą pomóc w wyborze konfiguracji ZFS ze sprzętem i programowymi prodktami zarządzania nośnikami danych:</para>
<itemizedlist>
<listitem>
<para>Przy tworzeniu konfiguracji ZFS na LUN-ach ze sprzętowych macierzy RAID-u, należy zrozumieć zależności między opcjami nadmiarowości ZFS a opcjami nadmiarowości macierzy. Niektóre konfiguracje mogą dawać w rezultacie taką samą nadmiarowość i wydajność, inne nie.</para>
</listitem>
<listitem>
<para>Można użyć dla ZFS logicznych woluminów dostarczanych przez programowae produkty do zarządzania nośnikami danych, na przykład <trademark>Solaris</trademark> Volume
Manager (SVM) lub Veritas Volume Manager (VxVM). Konfiguracje takie są jednak niezalecane. ZFS pracuje wprawdzie prawidłowo, ale rezultatem może być nieoptymalna wydajność.</para>
</listitem>
</itemizedlist>
<para>Więcej informacji o zaleceniach dotyczących pul nośników danych na stronie najlepszych praktyk ZFS:</para>
<para>
<link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:type="url" xlink:href="http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide">http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide</link>
</para>
<para>Dyski identyfikuje się zarówno za pomocą ścieżki, jak i ich ID, jeśli 
jest dostępne. Metoda ta pozwala na rekonfigurowanie urządzeń w systemie bez 
uaktualniania stanu ZFS. Jeśli dysk zostanie przeniesiony z kontrolera 1., 
do kontrolera 2., ZFS użyje ID urządzenia do wykrycia go i ustalenia, że powinno
 być teraz używane za pośrednictwem kontrolera 2. ID urządzenia jest unikalne 
dla firmware'u. Wprawdzie mało prawdopodobna jest zmiana ID urządzenia, jednak 
niektóre uaktualnienia firmware'u dokonywały takich zmian. W takiej sytuacji, 
ZFS może użyć urządzenia za pośrednictwem jego ścieżki systemowej i 
automatycznie uaktualnić przechowywane ID urządzenia. Jeśli dokonano zarówno 
zmiany ścieżki jak i ID urządzenia, aby możliwe było użycie puli, należy 
wyeksportować pulę, a później ją zaimportować.</para>
</sect2>
<sect2 xml:id="gazcr">
<title>Używanie plików w pulach ZFS</title>
<para>ZFS umożliwia korzystanie z plików UFS jako urządzeń w puli. Opcja ta jest
 pomyślana głównie w celach testowych i do prostych eksperymentów, nie zaś do 
użycia w środowisku produkcyjnym. Powodem jest to, że <emphasis role="strong">
spójność plików zależy od systemu plików, na których pliki są przechowywane.
</emphasis>. Jeśli zostanie utworzona pula ZFS na bazie plików przechowywanych 
na systemie plików UFS, wtedy pośrednio żąda się gwarancji semantyki 
synchronicznej oraz poprawności od systemu plików UFS.
<indexterm xml:id="indexterm-88">
<primary>Pule nośników danych ZFS</primary>
<secondary>używanie plików</secondary>
</indexterm>
<indexterm xml:id="indexterm-89">
<primary>pliki</primary>
<secondary>jako komponenty pul nośników danych ZFS</secondary>
</indexterm>
</para>
<para>Pliki mogą się jednak okazać przydatne przy pierwszych próbach z ZFS-em, 
bądź przy eksperymentowaniu z bardziej skomplikowanymi układami, kiedy nie ma 
wystarczającej ilości prawdziwych nośników. Wszystkie pliki należy wskazywać za 
pomocą pełnej ścieżki dostępu i muszą mieć przynajmniej 128MB. Jeśli plik 
zostanie przeniesiony bądź zmieniona będzie jego nazwa, pula musi zostać 
wyeksportowana i zaimportowana, ponieważ plikom nie przypisuje się żadnego 
unikalnego ID, po którym mogłyby być znów odnalezione.</para>
</sect2>
<sect2 xml:id="gazca">
<title>Rozpoznawanie urządzeń wirtualne w pulach nośników danych</title>
<para>Każda pula nośników składa się z jednego lub więcej wirtualnych urządzeń. 
<emphasis>Urządzenie wirtualne</emphasis> to wewnętrzna reprezentacja puli 
nośników, opisująca układ fizycznych nośników i ich charakterystyki awaryjności.
 Wirtualne urządzenie jako takie reprezentuje urządzenia dyskowe lub pliki, 
używane do tworzenia puli. <indexterm xml:id="indexterm-90">
<primary>Pule nośników danych ZFS</primary>
<secondary>urządzenia wirtualne</secondary>
</indexterm>
<indexterm xml:id="indexterm-91">
<primary>urządzenia wirtualne</primary>
<secondary>jako komponenty pul nośników danych ZFS</secondary>
</indexterm>
</para>
<para>Dwa urządzenia wirtualne najwyższego poziomu (top level) zapewniają 
redundancję danych: mirror i RAID-Z. Urządzenia te składają się z dysków, 
bloków dysków lub plików. </para>
<para>Dyski, bloki dysków i pliki używane w pulach poza mirrorami i RAID-Z 
same funkcjonują jako urządzenia wirtualne najwyższego poziomu. </para>
<para>Pule nośników danych zawierają zazwyczaj wiele urządzeń wirtualnych 
najwyższego poziomu. ZFS dynamicznie rozdziela dane pomiędzy wszystkie 
urządzenia najwyższego poziomu w puli.</para>
</sect2>
</sect1>
<sect1 xml:id="gcfof">
<title>Opcje replikacji danych w pulach ZFS</title>
<para>ZFS zapewnia redundancję danych, jak i właściwości samonaprawiania
uszkodzonych danych w konfiguracji mirror oraz RAID-Z.<indexterm xml:id="indexterm-92">
<primary>opcje replikacji danych w ZFS</primary>
<secondary>mirror lub RAID-Z</secondary>
</indexterm>
</para>
<itemizedlist>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gamss">Konfiguracja puli nośników danych: mirror</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gamtu">Konfiguracja puli nośników danych RAID-Z</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazch">Samonaprawiające się dane w konfiguracjach z replikacją</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazdd">Dynamiczny striping w pulach danych</olink>
</para>
</listitem>
</itemizedlist>
<sect2 xml:id="gamss">
<title>Konfiguracja puli nośników danych: mirror</title>
<para>Konfiguracja mirror puli nośników danych wymaga użycia 
przynajmniej dwóch dysków, najlepiej na osobnych kontrolerach. W mirrorze można używać wielu dysków. Dodatkowo można stworzyć więcej niż jedno 
lustro w puli. Prosta konfiguracja mirror wygląda podobnie do poniższej:
<indexterm xml:id="indexterm-93">
<primary>konfiguracja lustrzana</primary>
<secondary>opis</secondary>
</indexterm>
<indexterm xml:id="indexterm-94">
<primary>konfiguracja lustrzana</primary>
<secondary>opcja redundancji</secondary>
</indexterm>
<indexterm xml:id="indexterm-95">
<primary>konfiguracja lustrzana</primary>
<secondary>koncepcja</secondary>
</indexterm>
<indexterm xml:id="indexterm-96">
<primary>pule nośników danych ZFS</primary>
<secondary>konfiguracja lustrzana, opis</secondary>
</indexterm>
</para>
<screen>mirror c1t0d0 c2t0d0</screen>
<para>Bardziej skomplikowana konfiguracja będzie podobna do poniższej:</para>
<screen>mirror c1t0d0 c2t0d0 c3t0d0 mirror c4t0d0 c5t0d0 c6t0d0</screen>
<para>Więcej informacji o tworzeniu lustrzanej konfiguracji puli nośników 
danych w <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazhv">tworzeniu 
lustrzanych pul nośników danych</olink>.</para>
</sect2>
<sect2 xml:id="gamtu">
<title>Konfiguracja puli nośników danych RAID-Z</title>
<para>Oprócz mirrora ZFS umożliwia 
konfigurację RAID-Z z pojedynczą lub podwójną kontrolą parzystości.
RAID-Z z pojedynczą kontrolą parzystości jest podobny do RAID-5. RAID-Z
z podwójną kontrolą parzystości jest zbliżony do RAID-6.
<indexterm xml:id="indexterm-97">
<primary>konfiguracja RAID-Z</primary>
<secondary>pojedyncza parzystość, opis</secondary>
</indexterm>
<indexterm xml:id="indexterm-98">
<primary>konfiguracja RAID-Z</primary>
<secondary>podwójna parzystość, opis</secondary>
</indexterm>
<indexterm xml:id="indexterm-99">
<primary>konfiguracja RAID-Z</primary>
<secondary>opcja redundancji</secondary>
</indexterm>
<indexterm xml:id="indexterm-100">
<primary>konfiguracja RAID-Z</primary>
<secondary>koncepcja</secondary>
</indexterm>
<indexterm xml:id="indexterm-101">
<primary>pule nośników danych ZFS</primary>
<secondary>konfiguracja RAID-Z, opis</secondary>
</indexterm>
</para>
<para>Wszystkie tradycyjne algorytmy RAID-5 i podobne (RAID-4. RAID-5. RAID-6, 
RDP i PARZYSTE-NIEPARZYSTE na przykład) obarczone są problemem tak zwanej 
"luki zapisu RAID-5". Jeśli tylko fragment bloku danych RAID-5 zostanie zapisany
 i nastąpi awaria zasilania zanim wszystkie bloki zostaną zapisane na dysk, 
parzystość i dane będą rozsynchronizowane, a przez to bezużyteczne na zawsze, 
chyba że następny zapis pełnego bloku nadpisze ten fragment. W RAID-Z ZFS używa 
bloków o zmiennej długości, zatem każdy zapis jest zapisem pełnego bloku. 
Jest to możliwe tylko dlatego, że ZFS integruje system plików i zarządzanie 
urządzeniami w taki sposób, że metadane systemu plików zawierają wystarczająco 
dużo informacji o modelu redundancji w niższej warstwie, aby poradzić sobie z 
blokami RAID o zmiennej długości. RAID-Z to pierwszy to pierwsze na świecie 
czysto programowe rozwiązanie problemu luki zapisu RAID-5.</para>
<para>Konfiguracja RAID-Z z N dyskami o rozmiarze X z P dyskami przeznaczonymi
na dane parzystości może pomieścić około (N-P)*X bajtów i wytrzymać uszkodzenie
jednego dysku zanim zostanie stracona spójność danych. Wymagane są co najmniej
dwa dyski dla konfiguracji RAID-Z z pojedynczą kontrolą parzystości i przynajmniej
trzy dyski dla konfiguracji RAID-Z z podwójną kontrolą parzystości. Na przykład przy trzech dyskach w konfiguracji RAID-Z z pojedynczą kontrolą parzystości, dane
parzystości zajmują przestrzeń dokładnie odpowiadającą jednemu z trzech dysków.
Poza tym nie jest wymagany żaden specjalny sprzęt do utworzenia konfiguracji
RAID-Z.</para>
<para>Koncepcyjnie, konfiguracja RAID-Z z trzema dyskami wygląda jak poniżej:
</para>
<screen>raidz c1t0d0 c2t0d0 c3t0d0</screen>
<para>Bardziej skomplikowana konfiguracja RAID-Z wygląda podobnie do poniższej:
</para>
<screen remap="wide">raidz c1t0d0 c2t0d0 c3t0d0 c4t0d0 c5t0d0 c6t0d0 c7t0d0 raidz c8t0d0 c9t0d0 c10t0d0 c11t0d0
c12t0d0 c13t0d0 c14t0d0</screen>
<para>Jeśli tworzona jest konfiguracja z wieloma dyskami, jak w powyższym 
przykładzie, konfiguracja RAID-Z z 14 dyskami sprawdza się lepiej jako dwie 
konfiguracje RAID-Z z 7 dyskami każda. Konfiguracje RAID-Z grupami o 
jednocyfrowej liczbie dysków powinny mieć lepszą wydajność.</para>
<para>Więcej informacji o tworzeniu pul RAID-Z w 
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gcvjg">Tworzenie pul RAID-Z
</olink>.</para>
<para>Więcej informacji na temat wyboru pomiędzy konfiguracją
z mirrorem a RAID-Z opartych na rozważaniach na temat wydajności
oraz dostępnego miejsca znajduje się na następującym blogu:</para>
<para>
<link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:type="url" xlink:href="http://blogs.sun.com/roller/page/roch?entry=when_to_and_not_to">http://blogs.sun.com/roller/page/roch?entry=when_to_and_not_to</link>
</para>
<para>Więcej informacji na temat zaleceń dla konfiguracji
puli nośników danych RAID-Z znajduje się na stronie
opisującej najlepsze praktyki stosowania ZFS:</para>
<para>
<link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:type="url" xlink:href="http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide">http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide</link>
</para>
</sect2>
<sect2 xml:id="gazch">
<title>Samonaprawiające się dane w konfiguracjach z redundancją</title>
<para>W konfiguracji lustrzanej lub RAID-Z ZFS zapewnia samonaprawianie się 
danych.</para>
<para>Kiedy wykryty zostanie blok z uszkodzonymi danymi, ZFS nie tylko pobiera 
poprawne dane z innej kopii, ale również naprawia uszkodzone dane zastępując je 
poprawną kopią.<indexterm xml:id="indexterm-102">
<primary>samonaprawiające się dane</primary>
<secondary>opis</secondary>
</indexterm>
</para>
</sect2>
<sect2 xml:id="gazdd">
<title>Dynamiczny striping w pulach danych</title>
<para>Dla każdego urządzenia wirtualnego dodanego do puli, ZFS dynamicznie 
zapisuje dane równomiernie na wszystkich dostępnych dyskach (stripe). Decyzja 
o tym, gdzie zapisać dane podejmowana jest w momencie zapisu, nie pojawiają się 
zatem bloki o stałej wielkości w momencie alokacji.
<indexterm xml:id="indexterm-103">
<primary>dynamiczny striping</primary>
<secondary>opis</secondary>
</indexterm>
<indexterm xml:id="indexterm-104">
<primary>dynamiczny striping</primary>
<secondary>możliwości puli nośników danych</secondary>
</indexterm>
<indexterm xml:id="indexterm-105">
<primary>Pule nośników danych ZFS</primary>
<secondary>dynamiczny striping</secondary>
</indexterm>
</para>
<para>Kiedy urządzenia wirtualne dodawane są do puli. ZFS stopniowo zapisuje 
dane na nowym urządzeniu w celu zapewnienia wydajności i zgodności z regułami 
alokacji przestrzeni dyskowej. Każde urządzenie wirtualne może być także lustrem
 lub urządzeniem RAID-Z zawierającym inne urządzenia dyskowe lub pliki. 
Konfiguracja taka pozwala na elastyczność w kontrolowaniu charakterystyk 
awaryjności w puli. Na przykład, można utworzyć następującą pulę z 4 dysków:
</para>
<itemizedlist>
<listitem>
<para>Cztery dyski używające dynamicznego stripingu</para>
</listitem>
<listitem>
<para>Jedna konfiguracja RAID-Z z czterema urządzeniami</para>
</listitem>
<listitem>
<para>Dwuurządzeniowy mirror z użyciem dynamicznego stripingu</para>
</listitem>
</itemizedlist>
<para>Wprawdzie ZFS umożliwia kombinowanie różnych typów urządzeń wirtualnych w 
jednej puli, jednak praktyka taka jest niezalecana. Można, na przykład, utworzyć
 pulę z dwuurządzeniowym mirrorem i trzyurządzeniowym RAID-Z. Odporność na 
awarie jest jednak tak dobra, jak dobre jest najgorsze urządzenie wirtualne, w 
tym przypadku RAID-Z. Zalecaną praktyką jest stosowanie takich samych urządzeń 
na tym samym poziomie redundancji w każdym urządzeniu.</para>
</sect2>
</sect1>
<sect1 xml:id="gaypw">
<title>Tworzenie i usuwanie pul nośników danych ZFS</title>
<para>Następujące podrozdziały opisują różne scenariusze dla tworzenia i niszczenia
pul nośników danych ZFS.</para>
<itemizedlist>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gaynr">Tworzenie puli nośników danych ZFS</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazhs">Obsługa błędów przy tworzeniu pul nośników danych ZFS</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gammr">Niszczenie puli nośników danych ZFS</olink>
</para>
</listitem>
</itemizedlist>
<para>Zgodnie z założeniami projektowymi, tworzenie i usuwanie pul jest proste i
 szybkie. Należy jednak zachować ostrożność przy wykonywaniu tych operacji. 
Wprawdzie ZFS stara się uniemożliwić włączenie już używanych urządzeń do nowej 
puli, nie zawsze jednak może wiedzieć, kiedy urządzenie jest już wykorzystywane 
w jakiejś puli. Usuwanie puli jest jeszcze prostsze. Komendy <command>zpool 
destroy</command> należy używać ostrożnie. To prosta komenda o poważnych 
konsekwencjach.<indexterm xml:id="indexterm-106">
<primary>tworzenie</primary>
<secondary>pul nośników danych ZFS</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-107">
<primary>usuwanie</primary>
<secondary>pul nośników danych ZFS</secondary>
<tertiary>opis</tertiary>
</indexterm>
</para>
<sect2 xml:id="gaynr">
<title>Tworzenie puli nośników danych ZFS</title>
<para>Do tworzenie puli nośników danych służy komenda <command>zpool create
</command>. Argumentami są: nazwa puli i dowolna liczba urządzeń wirtualnych. 
Nazwa puli musi spełniać wymogi nazewnicze wyjaśnione w 
<olink remap="external" targetdoc="chapter-1.xml" targetptr="gbcpt">Wymogi nazewnicze 
komponentów ZFS</olink>.<indexterm xml:id="indexterm-108">
<primary>tworzenie</primary>
<secondary>Puli nośników danych ZFS (<command>zpool create</command>)
</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-109">
<primary>
<command>zpool create</command>
</primary>
<secondary>prosta pula</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-110">
<primary>Pule nośników danych ZFS</primary>
<secondary>tworzenie (<command>zpool create</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<sect3 xml:id="gazgt">
<title>Tworzenie prostej puli nośników danych</title>
<para>Poniższa komenda tworzy nową pulę o nazwie <filename>tank</filename> 
składającą się z dysków <filename>c1t0d0</filename> i <filename>c1t1d0
</filename>:</para>
<screen># <userinput>zpool create tank c1t0d0 c1t1d0</userinput>
</screen>
<para>Dyski te znajdują się w katalogu <filename>/dev/dsk</filename> i zostaną 
zaetykietowane przez ZFS tak, aby zawierały jeden duży blok. Dane będą 
dynamicznie stripowane po wszystkich dyskach.</para>
</sect3>
<sect3 xml:id="gazhv">
<title>Tworzenie konfiguracji mirror puli nośników danych</title>
<para>Do stworzenia puli mirror służy słowo kluczowe <literal>mirror
</literal>, po którym następuje dowolna liczba dysków, z których będzie się 
składało lustro. Poprzez powtarzanie słowa kluczowego <literal>mirror</literal> 
można stworzyć wiele mirrorów. Poniższa komenda tworzy pulę z dwoma 
dwuurządzeniowymi mirrorami:</para>
<screen># <userinput>zpool create tank mirror c1d0 c2d0 mirror c3d0 c4d0
</userinput>
</screen>
<para>Drugie słowo kluczowe <literal>mirror</literal> wskazuje, że konfigurowane
 jest nowe urządzenie wirtualne najwyższej warstwy. Dane są dynamicznie 
stripowane na oba mirrory z redundancją pomiędzy odpowiednimi dyskami.
<indexterm xml:id="indexterm-111">
<primary>tworzenie</primary>
<secondary>mirrorowanej puli nośników danych ZFS (<command>zpool create</command>)
</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-112">
<primary>
<command>zpool create</command>
</primary>
<secondary>konfiguracja mirror puli nośników danych</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-113">
<primary>konfiguracja mirror puli nośników danych (<command>zpool create
</command>)</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-114">
<primary>Pule nośników danych ZFS</primary>
<secondary>tworzenie konfiguracji mirror (<command>zpool create</command>)
</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<para>Na mirrorze ZFS można obecnie wykonać poniższe operacje:</para>
<itemizedlist>
<listitem>
<para>Dodanie dodatkiwego zestawu dysków do wirtualnego urządzenia najwyższego poziomu w istniejącej konfiguracji mirror. Więcej informacji w <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazgw">Dodawanie urządzeń do puli nośników danych</olink>.</para>
</listitem>
<listitem>
<para>Podłączanie dodatkowych dysków do istniejącego mirrora, lub dołączanie dodatkowych dysków do puli bez redundancji i utworzenie mirrora. Więcej informacji w <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gcfhe">Dołączanie i odłączanie urządzeń w puli nośników danych</olink>.</para>
</listitem>
<listitem>
<para>Podmiana dysku lub dysków w istniejącej konfiguracji mirror, o ile nowe dyski są co najmniej pojemności odłączanych urządzeń. Więcej informacji w <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazgd">Wymiana urządzeń w puli nośników danych</olink>.</para>
</listitem>
<listitem>
<para>Odłączenie dysku lub dysków w konfiguracji mirror, o ile pozostałe dyski zezwalają na wystarczającą nadmiarowość. 
Więcej informacji w <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gcfhe">Dołączanie i odłączanie urządzeń w puli nośników danych</olink>.</para>
</listitem>
</itemizedlist>
<para>Poniższych operacji nie można obecnie wykonać na mirrorze:</para>
<itemizedlist>
<listitem>
<para>Nie można po prostu usunąć urządzenia z mirrora. Istnieje RFE (Request for enhancement) dotyczący takiej możliwości.</para>
</listitem>
<listitem>
<para>Nie można rozdzielić lub rozbić mirrora do celów kopii zapasowych. Istnieje RFE dotyczący takiej możliwości.</para>
</listitem>
</itemizedlist>
</sect3>
<sect3 xml:id="gcvjg">
<title>Tworzenie pul nośników danych RAID-Z</title>
<para>Tworzenie puli RAID-Z z pojedynczą kontrolą parzystości jest identyczne jak tworzenie puli lustrzanej, z 
wyjątkiem słowa kluczowego <literal>raidz</literal> lub <literal>raidz1</literal> zamiast <literal>mirror
</literal>. Poniższy przykład pokazuje, jak stworzyć pulę z pojedynczym 
urządzeniem RAID-Z składającym się z pięciu dysków:
<indexterm xml:id="indexterm-115">
<primary>tworzenie</primary>
<secondary>pule nośników danych RAID-Z z pojedynczą parzystością(<command>zpool create</command>)
</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-116">
<primary>konfiguracja RAID-Z</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-117">
<primary>
<command>zpool create</command>
</primary>
<secondary>pula nośników danych RAID-Z</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-118">
<primary>Pule nośników danych ZFS</primary>
<secondary>tworzenie konfiguracji RAID-Z (<command>zpool create</command>)
</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool create tank raidz c1t0d0 c2t0d0 c3t0d0 c4t0d0 /dev/dsk/c5t0d0</userinput>
</screen>
<para>Przykład ten pokazuje, że dyski można wskazywać ich pełnymi ścieżkami. 
Urządzenie <filename>/dev/dsk/c5t0d0</filename> jest identyczne z urządzeniem 
<filename>c5t0d0</filename>.</para>
<para>Podobna konfiguracja może zostać stworzona przy pomocy bloków dysków. Na 
przykład:</para>
<screen># <userinput>zpool create tank raidz c1t0d0s0 c2t0d0s0 c3t0d0s0 c4t0d0s0 c5t0d0s0</userinput>
</screen>
<para>Dyski te muszą jednak być odpowiednio sformatowane, aby mieć właściwą 
wielkość bloku zero.</para>
<para>Można stworzyć konfigurację RAID-Z z podwójną kontrolą parzystości poprzez
użycie słowa kluczowego <literal>raidz2</literal> w czasie tworzenia puli.
Na przykład:<indexterm xml:id="indexterm-119">
<primary>tworzenie</primary>
<secondary>pul nośników danych RAID-Z podwójnej parzystości (<command>zpool create</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool create tank raidz2 c1t0d0 c2t0d0 c3t0d0</userinput>
# <userinput>zpool status -v tank</userinput>
  pool: tank
 state: ONLINE
 scrub: none requested
config:

        NAME          STATE     READ WRITE CKSUM
        tank          ONLINE       0     0     0
          raidz2      ONLINE       0     0     0
            c1t0d0    ONLINE       0     0     0
            c2t0d0    ONLINE       0     0     0
            c3t0d0    ONLINE       0     0     0

errors: No known data errors</screen>
<para>Aktualnie następujące operacje są dozwolone dla ZFS w konfiguracji
RAID-Z:</para>
<itemizedlist>
<listitem>
<para>Dodawanie kolejnego zestawu dysków jako dodatkowy <literal>vdev</literal> najwyższego poziomu do istniejącej konfiguracji RAID-Z. Więcej informacji w <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazgw">Dodawanie urządzeń do puli nośników danych</olink>.</para>
</listitem>
<listitem>
<para>Zastępowanie dysku lub dysków w istniejącej konfiguracji RAID-Z
tak długo, jak liczba dysków służących do wymiany jest większa lub równa liczbie
urządzeń, które będą wymieniane. Więcej informacji znajduje się w <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazgd">Wymianie urządzeń w puli nośników danych</olink>.</para>
</listitem>
</itemizedlist>
<para>Aktualnie następujące operacje nie są dozwolone przy konfiguracji
RAID-Z:</para>
<itemizedlist>
<listitem>
<para>Dołączanie dodatkowego dysku do istniejącej konfiguracji RAID-Z.</para>
</listitem>
<listitem>
<para>Odłączanie dysku z konfiguracji RAID-Z.</para>
</listitem>
<listitem>
<para>Nie można całkowicie usunąć urządzenia z konfiguracji RAID-Z. Odpowiedni
dokument RFE został wypełniony w celu dodania tej funkcjonalności.</para>
</listitem>
</itemizedlist>
<para>Więcej informacji o konfiguracji RAID-Z w <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gamtu">Konfiguracja pul nośników danych RAID-Z</olink>.</para>
</sect3>
</sect2>
<sect2 xml:id="gazhs">
<title>Obsługa błędów przy tworzeniu pul nośników danych ZFS</title>
<para>Błędy przy tworzeniu puli nośników mogą wystąpić z wielu powodów. Część z 
nich jest oczywista, na przykład brak urządzenia, jednak inne błędy są bardziej 
subtelne.</para>
<sect3 xml:id="gazht">
<title>Wykrywanie używanych urządzeń</title>
<para>Przed sformatowaniem dysku, ZFS sprawdza, czy dysk jest używany przez ZFS 
lub inną część systemu operacyjnego. Jeśli dysk jest używany, może wystąpić błąd
 podobny do poniższego:</para>
<screen># <userinput>zpool create tank c1t0d0 c1t1d0</userinput>
invalid vdev specification
use '-f' to override the following errors:
/dev/dsk/c1t0d0s0 is currently mounted on /. Please see umount(1M).
/dev/dsk/c1t0d0s1 is currently mounted on swap. Please see swap(1M).
/dev/dsk/c1t1d0s0 is part of active ZFS pool zeepool. Please see zpool(1M).</screen>
<para>Niektóre z tych błędów można obejść przy użyciu opcji <option>
f</option>, ale są też takie, których się nie da. Poniższych błędów nie uda się obejść za 
pomocą opcji <option>
f</option> i należy usunąć je ręcznie:<indexterm xml:id="indexterm-120">
<primary>wykrywanie</primary>
<secondary>używanych urządzeń</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-121">
<primary>używane urządzenia</primary>
<secondary>wykrywanie</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<variablelist>
<varlistentry>
<term>
<emphasis role="strong">Zamontowane systemy plików</emphasis>
</term>
<listitem>
<para>Dysk lub jeden z bloków zawiera system plików zamontowany w systemie. Błąd
 ten należy poprawić komendą <command>umount</command>.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong">System plików w /etc/vfstab</emphasis>
</term>
<listitem>
<para>Dysk zawiera system plików obecny w pliku <filename>/etc/vfstab</filename>
, ale nie jest on zamontowany. Błąd ten można poprawić usuwając lub oznaczając 
jako komentarz linię w pliku <filename>/etc/vfstab</filename>.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong">Urządzenie przeznaczone na zrzuty pamięci (dump)
</emphasis>
</term>
<listitem>
<para>Dysk jest używany jako urządzenie przeznaczone na zrzuty pamięci (dump). 
Błąd ten można poprawić komendą <command>dumpadm</command>.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong">Część puli ZFS</emphasis>
</term>
<listitem>
<para>Dysk lub plik jest częścią puli ZFS. Błąd ten można naprawić niszcząc pulę
 przy użyciu komendy <command>zpool</command>.</para>
</listitem>
</varlistentry>
</variablelist>
<para>Następujące testy na użycie urządzeń służą tylko jako pomocne ostrzeżenia 
i można je obejść opcją <option>
f</option> przy tworzeniu puli:</para>
<variablelist>
<varlistentry>
<term>
<emphasis role="strong">Zawiera system plików</emphasis>
</term>
<listitem>
<para>Dysk zawiera system plików, ale nie jest zamontowany i wydaje się być 
nieużywany.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong">Część woluminu</emphasis>
</term>
<listitem>
<para>Dysk jest częścią woluminu SVM.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong">Live upgrade</emphasis>
</term>
<listitem>
<para>Dysk jest używany jako alternatywne otoczenie startowe dla Solaris Live 
Upgrade.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong">Część wyeksportowanej puli ZFS</emphasis>
</term>
<listitem>
<para>Dysk jest częścią puli nośników wyeksportowanej lub ręcznie usuniętej z 
systemu. W drugim przypadku pulę przedstawia się jako <literal>potencjalnie 
aktywną</literal>, ponieważ dysk może, ale nie musi być dyskiem podłączonym 
przez sieć i używanym przez inny system. Obchodzenie potencjalnie aktywnej puli 
należy wykonywać bardzo ostrożnie.</para>
</listitem>
</varlistentry>
</variablelist>
<para>Poniższy przykład demonstruje użycie opcji <option>f</option>:</para>
<screen># <userinput>zpool create tank c1t0d0</userinput>
invalid vdev specification
use '-f' to override the following errors:
/dev/dsk/c1t0d0s0 contains a ufs filesystem.
# <userinput>zpool create -f tank c1t0d0</userinput>
</screen>
<para>Najlepiej jest poprawiać błędy, niż je obchodzić przy pomocy opcji 
<option>f</option>.</para>
</sect3>
<sect3 xml:id="gazgc">
<title>Niepasujące poziomy replikacji</title>
<para>Tworzenie pul z urządzeniami wirtualnymi o różnych poziomach replikacji 
jest niezalecane. Komenda <command>zpool</command> próbuje zapobiec utworzeniu 
puli z niepasującymi poziomami redundancji. Przy próbie utworzenia takiej 
konfiguracji puli, pojawią się błędy podobne do poniższych:
<indexterm xml:id="indexterm-122">
<primary>wykrywanie</primary>
<secondary>niepasujących poziomów replikacji</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-123">
<primary>niepasujące poziomy replikacji</primary>
<secondary>wykrywanie</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool create tank c1t0d0 mirror c2t0d0 c3t0d0</userinput>
invalid vdev specification 
use '-f' to override the following errors:
mismatched replication level: both disk and mirror vdevs are present
# <userinput>zpool create tank mirror c1t0d0 c2t0d0 mirror c3t0d0 c4t0d0 c5t0d0</userinput>
invalid vdev specification
use '-f' to override the following errors:
mismatched replication level: 2-way mirror and 3-way mirror vdevs are present
</screen>
<para>Błędy te można obejść za pomocą opcji <option>f</option>, ale postępowanie
 takie jest niezalecane. Polecenie ostrzeże również o tworzeniu puli mirror lub 
RAID-Z z użyciem urządzeń o różnych rozmiarach. Konfiguracja taka jest wprawdzie
 dozwolona, niepasujące poziomy redundancji powodują marnowanie przestrzeni na 
większym urządzeniu i wymaga użycia opcji <option>f</option>, aby wyłączyć 
ostrzeżenie.</para>
</sect3>
<sect3 xml:id="gazhd">
<title>Suchy przebieg tworzenia puli nośników danych</title>
<para>Tworzenie puli może się niespodziewanie nie udać na wiele sposobów, a 
formatowanie dysków może być szkodliwą czynnością, polecenie <command>zpool
create</command> ma dodatkową opcję <option>
n</option>, symulującą tworzenie puli bez zapisu danych na dysku. Opcja ta 
wykonuje testy na użyciu urządzeń i poprawność poziomów replikacji i informuje o
 wszelkich błędach. Jeśli nie ma błędów, pojawi się wynik podobny do poniższego:
<indexterm xml:id="indexterm-124">
<primary>suchy przebieg</primary>
<secondary>tworzenie puli nośników danych ZFS (<command>zpool create</command> 
<option>n</option>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-125">
<primary>
<command>zpool create</command> <option>n</option>
</primary>
<secondary>suchy przebieg</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-126">
<primary>Pule nośników danych ZFS</primary>
<secondary>wykonywanie suchego przebiegu (<command>zpool create</command> 
<option>n</option>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool create -n tank mirror c1t0d0 c1t1d0</userinput>
would create 'tank' with the following layout:

        tank
          mirror
            c1t0d0
            c1t1d0</screen>
<para>Niektórych błędów nie można wykryć bez rzeczywistego tworzenia puli. 
Najczęstszym przykładem jest dwukrotne wskazanie tego samego urządzenia 
dwukrotnie w tej samej konfiguracji. Błędu tego nie można wykryć bez próby 
zapisu danych, dlatego komenda <command>create -n</command> może poinformować o 
sukcesie, ale faktyczne utworzenie puli może się nie udać.</para>
</sect3>
<sect3 xml:id="gbeef">
<title>Domyślny punkt montowania pul</title>
<para>Po utworzeniu puli, domyślnym punktem montowania datasetu jest katalog 
<replaceable>/nazwa-puli</replaceable>. Tego katalogu nie powinno być, a jeśli 
jest, powinien być pusty. Jeśli nie istnieje, jest tworzony automatycznie. 
Jeśli istnieje, dataset jest automatycznie montowany w istniejącym katalogu. 
Aby utworzyć pulę z innym domyślnym punktem montowania należy użyć opcji 
<option>m</option> przy wykonywaniu komendy <command>zpool create</command>:
<indexterm xml:id="indexterm-127">
<primary>punkt montowania</primary>
<secondary>domyślny dla puli nośników danych ZFS</secondary>
</indexterm>
<indexterm xml:id="indexterm-128">
<primary>pule nośników danych ZFS</primary>
<secondary>domyślny punkt montowania</secondary>
</indexterm>
</para>
<screen># <userinput>zpool create home c1t0d0</userinput>
default mountpoint '/home' exists and is not empty
use '-m' option to specify a different default
# <userinput>zpool create -m /export/zfs home c1t0d0</userinput>
</screen>
<screen># <userinput>zpool create home c1t0d0</userinput>
default mountpoint '/home' exists and is not empty
use '-m' option to provide a different default
# <userinput>zpool create -m /export/zfs home c1t0d0</userinput>
</screen>
<para>Komenda ta tworzy nową pulę <literal>home</literal> i dataset <literal>
home</literal> z punktem montowania w <filename>/export/zfs</filename>.</para>
<para>Więcej informacji o punktach montowania w 
<olink remap="external" targetdoc="chapter-5.xml" targetptr="gaztn">Zarządzanie punktami 
montowania ZFS</olink>.</para>
</sect3>
</sect2>
<sect2 xml:id="gammr">
<title>Niszczenie puli nośników danych ZFS</title>
<para>Pule niszczy się przy użyciu komendy <command>zpool destroy</command>. 
Niszczy ona pulę nawet wtedy, kiedy zawiera ona datasety.
<indexterm xml:id="indexterm-129">
<primary>niszczenie</primary>
<secondary>pul nośników danych ZFS (<command>zpool destroy</command>)
</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-130">
<primary>
<command>zpool destroy</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-131">
<primary>pule nośników danych ZFS</primary>
<secondary>niszczenie (<command>zpool destroy</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool destroy tank</userinput>
</screen>
<caution>

<para>Przy niszczeniu pul należy zachować daleko idącą ostrożność. Należy 
upewnić się, że niszczona jest właściwa pula i że istnieją kopie zapasowe 
danych. Po przypadkowym zniszczeniu niewłaściwej puli można spróbować ją 
odzyskać. 
Więcej informacji w <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gcfhw">
Odzyskiwanie zniszczonych pul nośników danych ZFS</olink>.</para>
</caution>
<sect3 xml:id="gazhm">
<title>Niszczenie puli z uszkodzonym urządzeniem</title>
<para>Niszczenie puli wymaga zapisu na dysku danych wskazujących, że pula 
została zniszczona. Dane te zapewniają, że przy imporcie urządzenia nie będą 
przestawiały się jako potencjalna pula. Jeśli jedno lub więcej urządzeń jest 
niedostępnych, nadal można zniszczyć pulę. Dane o usunięciu puli nie będą jednak
 zapisane na tych urządzeniach.</para>
<para>Urządzenia te po naprawieniu będą przedstawiane jako <emphasis>
potencjalnie aktywne</emphasis> przy tworzeniu nowej puli i przedstawią się jako
 prawidłowe urządzenia przy importowaniu puli. Jeśli w puli jest wystarczająco 
uszkodzonych urządzeń aby cała pula była uszkodzona (co oznacza, że urządzenie 
wirtualne najwyższego poziomu jest uszkodzone), wtedy komenda zgłosi błąd i nie
 wykona się bez użycia opcji <option>f</option>. Opcja ta jest konieczna, 
ponieważ nie można odtworzyć puli i nie można ustalić, czy są w niej 
przechowywane dane. Na przykład:</para>
<screen># <userinput>zpool destroy tank</userinput>
cannot destroy 'tank': pool is faulted
use '-f' to force destruction anyway
# <userinput>zpool destroy -f tank</userinput>
</screen>
<para>Więcej informacji o zdrowiu pul i urządzeń w 
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gamno">Określanie zdrowia pul nośników 
danych ZFS</olink>.</para>
<para>Więcej informacji o importowaniu pul w 
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazuf">Importowanie pul nośników
 danych ZFS</olink>.</para>
</sect3>
</sect2>
</sect1>
<sect1 xml:id="gayrd">
<title>Zarządzanie urządzeniami w pulach nośników danych ZFS</title>
<para>Większość podstawowych informacji dotyczących urządzeń znajduje się w
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gcfog">Komponenty 
puli nośników danych ZFS</olink>. Po utworzeniu puli możliwych jest kilka 
akcji służących zarządzaniu fizycznym urządzeniami w puli.</para>
<itemizedlist>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazgw">Dodawanie urządzeń do puli nośników danych</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gcfhe">Dołączanie i odłączanie urządzeń z puli nośników danych</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazgm">Włączanie i wyłączanie urządzeń w puli nośników danych</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazge">Czyszczenie pul nośników danych</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazgd">Wymiana urządzeń w puli nośników danych</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gcvcw">Wyznaczanie urządzeń "gorącej rezerwy" (hot spare) w puli nośników danych</olink>
</para>
</listitem>
</itemizedlist>
<sect2 xml:id="gazgw">
<title>Dodawanie urządzeń do puli nośników danych</title>
<para>Poprzez dodanie do puli nowego wirtualnego urządzenia najwyższego poziomu,
 możliwe jest dynamiczne powiększenie przestrzeni dostępnej w puli. Przestrzeń 
ta jest automatycznie dostępna dla wszystkich datasetów w puli. Nowe urządzenie 
wirtualne dodaje się do puli za pomocą komendy <command>zpool add</command>.
Na przykład:<indexterm xml:id="indexterm-132">
<primary>dodawanie</primary>
<secondary>urządzeń do puli nośników danych ZFS (<command>zpool add</command>)
</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-133">
<primary>
<command>zpool add</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-134">
<primary>pule nośników danych ZFS</primary>
<secondary>dodawanie urządzeń (<command>zpool add</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool add zeepool mirror c2t1d0 c2t2d0</userinput>
</screen>
<para>Format urządzeń wirtualnych jest taki sam jak w przypadku komendy 
<command>zpool create</command> i dotyczą go te same zasady. Wykonywane są 
testy, czy urządzenia są używane i nie można zmienić poziomu redundancji 
bez użycia opcji <option>f</option>. Komenda umożliwia skorzystanie z opcji 
<option>n</option> umożliwiającej suchy przebieg. Na przykład:</para>
<screen># <userinput>zpool add -n zeepool mirror c3t1d0 c3t2d0</userinput>
would update 'zeepool' to the following configuration:
      zeepool
        mirror
            c1t0d0
            c1t1d0
        mirror
            c2t1d0
            c2t2d0
        mirror
            c3t1d0
            c3t2d0</screen>
<para>Komenda ta dodałaby dwa mirrorowane urządzenia <literal>c3t1d0</literal> 
i <literal>c3t2d0</literal> do istniejącej konfiguracji <filename>zeepool
</filename>.</para>
<para>Więcej informacji o sprawdzaniu urządzeń wirtualnych 
w <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazht">Wykrywaniu 
używanych urządzeń</olink>.</para>
<example xml:id="gevok">

<title>Dodawanie dysków do konfiguracji RAID-Z</title>
<para>Dodatkowe dyski mogą być dodane w podobny do sposób do konfiguracji RAID-Z.
Poniższy przykład pokazuje, jak przekształcić pulę nośników danych z jednym urządzeniem
RAID-Z składającym się z 3 dysków w pulę z dwoma urządzeniami RAID-Z, każde składające
się z 3 dysków.</para>
<screen># <userinput>zpool status</userinput>
  pool: rpool
 state: ONLINE
 scrub: none requested
config:
        NAME         STATE     READ WRITE CKSUM
        rpool        ONLINE       0     0     0
          raidz1     ONLINE       0     0     0
            c1t2d0   ONLINE       0     0     0
            c1t3d0   ONLINE       0     0     0
            c1t4d0   ONLINE       0     0     0

errors: No known data errors
# <userinput>zpool add rpool raidz c2t2d0 c2t3d0 c2t4d0</userinput>
# <userinput>zpool status</userinput>
  pool: rpool
 state: ONLINE
 scrub: none requested
config:
        NAME         STATE     READ WRITE CKSUM
        rpool        ONLINE       0     0     0
          raidz1     ONLINE       0     0     0
            c1t2d0   ONLINE       0     0     0
            c1t3d0   ONLINE       0     0     0
            c1t4d0   ONLINE       0     0     0
          raidz1     ONLINE       0     0     0
            c1t2d0   ONLINE       0     0     0
            c1t3d0   ONLINE       0     0     0
            c1t4d0   ONLINE       0     0     0

errors: No known data errors</screen>
</example>
</sect2>
<sect2 xml:id="gcfhe">
<title>Dołączanie i odłączanie urządzeń z puli nośników danych</title>
<para>Dodatkowo, oprócz komendy <command>zpool add</command>, istnieje komenda 
<command>zpool attach</command>, pozwalająca na dołączenie urządzenia do 
istniejącego mirrorowanego bądź niemirrorwanego urządzenia.<indexterm xml:id="indexterm-135">
<primary>dołączanie</primary>
<secondary>urządzeń do puli nośników danych ZFS (<command>zpool attach
</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-136">
<primary>
<command>zpool attach</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-137">
<primary>pule nośników danych ZFS</primary>
<secondary>dołączanie urządzeń (<command>zpool attach</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<example xml:id="gevnu">

<title>Przekształcenie podwójnego mirrora w potrójny.</title>
<para>W tym przykładzie pula <literal>zeepool</literal> jest istniejącym podwójnym mirrorem,
który zostanie przekształcony w potrójny mirror poprzez dołączenie nowego urządzenia
<literal>c2t1d0</literal> do istniejącego <literal>c1t1d0</literal>.</para>
<screen># <userinput>zpool status</userinput>
  pool: zeepool
 state: ONLINE
 scrub: none requested
config:
        NAME        STATE     READ WRITE CKSUM
        zeepool     ONLINE       0     0     0
          mirror    ONLINE       0     0     0
            c0t1d0  ONLINE       0     0     0
            c1t1d0  ONLINE       0     0     0
errors: No known data errors
# <userinput>zpool attach zeepool c1t1d0 c2t1d0</userinput>
# <userinput>zpool status</userinput>
  pool: zeepool
 state: ONLINE
 scrub: resilver completed with 0 errors on Fri Jan 12 14:47:36 2007
config:

        NAME        STATE     READ WRITE CKSUM
        zeepool     ONLINE       0     0     0
          mirror    ONLINE       0     0     0
            c0t1d0  ONLINE       0     0     0
            c1t1d0  ONLINE       0     0     0
            c2t1d0  ONLINE       0     0     0</screen>
<para>Jeśli istniejące urządzenie jest podwójnym mirrorem, przyłączenie nowego 
urządzenia spowoduje przekształcenie w potrójny mirror i tak dalej. W każdym 
razie nowe urządzenie natychmiast zaczyna resilverować.</para>
</example>
<example xml:id="gevpf">

<title>Przekształcanie puli ZFS bez redundancji w pulę
mirrorowaną</title>
<para>Dodatkowo można przekształcić pulę bez redundancji w pulę z redundancją poprzez
użycie polecenia <command>zpool attach</command>. Na przykład:</para>
<screen># <userinput>zpool create tank c0t1d0</userinput>
# <userinput>zpool status</userinput>
  pool: tank
 state: ONLINE
 scrub: none requested
config:
        NAME        STATE     READ WRITE CKSUM
        tank        ONLINE       0     0     0
          c0t1d0    ONLINE       0     0     0

errors: No known data errors
# <userinput>zpool attach tank c0t1d0 c1t1d0</userinput>
# <userinput>zpool status</userinput>
  pool: tank
 state: ONLINE
 scrub: resilver completed with 0 errors on Fri Jan 12 14:55:48 2007
config:
        NAME        STATE     READ WRITE CKSUM
        tank        ONLINE       0     0     0
          mirror    ONLINE       0     0     0
            c0t1d0  ONLINE       0     0     0
            c1t1d0  ONLINE       0     0     0</screen>
</example>
<para>Do odłączenia urządzenia z mirrorowanej puli można użyć komendy <command>zpool detach
</command>. Na przykład:</para>
<screen># <userinput>zpool detach zeepool c2t1d0</userinput>
</screen>
<para>ZFS nie wykona tej operacji, jeśli nie istnieje żadna prawidłowa kopia 
danych. Na przykład:<indexterm xml:id="indexterm-138">
<primary>odłączania</primary>
<secondary>urządzeń z puli nośników danych ZFS (<command>zpool detach</command>)
</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-139">
<primary>
<command>zpool detach</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-140">
<primary>pule nośników danych ZFS</primary>
<secondary>odłączanie urządzeń (<command>zpool detach</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool detach newpool c1t2d0</userinput>
cannot detach c1t2d0: only applicable to mirror and replacing vdevs</screen>
</sect2>
<sect2 xml:id="gazgm">
<title>Włączanie i wyłączanie urządzeń w puli nośników danych</title>
<para>ZFS umożliwia włączanie i wyłączanie pojedynczych urządzeń. Jeśli sprzęt 
zachowuje się niestabilnie lub niewłaściwie, ZFS nadal zapisuje i odczytuje zeń 
dane, zakładając, że usterka jest chwilowa. Jeśli tak nie jest, można nakazać 
ZFS ignorować urządzenie przez wyłączenie go. ZFS nie wysyła żadnych zapytań 
do wyłączonego urządzenia.<indexterm xml:id="indexterm-141">
<primary>włączanie i wyłączanie urządzeń</primary>
<secondary>pula nośników danych ZFS</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-142">
<primary>pula nośników danych ZFS</primary>
<secondary>włączanie i wyłączanie urządzeń</secondary>
<tertiary>opis</tertiary>
</indexterm>
</para>
<note>
<para>Nie trzeba wyłączać urządzeń, aby je wymienić.</para>
</note>
<para>Jeśli zachodzi konieczność tymczasowego odłączenia nośników, można 
użyć komendy <command>offline</command>. Na przykład, jeśli zachodzi konieczność
 odłączenia macierzy od układu przełączników Fibre Channel i podłączenia jej 
do innego układu przełączników, można wyłączyć LUN-y z macierzy używanej 
w pulach ZFS. Podłączoną i działającą już na nowych przełącznikach macierz można 
aktywować włączając te same LUN-y. Dane, które pojawiły się w puli w czasie, 
kiedy LUN-y były wyłączone, automatycznie zaczną się resilverować, kiedy tylko 
LUN-y zostaną włączone.</para>
<para>Scenariusz taki jest możliwy przy założeniu, że omawiane systemy widzą 
nośniki danych po podłączeniu ich do nowych przełączników, możliwe że przez 
inne niż poprzednio kontrolery, oraz że pule są skonfigurowane jako RAID-Z 
lub mirror.</para>
<sect3 xml:id="gazfy">
<title>Wyłączanie urządzenia</title>
<para>Urządzenie można wyłączyć używając komendy <command>zpool offline
</command>.
Urządzenie można wskazać przez ścieżkę lub skrót, jeśli jest dyskiem. Na 
przykład:</para>
<screen># <userinput>zpool offline tank c1t0d0</userinput>
bringing device c1t0d0 offline</screen>
<para>Należy mieć na uwadze następujące punkty przy wyłączaniu urządzenia:</para>
<itemizedlist>
<listitem>
<para>Niemożliwe jest takie wyłączanie urządzeń, aby uszkodzić pulę. Nie można 
na przykład wyłączyć dwóch urządzeń w RAID-Z ani wyłączyć wirtualnego urządzenia
 najwyższego poziomu. <indexterm xml:id="indexterm-143">
<primary>wyłączanie urządzenia (<command>zpool offline</command>)</primary>
<secondary>pula nośników danych ZFS</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-144">
<primary>
<command>zpool offline</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-145">
<primary>pule nośników danych ZFS</primary>
<secondary>wyłączanie urządzenia (<command>zpool offline</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool offline tank c1t0d0</userinput>
cannot offline c1t0d0: no valid replicas</screen>
</listitem>
<listitem>
<para>Domyślnie wyłączenie jest trwałe. Urządzenie pozostaje wyłączone po restarcie systemu.</para>
<para>Tymczasowego wyłączenia można dokonać przy użyciu <command>zpool offline</command> <option>
t</option>. Na przykład:</para>
<screen># <userinput>zpool offline -t tank c1t0d0</userinput>
 bringing device 'c1t0d0' offline</screen>
<para>Po restarcie systemu urządzenie to zostanie automatycznie przywrócone do stanu online <literal>ONLINE</literal>.</para>
</listitem>
<listitem>
<para>Przy wyłączeniu urządzenia, nie jest ono usuwane z puli. Podczas próbu użycia urządzenia w innej puli, nawet po zniszczeniu oryginalnej puli, wydrukowana zostanie wiadomość podobna do poniższej:</para>
<screen>
<replaceable>device</replaceable> is part of exported or potentially active ZFS <replaceable>pool</replaceable>. Please see zpool(1M)</screen>
<para>Przed użyciem wyłączonego urządzenia w innej puli, najepierw należy urządzenie to włączyć, później zaś zniszczyć pulę, do której jest podłączone. </para>
<para>Innym sposobem na użycie urządzenia w nowej puli, bez niszczenia aktualnej puli, jest zastąpienie urządzenia podobnym. Więcej informacji o wymianie urządzeń w puli w <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazgd">Wymiana urządzeń w puli nośników danych</olink>.</para>
</listitem>
</itemizedlist>
<para>Wyłączone urządzenie ma flagę <literal>OFFLINE</literal> podczas sprawdzania statusu puli. Więcej informacji o sprawdzaniu statusu uli w <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gaynp">Sprawdzanie statusu puli</olink>.</para>
<para>Więcej informacji o zdrowiu urządzeń w <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gamno">Ustalanie zdrowia urządzeń w pulach nośników danych ZFS</olink>.</para>
</sect3>
<sect3 xml:id="gazgk">
<title>Włączanie urządzenia</title>
<para>Po wyłączeniu urządzenia można je włączyć komendą <command>zpool online</command>:</para>
<screen># <userinput>zpool online tank c1t0d0</userinput>
bringing device c1t0d0 online</screen>
<para>Po włączeniu urządzenia, wszelkie dane, które pojawiły się w puli są synchronizowane z nowym urządzeniem. Nie można używać wyłączania do wymiany dysków. Próba wymiany wyłączonego urządzenia spowoduje oflagowanie go jako uszkodzone przy włączaniu.<indexterm xml:id="indexterm-146">
<primary>włączanie urządzenia</primary>
<secondary>pula nośników danych ZFS (<command>zpool online</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-147">
<primary>
<command>zpool online</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-148">
<primary>pule nośników danych ZFS (<command>zpool online</command>)</primary>
<secondary>włączanie urządzenia</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<para>Przy próbie włączenia uszkodzonego urządzenia, przy użyciu komendy <command>fmd</command> pojawi się komunikat podobny do poniższego:</para>
<screen># <userinput>zpool online tank c1t0d0</userinput>
Bringing device c1t0d0 online
# 
SUNW-MSG-ID: ZFS-8000-D3, TYPE: Fault, VER: 1, SEVERITY: Major
EVENT-TIME: Thu Aug 31 11:13:59 MDT 2006
PLATFORM: SUNW,Ultra-60, CSN: -, HOSTNAME: neo
SOURCE: zfs-diagnosis, REV: 1.0
EVENT-ID: e11d8245-d76a-e152-80c6-e63763ed7e4f
DESC: A ZFS device failed.  Refer to http://sun.com/msg/ZFS-8000-D3 for more information.
AUTO-RESPONSE: No automated response will occur.
IMPACT: Fault tolerance of the pool may be compromised.
REC-ACTION: Run 'zpool status -x' and replace the bad device.</screen>
<para>Więcej informacji o wymianie uszkodzonych urządzeń w <olink remap="external" targetdoc="chapter-9.xml" targetptr="gbbvb">Naprawianiu brakujących urządzeń</olink>.</para>
</sect3>
</sect2>
<sect2 xml:id="gazge">
<title>Czyszczenie pul nośników danych</title>
<para>Jeśli urządzenie zostało wyłączone z powodu uszkodzenia, które powodowało wyświetlanie się błędów w wyniku komendy <command>zpool status</command>, można wyczyścić licznik błędów komendą <command>zpool clear</command>.<indexterm xml:id="indexterm-149">
<primary>czyszczenie</primary>
<secondary>urządzenie w puli nośników danych ZFS (<command>zpool clear</command>)</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-150">
<primary>
<command>zpool clear</command>
</primary>
<secondary>opis</secondary>
</indexterm>
</para>
<para>Komenda ta wydana bez argumentów czyści wszystkie błędy wszystkich urządzeń w puli. Na przykład:</para>
<screen># <userinput>zpool clear tank</userinput>
</screen>
<para>Jeśli jako argumenty podano jedno lub więcej urządzeń, komenda wyczyści błędy powiązane tylko z tymi urządzeniami. Na przykład:<indexterm xml:id="indexterm-151">
<primary>czyszczenie urządzeń</primary>
<secondary>puna nośników danych ZFS</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-152">
<primary>
<command>zpool clear</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-153">
<primary>pule nośników danych ZFS</primary>
<secondary>czyszczenie urządzenia</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool clear tank c1t0d0</userinput>
</screen>
<para>Więcej informacji o czyszczeniu błędów <command>zpool</command> w <olink remap="external" targetdoc="chapter-9.xml" targetptr="gbbzv">Czyszczeniu tymczasowych błędów</olink>.</para>
</sect2>
<sect2 xml:id="gazgd">
<title>Wymiana urządzeń w puli nośników danych</title>
<para>Urządzenie w puli można wymienić komendą <command>zpool
replace</command>.<indexterm xml:id="indexterm-154">
<primary>wymiana</primary>
<secondary>urządzenia (<command>zpool replace</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-155">
<primary>
<command>zpool replace</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-156">
<primary>pule nośników danych ZFS</primary>
<secondary>wymiana urządzenia (<command>zpool replace</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool replace tank c1t1d0 c1t2d0</userinput>
</screen>
<para>W tym przykładzie poprzednie urządzenie, <literal>c1t1d0</literal>, zostało zastąpione urządzeniem <literal>c1t2d0</literal>.</para>
<para>Pojemność urządzenia dołączanego do puli musi być nie mniejsza niż najmniejsza ze wszystkich pojemności urządzeń w konfiguracji mirror lub RAID-Z. Jeśli dołączone urządzenie jest większe, pojemność puli w konfiguracji innej niż RAID-Z i mirror zostanie powiększona po zakończeniu wymiany urządzenia.</para>
<para>Więcej informacji o wymianie urządzeń w <olink remap="external" targetdoc="chapter-9.xml" targetptr="gbbvb">Naprawianiu brakującego urządzenia</olink> i <olink remap="external" targetdoc="chapter-9.xml" targetptr="gbbvf">Naprawianiu uszkodzonego urządzenia</olink>.</para>
</sect2>
<sect2 xml:id="gcvcw">
<title>Wyznaczanie urządzeń "gorącej rezerwy" (hot spare) w puli nośników danych</title>
<para>Parametr Hot Spares ZFS-a pozwala na wskazanie dysków, którymi można zastąpić uszkodzone urządzenia w jednej lub więcej pulach nośników danych. Po wskazaniu urządzenia jako <emphasis>hot spare</emphasis>, przejmuje ono automatycznie rolę uszkodzonego aktywnego dysku.<indexterm xml:id="indexterm-157">
<primary>hot spares</primary>
<secondary>tworzenie</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<para>Urządzenia można wskazać jako hot spare w następujący sposób:</para>
<itemizedlist>
<listitem>
<para>Przy tworzeniu puli komendą <command>zpool create</command></para>
</listitem>
<listitem>
<para>Po utworzeniu puli komendą <command>zpool add</command></para>
</listitem>
<listitem>
<para>Urządzenia hot spare mogą być współdzielone między kilkoma pulami.</para>
</listitem>
</itemizedlist>
<para>Wskazanie hot sprae podczas tworzenia puli. Na przykład:<indexterm xml:id="indexterm-158">
<primary>hot spares</primary>
<secondary>opis</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool create zeepool mirror c1t1d0 c2t1d0 spare c1t2d0 c2t2d0</userinput>
# <userinput>zpool status zeepool</userinput>
pool: zeepool
 state: ONLINE
 scrub: none requested
config:

        NAME         STATE     READ WRITE CKSUM
        zeepool      ONLINE       0     0     0
          mirror     ONLINE       0     0     0
            c1t1d0   ONLINE       0     0     0
            c2t1d0   ONLINE       0     0     0
        spares
          c1t2d0     AVAIL
          c2t2d0     AVAIL   </screen>
<para>Dodanie hot spare po utworzeniu puli. Na przykład:</para>
<screen># <userinput>zpool add -f zeepool spare c1t3d0 c2t3d0</userinput>
# <userinput>zpool status zeepool</userinput>
pool: zeepool
 state: ONLINE
 scrub: none requested
config:

        NAME         STATE     READ WRITE CKSUM
        zeepool      ONLINE       0     0     0
          mirror     ONLINE       0     0     0
            c1t1d0   ONLINE       0     0     0
            c2t1d0   ONLINE       0     0     0
        spares
          c1t3d0     AVAIL
          c2t3d0     AVAIL   </screen>
<para>Wiel pul może wpółdzielić jedno urządzenie hot spare. Na przykład:</para>
<screen># <userinput>zpool create zeepool mirror c1t1d0 c2t1d0 spare c1t2d0 c2t2d0</userinput>
# <userinput>zpool create tank raidz c3t1d0 c4t1d0 spare c1t2d0 c2t2d0</userinput>
</screen>
<para>Hot spares można usunąć z puli poleceniem <command>zpool
remove</command>. Na przykład:</para>
<screen># <userinput>zpool remove zeepool c1t2d0</userinput>
# <userinput>zpool status zeepool</userinput>
pool: zeepool
 state: ONLINE
 scrub: none requested
config:

        NAME         STATE     READ WRITE CKSUM
        zeepool      ONLINE       0     0     0
          mirror     ONLINE       0     0     0
            c1t1d0   ONLINE       0     0     0
            c2t1d0   ONLINE       0     0     0
        spares
          c1t3d0     AVAIL</screen>
<para>Urządzenia hot spare nie można usunąć z puli, jeśli jest aktualnie wykorzystywane.</para>
<para>Podczas używania urządzeń hot spare z ZFS należy pamiętać o:</para>
<itemizedlist>
<listitem>
<para>Komendy <command>zpool remove</command> można obecnie używać tylko do usuwania z puli urządzeń hot spare.</para>
</listitem>
<listitem>
<para>Urządzenie hot spare powinno być nie mniejsze niż największe urządzenie w puli. Wprawdzie dozwolone jest tworzenie mniejszych hot spare, ale przy awarii większego dysku próba uruchomienia urządzenia hot spare, automatycznie bądź komendą <command>zpool replace</command>, powoduje błąd podobny do poniższego:</para>
<screen>cannot replace <replaceable>disk3</replaceable> with <replaceable>disk4</replaceable>: device is too small</screen>
</listitem>
</itemizedlist>
<sect3 xml:id="gcvdi">
<title>Włączanie i wyłączanie urządzeń hot spare w puli</title>
<para>Urządzenia hot spare włączane są w następujący sposób:</para>
<itemizedlist>
<listitem>
<para>Ręczna wymiana - ręczna wymiana uszkodzonego urządzenia w puli za pomocą komendy <command>zpool replace</command>.</para>
</listitem>
<listitem>
<para>Automatyczna wymiana – po pojawieniu się informacji o uszkodzeniu urządzenia, agent FMA szuka w puli wolnych urządzeń hot spare. Jeśli takie są, podmienia uszkodzone urządzenie.</para>
<para>Jeśli urządzeni hot spare zawiedzie, agent je odłącze, efektywnie anulując podmianę. Próbuje wtedy podłączyć inne urządzenie hot spare, jeśli jest dostępne. W chwili obecnej mechanizm ten jest ograniczony przez fakt, że ZFS emituje sygnał faul, tylko wtedy, gdy urządzenie zniknie z puli. </para>
<para>Nie ma obecnie możliwości automatycznego przywrócenia oryginalnego urządzenia. Należy samodzielnie wykonać polecenia z poniższego przykładu. W przyszłości ZFS będzie umiał korzystać ze zdarzeń hotplug u automatycznie wymieniać uszkodzone urządzenia po ich wymianie w systemie.</para>
</listitem>
</itemizedlist>
<para>Ręczna wymiana urządzenia na hot spare komendą <command>zpool
replace</command>. Na przykład:</para>
<screen># <userinput>zpool replace zeepool c2t1d0 c2t3d0</userinput>
# <userinput>zpool status zeepool</userinput>
  pool: zeepool
 state: ONLINE
 scrub: resilver completed with 0 errors on Fri Jun  2 13:44:40 2006
config:

        NAME            STATE     READ WRITE CKSUM
        zeepool         ONLINE       0     0     0
          mirror        ONLINE       0     0     0
            c1t2d0      ONLINE       0     0     0
            spare       ONLINE       0     0     0
              c2t1d0    ONLINE       0     0     0
              c2t3d0    ONLINE       0     0     0
        spares
          c1t3d0        AVAIL
          c2t3d0        INUSE     currently in use

errors: No known data errors</screen>
<para>Uszkodzone urządzenie jest automatycznie wymieniane, jeśli jest dostępne hot spare. Na przykład:</para>
<screen># <userinput>zpool status -x</userinput>
  pool: zeepool
 state: DEGRADED
status: One or more devices could not be opened.  Sufficient replicas exist for
        the pool to continue functioning in a degraded state.
action: Attach the missing device and online it using 'zpool online'.
   see: http://www.sun.com/msg/ZFS-8000-D3
 scrub: resilver completed with 0 errors on Fri Jun  2 13:56:49 2006
config:

        NAME                 STATE     READ WRITE CKSUM
        zeepool              DEGRADED     0     0     0
          mirror             DEGRADED     0     0     0
            c1t2d0           ONLINE       0     0     0
            spare            DEGRADED     0     0     0
              c2t1d0         UNAVAIL      0     0     0  cannot open
              c2t3d0         ONLINE       0     0     0
        spares
          c1t3d0             AVAIL
          c2t3d0             INUSE     currently in use

errors: No known data errors</screen>
<para>Hot spare można wyłączyć na trzy sposoby:</para>
<itemizedlist>
<listitem>
<para>Usuwając hot spare z puli</para>
</listitem>
<listitem>
<para>Wymieniając oryginalne urządzenie na hot spare</para>
</listitem>
<listitem>
<para>Włączając urządzenie hot spare jako aktywne urządzenie w puli</para>
</listitem>
</itemizedlist>
<para>Po wymianie uszkodzonego urządzenia, komendą <command>zpool detach</command> należy odłączyć urządzenie hot spare, aby przywrócić je do zestawu urządzeń hot spare w puli. Na przykład:</para>
<screen># <userinput>zpool detach zeepool c2t3d0</userinput>
# <userinput>zpool status zeepool</userinput>
  pool: zeepool
 state: ONLINE
 scrub: resilver completed with 0 errors on Fri Jun  2 13:58:35 2006
config:

        NAME               STATE     READ WRITE CKSUM
        zeepool            ONLINE       0     0     0
          mirror           ONLINE       0     0     0
            c1t2d0         ONLINE       0     0     0
            c2t1d0         ONLINE       0     0     0
        spares
          c1t3d0           AVAIL
          c2t3d0           AVAIL

errors: No known data errors</screen>
</sect3>
</sect2>
</sect1>
<sect1 xml:id="gaynp">
<title>Sprawdzanie stanu puli nośników danych ZFS</title>
<para>Komenda <command>zpool list</command> pozwala na sprawdzanie stanu puli na wiele sposobów. Dostępne w ten sposób informacje należą do trzech kategorii: podstawowe użycie, statystyki we/wy i zdrowie urządzeń. Wszystkie trzy są omawiane w poniższym podrozdziale.<indexterm xml:id="indexterm-159">
<primary>lista</primary>
<secondary>pule nośników danych ZFS</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-160">
<primary>
<command>zpool list</command>
</primary>
<secondary>opis</secondary>
</indexterm>
</para>
<itemizedlist>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gamml">Wyświetlanie podstawowych informacji o pulach nośników danych ZFS</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gammt">Podgląd statystyk We/Wy pul nośników danych ZFS</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gamno">Określanie zdrowia pul nośników danych ZFS</olink>
</para>
</listitem>
</itemizedlist>
<sect2 xml:id="gamml">
<title>Wyświetlanie podstawowych informacji o pulach nośników danych ZFS</title>
<para>Podstawowe informacje o pulach można wyświetlić za pomocą komendy <command>zpool list</command>.</para>
<sect3 xml:id="gazij">
<title>Informacje o wszystkich dostępnych pulach</title>
<para>Jeśli nie podano argumentów, komenda wyświetli wszystkie pola o wszystkich pulach w systemie. Na przykład:</para>
<screen># <userinput>zpool list</userinput>
NAME                    SIZE    USED   AVAIL    CAP  HEALTH     ALTROOT
tank                   80.0G   22.3G   47.7G    28%  ONLINE     -
dozer                   1.2T    384G    816G    32%  ONLINE     -</screen>
<para>Powyższy wydruk pokazuje następujące informacje:</para>
<variablelist>
<varlistentry>
<term>
<literal>NAME</literal>
</term>
<listitem>
<para>Nazwa puli.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>SIZE</literal>
</term>
<listitem>
<para>Całkowita pojemność puli, równa sumie pojemności wszystkich urządzeń wirtualnych najwyższego poziomu.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>USED</literal>
</term>
<listitem>
<para>Miejsce alokowane przez wszystkie datasety i wewnętrzne metadane. Należy zauważyć, że wielkości raportowane w tym polu różnią się od wielkości raportowanych na poziomie systemu plików.</para>
<para>Więcej o ustalaniu dostępnej przestrzeni w systemie plików w <olink remap="external" targetdoc="chapter-3.xml" targetptr="gbchp">Obliczaniu miejsca w ZFS</olink>.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>AVAILABLE</literal>
</term>
<listitem>
<para>Wielkość niealokowanej przestrzeni w puli.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>CAPACITY</literal> (<literal>CAP</literal>)</term>
<listitem>
<para>Wielkość używanej przestrzeni, raportowana jako procent całkowitej pojemności.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>HEALTH</literal>
</term>
<listitem>
<para>Obecny poziom zdrowia puli.</para>
<para>Więcej o zdrowiu puli w <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gamno">Określaniu zdrowia pul nośników danych ZFS</olink>.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>ALTROOT</literal>
</term>
<listitem>
<para>Alternatywny główny katalog puli, jeśli jest.</para>
<para>Więcej informacji o alternatywnych głównych katalogach pul w <olink remap="external" targetdoc="chapter-8.xml" targetptr="gbcgl">Alternatywnych głównych katalogach pul ZFS</olink>.</para>
</listitem>
</varlistentry>
</variablelist>
<para>Można również odczytać statystyki konkretnej puli przez podanie jej w argumencie komendy. Na przykład:<indexterm xml:id="indexterm-161">
<primary>lista</primary>
<secondary>pule nośników danych ZFS</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-162">
<primary>
<command>zpool list</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-163">
<primary>pule nośników danych ZFS</primary>
<secondary>lista</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool list tank</userinput>
NAME                    SIZE    USED   AVAIL    CAP  HEALTH     ALTROOT
tank                   80.0G   22.3G   47.7G    28%  ONLINE     -</screen>
</sect3>
<sect3 xml:id="gazil">
<title>Konkretne statystyki puli nośników danych</title>
<para>Konkretne statystyki można odczytać używając opcji <option>
o</option>.
Opcja ta umożliwia tworzenie własnych raportów lub szybkie dostęp do pożądanych informacji. Aby, na przykład, wydrukować tylko nazwę i pojemność pul, należy użyć następującej składni:</para>
<screen># <userinput>zpool list -o name,size</userinput>
NAME                    SIZE
tank                   80.0G
dozer                   1.2T</screen>
<para>Nazwy kolumn odpowiadają właściwościom wydrukowanym w <olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazij">Drukowaniu informacji o wszystkich pulach nośników danych</olink>.</para>
</sect3>
<sect3 xml:id="gazje">
<title>Skryptowe przetwarzanie wyjścia pul nośników danych ZFS</title>
<para>Domyślny format wyjścia komendy <command>zpool list</command> jest przede wszystkim czytelny i niełatwo użyć go w skryptach powłoki. Ułatwieniem w zastosowaniach programowych jest użycie opcji <option>
H</option>, która wyłącza drukowanie nazw kolumn i oddzielanie pól tabulatorami zamiast spacji. Aby uzyskać, na przykład, prostą listę nazw wszystkich pul w systemie:<indexterm xml:id="indexterm-164">
<primary>przetwarzanie skryptowe</primary>
<secondary>wyjście pul nośników danych ZFS</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-165">
<primary>
<command>zpool list -Ho name</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-166">
<primary>pule nośników danych ZFS</primary>
<secondary>skryptowe przetwarzanie wyjścia pul nośników danych</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool list -Ho name</userinput>
tank
dozer</screen>
<para>Poniżej jest następny przykład:</para>
<screen># <userinput>zpool list -H -o name,size</userinput>
tank   80.0G
dozer  1.2T</screen>
</sect3>
</sect2>
<sect2 xml:id="gammt">
<title>Podgląd statystyk We/Wy pul nośników danych ZFS</title>
<para>Statystyki We/Wy dla pul lub konkretnych urządzeń wirtualnych można uzyskać za pomocą komendy <command>zpool iostat</command>. Podobnie jak komenda <command>iostat</command>, ta komenda może wydrukować statyczny obraz sumarycznego We/Wy oraz aktualne statystyki dla wybranego okresu. Zwracane są następujące statystyki:<indexterm xml:id="indexterm-1647">
<primary>drukowanie</primary>
<secondary>statystyk We/Wy pul nośników danych ZFS</secondary>
<tertiary>opis</tertiary>
</indexterm>
</para>
<variablelist>
<varlistentry>
<term>
<literal>USED CAPACITY</literal>
</term>
<listitem>
<para>Ilość danych aktualnie przechowywanych w puli lub na urządzeniu. Liczba ta różni się nieznacznie od ilości miejsca faktycznie dostępnej systemom plików ze względu na wewnętrzne szczegóły implementacji.</para>
<para>Więcej informacji o różnicach w pojemności pul i datasetów w <olink remap="external" targetdoc="chapter-3.xml" targetptr="gbchp">Obliczaniu pojemności ZFS</olink>.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>AVAILABLE CAPACITY</literal>
</term>
<listitem>
<para>Ilość miejsca dostępna w puli lub na urządzeniu. Podobnie jak w przypadku pola <literal>used</literal>, różni się nieznacznie od miejsca dostępnego datasetom.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>READ OPERATIONS</literal>
</term>
<listitem>
<para>Liczba operacji odczytania We/Wy wysłanych do puli lub urządzenia, włącznie z żądaniami metadanych.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>WRITE OPERATIONS</literal>
</term>
<listitem>
<para>Liczba operacji zapisu We/Wy wysłanych do puli lub urządzenia.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>READ BANDWIDTH</literal>
</term>
<listitem>
<para>Przepustowość wszystkich operacji odczytu (włącznie z metadanymi) wyrażona w jednostkach na sekundę.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>WRITE BANDWIDTH</literal>
</term>
<listitem>
<para>Przepustowość wszystkich operacji zapisu wyrażona w jednostkach na sekundę.</para>
</listitem>
</varlistentry>
</variablelist>
<sect3 xml:id="gazng">
<title>Drukowanie statystyk dla wszystkich pul</title>
<para>Jeśli nie podano żadnych opcji, komenda <command>zpool iostat</command> drukuje skumulowane statystyki od momentu rozruchu systemu dla wszystkich pul w systemie. Na przykład:<indexterm xml:id="indexterm-168">
<primary>drukowanie</primary>
<secondary>statystyki We/Wy dla pul nośników danych ZFS</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-169">
<primary>
<command>zpool iostat</command>
</primary>
<secondary>pool-wide (przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-170">
<primary>pule nośników danych ZFS</primary>
<secondary>statystyki We/Wy dla pul nośników danych</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool iostat</userinput>
               capacity     operations    bandwidth
pool         used  avail   read  write   read  write
----------  -----  -----  -----  -----  -----  -----
tank         100G  20.0G   1.2M   102K   1.2M  3.45K
dozer       12.3G  67.7G   132K  15.2K  32.1K  1.20K</screen>
<para>Ze względu na fakt, że statystyki te są kumulatywne od momentu rozruchu systemu, przepustowość może wydawać się niska, jeśli pula jest względnie nieaktywna. Bliższe rzeczywistości statystyki można uzyskać wskazując przedział czasu, dla którego mają zostać wydrukowane dane. Na przykład:</para>
<screen># <userinput>zpool iostat tank 2</userinput>
               capacity     operations    bandwidth
pool         used  avail   read  write   read  write
----------  -----  -----  -----  -----  -----  -----
tank         100G  20.0G   1.2M   102K   1.2M  3.45K
tank         100G  20.0G    134      0  1.34K      0
tank         100G  20.0G     94    342  1.06K   4.1M</screen>
<para>W tym przykładzie komenda drukuje statystyki użycia puli <literal>tank</literal> co dwie sekundy, aż do zatrzymania procesu klawiszami Ctrl-C.
Można dodatkowo zdefiniować ilość powtórzeń wykonania komendy za pomocą parametru <literal>count</literal>. Na przykład <command>zpool iostat 2 3</command> wydrukuje podsumowanie trzy razy co dwie sekundy, łącznie sześć sekund. Jeśli w systemie jest tylko jedna pula, statystyki będą drukowane w kolejnych liniach. Jeśli istnieje więcej pul, co każdą iterację dodawane jest linia z myślnikiem, w celu wizualnego oddzielenia każdego wykonania.</para>
</sect3>
<sect3 xml:id="gazne">
<title>Drukowanie statystyk urządzeń wirtualnych</title>
<para>Oprócz statystyk We/Wy dla pul, komenda <command>zpool iostat</command> może wydrukować statystyki dla konkretnych urządzeń wirtualnych. Można nią zidentyfikować zbyt wolne urządzenia lub po prostu obserwować rozłożenie We/Wy generowanego przez ZFS. Dokładny układ urządzenia oraz jego statystyki We/Wy można wydrukować komendą <command>zpool iostat -v</command>.
Na przykład:<indexterm xml:id="indexterm-171">
<primary>drukowanie</primary>
<secondary>Statystyki We/Wy urządzeń wirtualnych w pulach nośników danych ZFS</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-172">
<primary>
<command>zpool iostat -v</command>
</primary>
<secondary>urządzenia wirtualne (przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-173">
<primary>pule nośników danych ZFS</primary>
<secondary>statystyki We/Wy urządzeń wirtualnych</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool iostat -v</userinput>
               capacity     operations    bandwidth
tank         used  avail   read  write   read  write
----------  -----  -----  -----  -----  -----  -----
mirror      20.4G  59.6G      0     22      0  6.00K
  c1t0d0        -      -      1    295  11.2K   148K
  c1t1d0        -      -      1    299  11.2K   148K
----------  -----  -----  -----  -----  -----  -----
total       24.5K   149M      0     22      0  6.00K</screen>
<para>Podczas przeglądania statystyk We/Wy urządzeń wirtualnych należy pamiętać o dwóch rzeczach.</para>
<itemizedlist>
<listitem>
<para>Po pierwsze, zużycie miejsca jest dostępne tylko urządzeniom najwyższego poziomu. Sposób alokacji pomiędzy mirrorami i urządzeniami RAID-Z jest charakterystyczna dla implementacji i trudna do wyrażenia w pojedynczej liczbie.</para>
</listitem>
<listitem>
<para>Po drugie, liczby mogą nie sumować się w sposób, jakiego można się spodziewać. W szczególności operacje na urządzeniach RAID-Z i morrorach nie będą równe. Różnica jest szczególnie zauważalna zaraz po utworzeniu puli, ponieważ większość We/Wy wykonywana jest bezpośrednio na dyskach jako część procesu tworzenia puli, nie jest zaś rejestrowana na poziomie mirrora. Z czasem liczby te powinny znacznie się do siebie zbliżyć, aczkolwiek zepsute, milczące lub wyłączone urządzenia mogą zmienić tę symetrię.</para>
</listitem>
</itemizedlist>
<para>Przy przeglądaniu statystyk urządzeń wirtualnych dostępne są te same opcje (interwał i liczba powtórzeń) co przy przeglądaniu statystyk pul.</para>
</sect3>
</sect2>
<sect2 xml:id="gamno">
<title>Określanie zdrowia pul nośników danych ZFS</title>
<para>ZFS zapewnia zintegrowaną metodę sprawdzania zdrowia pul i urządzeń. Zdrowie puli zależy od zdrowia poszczególnych urządzeń, które się na nią składają. Informacje te można wydrukować komendą <command>zpool status</command>. Dodatkowo potencjalne uszkodzenia pul i urządzeń są raportowane przez <command>fmd</command> i wyświetlane na konsoli systemowej i logowane w pliku <command>/VAT/ADM/messa ges</command>.
W tym podrozdziale przedstawione są sposoby ustalania zdrowia pul i urządzeń. Nie przedstawiamy sposobów radzenia sobie z  uszkodzeniami pul i urządzeń. Informacje o rozwiązywaniu problemów i odzyskiwaniu danych są w <olink remap="external" targetdoc="chapter-9.xml" targetptr="gavwg">Rozdziale 9, Rozwiązywanie problemów i odzyskiwanie danych na ZFS</olink>.<indexterm xml:id="indexterm-174">
<primary>drukowanie</primary>
<secondary>zdrowie pul nośników danych</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-175">
<primary>pule nośników danych ZFS</primary>
<secondary>drukowanie zdrowia</secondary>
</indexterm>
</para>
<para>Każde z urządzeń może mieć jeden z poniższych stanów:</para>
<variablelist>
<varlistentry>
<term>
<literal>ONLINE</literal>
</term>
<listitem>
<para>Urządzenie pracuje normalnie. Mogą pojawiać się chwilowe błędy, urządzenie ogólnie jest sprawne.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>DEGRADED</literal>
</term>
<listitem>
<para>Na urządzeniu wirtualnym zdarzyły się jakieś błędy, ale nadal może funkcjonować. Stan ten pojawia się najczęściej, gdy mirror lub RAID-Z utraciło jedno lub więcej ciągłych urządzeń. Odporność puli na uszkodzenia i błędy może być obniżona, ponieważ po uszkodzeniu następnego urządzenia pula może być trwale uszkodzona.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>FAULTED</literal>
</term>
<listitem>
<para>Urządzenie wirtualne jest niedostępne. Stan ten wskazuje najczęściej na uszkodzenie urządzenia uniemożliwiające korzystanie z niego, na przykład takie, w którym ZFS nie może ani wysyłać ani odczytywać z niego danych. Jeśli urządzenie wirtualne najwyższego poziomu jest w takim stanie, cała pula jest niedostępna.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>OFFLINE</literal>
</term>
<listitem>
<para>Urządzenie zostało ręcznie wyłączone przez administratora.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>UNAVAILABLE</literal>
</term>
<listitem>
<para>Nie można otworzyć urządzenia bądź urządzenia wirtualnego. W niektórych przypadkach pule z urządzeniami w stanie <literal>UNAVAILABLE</literal> oznaczone są jako <literal>DEGRADED</literal>. Jeśli urządzenie wirtualne najwyższego poziomu jest niedostępne, wszystkie urządzenia w puli są niedostępne.</para>
</listitem>
</varlistentry>
</variablelist>
<para>Zdrowie puli określane jest na podstawie zdrowia wszystkich jej urządzeń wirtualnych najwyższego poziomu. Jeśli wszystkie urządzenia wirtualne są w stanie <literal>ONLINE</literal>, to pula również określana jest jako <literal>ONLINE</literal>. Jeśli jedno z urządzeń wirtualnych jest w stanie <literal>DEGRADED</literal> lub <literal>UNAVAILABLE</literal>, to pula jest również w stanie <literal>DEGRADED</literal>. Jeśli urządzenie wirtualne najwyższego poziomu oznaczone jest jako <literal>FAULTED</literal> lub <literal>OFFLINE</literal>, to cała pula jest również oznaczana jako <literal>FAULTED</literal>. Pula w stanie <literal>FAULTED</literal> jest całkowicie niedostępna. Nie można odzyskać żadnych danych dopóki konieczne urządzenia nie zostaną dołączone lub naprawione. Pula w stanie <literal>DEGRADED</literal> nadal działa, ale odpowiedni poziom redundancji danych lub przepustowości może być nieosiągalny.</para>
<sect3 xml:id="gazqw">
<title>Podstawowe dane o zdrowiu pul nośników danych</title>
<para>Najszybszym sposobem wydrukowania przeglądu zdrowia pul jest użycie komendy <command>zpool status</command>:<indexterm xml:id="indexterm-176">
<primary>drukowanie</primary>
<secondary>zdrowie pul nośników danych ZFS</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-177">
<primary>
<command>zpool status -x</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-178">
<primary>pule nośników danych ZFS</primary>
<secondary>drukowanie stanu zdrowia</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool status -x</userinput>
all pools are healthy</screen>
<para>Konkretne pule można sprawdzić przez podanie nazwy puli komendzie. Każda pula, której stan jest inni niż <literal>ONLINE</literal> powinna zostać dokładnie sprawdzona w sposób przedstawiony w następnym podrozdziale.</para>
</sect3>
<sect3 xml:id="gazqf">
<title>Dokładne dane o zdrowiu</title>
<para>Dokładniejsze dane o zdrowiu można uzyskać przy użyciu opcji <option>
v</option>.
Na przykład:</para>
<screen># <userinput>zpool status -v tank</userinput>
  pool: tank
 state: DEGRADED
status: One or more devices could not be opened.  Sufficient replicas exist 
        for the pool to continue functioning in a degraded state.
action: Attach the missing device and online it using 'zpool online'.
   see: http://www.sun.com/msg/ZFS-8000-2Q
 scrub: none requested
config:

        NAME                STATE     READ WRITE CKSUM
        tank                DEGRADED     0     0     0
          mirror            DEGRADED     0     0     0
            c1t0d0          FAULTED      0     0     0  cannot open
            c1t1d0          ONLINE       0     0     0
errors: No known data errors</screen>
<para>Powyższy wynik podaje dokładne powody, dla których pula jest w obecnym stanie, włącznie z czytelnym opisem i odnośnikiem do artykułu w bazie wiedzy dotyczącym danego problemu. Każdy artykuł w bazie wiedzy zawiera najświeższe informacje o najlepszych sposobach naprawiana danego błędu. Przy użyciu dokładnych danych o konfiguracji powinno dać się ustalić, które urządzenie jest uszkodzone i jak naprawić pulę.<indexterm xml:id="indexterm-179">
<primary>drukowanie</primary>
<secondary>szczegółowe informacje o zdrowiu puli nośników danych ZFS</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-180">
<primary>
<command>zpool status -v</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-181">
<primary>pule nośników danych ZFS</primary>
<secondary>drukowanie szczegółowych informacji o zdrowiu</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<para>W powyższym przykładzie uszkodzone urządzenie powinno zostać wymienione. Po wymianie urządzenia należy użyć komendy <command>zpool online</command>, aby włączyć urządzenie. Na przykład:</para>
<screen># <userinput>zpool online tank c1t0d0</userinput>
Bringing device c1t0d0 online
# <userinput>zpool status -x</userinput>
all pools are healthy</screen>
<para>Jeśli w puli jest wyłączone urządzenie, wyjście komendy wskazuje pulę. Na przykład:</para>
<screen># <userinput>zpool status -x</userinput>
  pool: tank
 state: DEGRADED
status: One or more devices has been taken offline by the administrator.
        Sufficient replicas exist for the pool to continue functioning in a
        degraded state.
action: Online the device using 'zpool online' or replace the device with
        'zpool replace'.
 scrub: none requested
config:

        NAME         STATE     READ WRITE CKSUM
        tank         DEGRADED     0     0     0
          mirror     DEGRADED     0     0     0
             c1t0d0  ONLINE       0     0     0
             c1t1d0  OFFLINE      0     0     0

errors: No known data errors</screen>
<para>Kolumny <literal>READ</literal> i <literal>WRITE</literal> podają liczbę błędów We/Wy, które pojawiły się na danych urządzeniu, a kolumna <literal>CKSUM</literal> podaje liczbę błędów sum kontrolnych, które pojawiły się na danych urządzeniu. Obydwa liczniki wskazują prawdopodobnie potencjalne uszkodzenie urządzenia i należy podjąć działania naprawcze. Jeśli wartości różne od zera podawane są dla urządzeń wirtualnych najwyższego poziomu, część danych może być niedostępna. Liczniki błędów rejestrują wszystkie znane błędy danych.</para>
<para>W powyższym przykładzie wyłączone urządzenie nie powoduje żadnych błędów w danych.</para>
<para>Więcej informacji o diagnozowaniu i reperowaniu uszkodzonych pul i danych w <olink remap="external" targetdoc="chapter-9.xml" targetptr="gavwg">Rozdziale 9, Rozwiązywanie problemów i odzyskiwanie danych na ZFS</olink>.</para>
</sect3>
</sect2>
</sect1>
<sect1 xml:id="gbchy">
<title>Migracja pul nośników danych ZFS</title>
<para>Zdarza się, że konieczne jest przeniesienie puli między komputerami. Aby to zrobić, urządzenia należy odłączyć od oryginalnego komputera i podłączyć je w docelowym. Można to zrobić przez fizyczne przełączenie okablowania lub przez urządzenia wieloportowe, jak urządzenia w SAN. ZFS umożliwia eksport puli z maszyny i import jej na inną, nawet jeśli komputery mają inny porządek bitów (ang. endianness). Więcej informacji o replikacji lub migracji systemów plików między różnymi pulami nośników danych, które mogą znajdować się na różnych maszynach w <olink remap="external" targetdoc="chapter-6.xml" targetptr="gbchx">Zapisywanie i odzyskiwanie danych ZFS</olink>.<indexterm xml:id="indexterm-182">
<primary>migracja pul nośników danych ZFS</primary>
<secondary>opis</secondary>
</indexterm>
<indexterm xml:id="indexterm-183">
<primary>pule nośników danych ZFS</primary>
<secondary>migracja</secondary>
<tertiary>opis</tertiary>
</indexterm>
</para>
<itemizedlist>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazre">Przygotowanie do migracji puli nośników danych ZFS</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazqr">Eksportowanie puli nośników danych ZFS</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazru">Określanie pul nośników danych gotowych do importu</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gaztk">Szukanie pul nośników danych ZFS w alternatywnych katalogach</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gazuf">Importowanie pul nośników danych ZFS</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gcfhw">Odzyskiwanie zniszczonych pul nośników danych ZFS</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-4.xml" targetptr="gcikw">Uaktualnianie pul nośników danych ZFS</olink>
</para>
</listitem>
</itemizedlist>
<sect2 xml:id="gazre">
<title>Przygotowanie do migracji puli nośników danych ZFS</title>
<para>Pule nośników danych muszą być ręcznie eksportowane, aby zaznaczyć, że są gotowe do migracji. Operacja ta zapisuje wszelkie niezapisane wcześniej dane, nanosi dane wskazujące, że wykonano eksport i usuwa wszelką znajomość puli z systemu.</para>
<para>Jeśli pula nie zostanie wyeksportowana, ale dyski zostaną po prostu usunięte z systemu, wciąż można zaimportować pulę w innym systemie. Należy jednak liczyć się z utratą kilku ostatnich sekund transakcji a pula będzie oznaczona jako uszkodzona na oryginalnym komputerze, ponieważ urządzenia są niedostępne. Domyślnie komputer docelowy odmawia importu puli, która nie była wyeksportowana. Jest to konieczne, aby zapobiec przypadkowemu importowi aktywnej puli składającej się z sieciowych nośników danych, które mogą być używane przez inny system.</para>
</sect2>
<sect2 xml:id="gazqr">
<title>Eksportowanie puli nośników danych ZFS</title>
<para>Eksport puli wykonuje się za pomocą komendy <command>zpool export</command>. Na przykład:</para>
<screen># <userinput>zpool export tank</userinput>
</screen>
<para>Po wykonaniu komendy pula <literal>tank</literal> jest niewidoczna dla systemu. Komenda próbuje odmontować wszelkie systemy plików w puli zamontowane w systemie. Jeśli odmontowanie któregokolwiek się nie uda, można odmontować je siłą przy użyciu opcji <option>f</option>.
Na przykład:</para>
<screen># <userinput>zpool export tank</userinput>
cannot unmount '/export/home/eschrock': Device busy
# <userinput>zpool export -f tank</userinput>
</screen>
<para>Jeśli w trakcie eksportu urządzenia są niedostępne, nie można oznaczyć dysków jako wyeksportowane. Jeśli jeden z tych dysków zostanie później podłączony do systemu bez żadnych działających urządzeń, będzie oznaczony jako potencjalnie aktywny. Jeśli woluminy ZFS w puli są używane, puli nie można wyeksportować nawet z użyciem opcji <option>
f</option>. Aby wyeksportować pulę z woluminem ZFS, najpierw należy upewnić się, że wszyscy użytkownicy wolumenu już go nie używają.<indexterm xml:id="indexterm-184">
<primary>eksportowanie</primary>
<secondary>puli nośników danych ZFS</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-185">
<primary>
<command>zpool export</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-186">
<primary>pule nośników danych ZFS</primary>
<secondary>eksportowanie</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<para>Więcej informacji o ZFS woluminach w <olink remap="external" targetdoc="chapter-8.xml" targetptr="gaypf">Woluminach ZFS</olink>.</para>
</sect2>
<sect2 xml:id="gazru">
<title>Określanie pul nośników danych gotowych do importu</title>
<para>Po usunięciu puli z systemu (przez wyeksportowanie lub usunięcie dysków), należy podłączyć urządzenia do docelowego systemu. Wprawdzie ZFS potrafi poradzić sobie w niektórych sytuacjach, kiedy tylko część urządzeń jest dostępna, wszystkie urządzenia w puli muszą zostać przeniesione między systemami. Urządzenia nie muszą zostać podłączone z tymi samymi nazwami. ZFS wykrywa wszelkie urządzenia o zmienionych nazwach i poprawia odpowiednio swoją konfigurację. Do wykrywania dostępnych pul służy komenda <command>zpool import</command> bez żadnych opcji. Na przykład:</para>
<screen># <userinput>zpool import</userinput>
  pool: tank
    id: 3778921145927357706
 state: ONLINE
action: The pool can be imported using its name or numeric identifier.
config:

        tank        ONLINE
          mirror    ONLINE
            c1t0d0  ONLINE
            c1t1d0  ONLINE</screen>
<para>W tym przykładzie pula <literal>tank</literal> jest dostępna do zaimportowania w systemie docelowym. Każda pula jest identyfikowana po nazwie oraz po unikalnym identyfikatorze liczbowym. Jeśli dostępnych jest kilka pul o tej samej nazwie, można je rozróżnić po identyfikatorach liczbowych.<indexterm xml:id="indexterm-187">
<primary>identyfikowanie</primary>
<secondary>puli nośników danych ZFS do importu (<command>zpool import -a</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-188">
<primary>
<command>zpool import -a</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-189">
<primary>pule nośników danych ZFS</primary>
<secondary>identyfikowanie do importu (<command>zpool import -a</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<para>Podobnie jak komenda <command>zpool status</command>, polecenie <command>zpool
import</command> odsyła do artykułu w bazie wiedzy zawierającego najaktualniejsze informacje odnośnie procedur naprawiania problemu, który uniemożliwia zaimportowanie puli. W tym przypadku, użytkownik może zmusić system do zaimportowania puli. Import puli podłączonej przez sieć i wykorzystywanej przez inny system może spowodować uszkodzenia danych i panikę jądra, ponieważ oba systemy będą usiłowały zapisywać na tych samych nośnikach. Jeśli nie wszystkie urządzenia w puli są dostępne, lecz zachowana jest wystarczająca redundancja aby pula była użyteczna, stan jej określany jest jako <literal>DEGRADED</literal>. Na przykład:</para>
<screen># <userinput>zpool import</userinput>
  pool: tank
    id: 3778921145927357706
 state: DEGRADED
status: One or more devices are missing from the system.
action: The pool can be imported despite missing or damaged devices.  The
        fault tolerance of the pool may be compromised if imported.
   see: http://www.sun.com/msg/ZFS-8000-2Q
config:

        tank         DEGRADED
          mirror     DEGRADED
            c1t0d0   UNAVAIL   cannot open
            c1t1d0   ONLINE</screen>
<para>W tym przykładzie pierwszy dysk jest uszkodzony lub w ogóle go nie ma, wciąż jednak można zaimportować pulę, ponieważ mirrorowane dane nadal są dostępne. Jeśli zbyt wiele urządzeń jest niedostępnych lub uszkodzonych, import puli nie powiedzie się. Na przykład:</para>
<screen># <userinput>zpool import</userinput>
  pool: dozer
    id: 12090808386336829175
 state: FAULTED
action: The pool cannot be imported. Attach the missing
        devices and try again.
   see: http://www.sun.com/msg/ZFS-8000-6X
config:
        raidz               FAULTED
          c1t0d0    ONLINE
          c1t1d0    FAULTED
          c1t2d0    ONLINE
          c1t3d0    FAULTED</screen>
<para>W tym przykładzie brakuje dwóch dysków w wirtualnym urządzeniu RAID-Z, co oznacza brak wystarczającej redundancji danych i niemożność odtworzenia puli. W tym przypadku ZFS nie wie, które jeszcze urządzenia są częścią puli, aczkolwiek stara się dostarczyć jak najwięcej informacji. Na przykład:</para>
<screen># <userinput>zpool import</userinput>
pool: dozer
    id: 12090808386336829175
 state: FAULTED
status: One or more devices are missing from the system.
action: The pool cannot be imported. Attach the missing
        devices and try again.
   see: http://www.sun.com/msg/ZFS-8000-6X
config:
        dozer          FAULTED   missing device
          raidz       ONLINE
            c1t0d0    ONLINE
            c1t1d0    ONLINE
            c1t2d0    ONLINE
            c1t3d0    ONLINE
        Additional devices are known to be part of this pool, though their
        exact configuration cannot be determined.</screen>
</sect2>
<sect2 xml:id="gaztk">
<title>Szukanie pul nośników danych ZFS w alternatywnych katalogach</title>
<para>Domyślnie komenda <command>zpool import</command> szuka urządzeń tylko w katalogu <filename>/dev/dsk</filename>. Jeśli urządzenie istnieje w innym katalogu lub używana jest pula zbudowana z plików, należy użyć opcji <option>d</option> w celu przeszukania innych katalogów. Na przykład:</para>
<screen># <userinput>zpool create dozer mirror /file/a /file/b</userinput>
# <userinput>zpool export dozer</userinput>
# <userinput>zpool import -d /file</userinput>
  pool: dozer
    id: 10952414725867935582
 state: ONLINE
action: The pool can be imported using its name or numeric identifier.
config:

        dozer        ONLINE
          mirror     ONLINE
            /file/a  ONLINE
            /file/b  ONLINE
# <userinput>zpool import -d /file dozer</userinput>
</screen>
<para>Jeśli urządzenia znajdują się w kliku katalogach można podać kilka opcji <option>d</option>.<indexterm xml:id="indexterm-190">
<primary>importowanie</primary>
<secondary>puli nośników danych ZFS z alternatywnych katalogów (<command>zpool import -d</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-191">
<primary>
<command>zpool import -d</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-192">
<primary>pule nośników danych ZFS</primary>
<secondary>importowanie z alternatywnych katalogów (<command>zpool import -d</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
</sect2>
<sect2 xml:id="gazuf">
<title>Importowanie pul nośników danych ZFS</title>
<para>Po zidentyfikowaniu puli do importu, można ją zaimportować przez wskazanie nazwy bądź liczbowego identyfikatora komendzie <command>zpool import</command>. Na przykład:</para>
<screen># <userinput>zpool import tank</userinput>
</screen>
<para>Jeśli kilka pul ma tę samą nazwę, można wskazać konkretną za pomocą liczbowego identyfikatora. Na przykład:</para>
<screen># <userinput>zpool import</userinput>
  pool: dozer
    id: 2704475622193776801
 state: ONLINE
action: The pool can be imported using its name or numeric identifier.
config:

        dozer       ONLINE
          c1t9d0    ONLINE

  pool: dozer
    id: 6223921996155991199
 state: ONLINE
action: The pool can be imported using its name or numeric identifier.
config:

        dozer       ONLINE
          c1t8d0    ONLINE
# <userinput>zpool import dozer</userinput>
cannot import 'dozer': more than one matching pool
import by numeric ID instead
# <userinput>zpool import 6223921996155991199</userinput>
</screen>
<para>Jeśli nazwa puli pokrywa się z nazwą już istniejącej puli, można zaimportować ją pod inną nazwą. Na przykład:</para>
<screen># <userinput>zpool import dozer zeepool</userinput>
</screen>
<para>Powyższa komenda zaimportowała pulę wyeksportowaną jako<literal>dozer</literal> przy użyciu nowej nazwy <literal>zeepool</literal>. Jeśli pula nie została czysto wyeksportowana, ZFS będzie wymagał flagi <option>f</option>, aby uniknąć przypadkowego zaimportowania puli, która jest nadal używana przez inny system. Na przykład:</para>
<screen># <userinput>zpool import dozer</userinput>
cannot import 'dozer': pool may be in use on another system
use '-f' to import anyway
# <userinput>zpool import -f dozer</userinput>
</screen>
<para>Pule można też importować z alternatywnym głównym katalogiem przy pomocy opcji <option>R</option> .
Więcej informacji o alternatywnych katalogach głównych w <olink remap="external" targetdoc="chapter-8.xml" targetptr="gbcgl">Używanie alternatywnych katalogów głównych pul nośników danych ZFS</olink>.<indexterm xml:id="indexterm-193">
<primary>importowanie</primary>
<secondary>puli nośników danych ZFS</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-194">
<primary>
<command>zpool import</command> <replaceable>name</replaceable>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-195">
<primary>pule nośników danych ZFS</primary>
<secondary>importowanie</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
</sect2>
<sect2 xml:id="gcfhw">
<title>Odzyskiwanie zniszczonych pul nośników danych ZFS</title>
<para>Zniszczoną pulę można odzyskać za pomocą komendy <command>zpool import</command> <option>
D</option>. Na przykład:</para>
<screen># <userinput>zpool destroy tank</userinput>
# <userinput>zpool import -D</userinput>
pool: tank
    id: 3778921145927357706
 state: ONLINE (DESTROYED)
action: The pool can be imported using its name or numeric identifier.  The
        pool was destroyed, but can be imported using the '-Df' flags.
config:

        tank        ONLINE
          mirror    ONLINE
            c1t0d0  ONLINE
            c1t1d0  ONLINE</screen>
<para>W powyższym wyniku komendy <command>zpool import</command> można zidentyfikować pulę jako zniszczoną ze względu na następującą informację o stanie:</para>
<screen>state: ONLINE (DESTROYED)</screen>
<para>Aby odzyskać zniszczoną pulę, należy wykonać komendę <command>zpool import</command> <option>D</option> i z flagą <option>f</option>.
Na przykład:</para>
<screen># <userinput>zpool import -Df tank</userinput>
# <userinput>zpool status tank</userinput>
  pool: tank
 state: ONLINE
 scrub: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        tank        ONLINE       0     0     0
          mirror    ONLINE       0     0     0
            c1t0d0  ONLINE       0     0     0
            c1t1d0  ONLINE       0     0     0

errors: No known data errors</screen>
<para>Jeśli jedno z urządzeń w zniszczonej puli jest niedostępne lub uszkodzone, odzyskanie zniszczonej puli może być niemożliwe. W takim przypadku należy najpierw zaimportować zdegradowana pulę, a następnie spróbować naprawić uszkodzenie. Na przykład:<indexterm xml:id="indexterm-196">
<primary>odzyskiwanie</primary>
<secondary>zniszczonej puli nośników danych ZFS</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-197">
<primary>
<command>zpool import -D</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-198">
<primary>pule nośników danych ZFS</primary>
<secondary>odzyskiwanie uszkodzonej puli</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool destroy dozer</userinput>
# <userinput>zpool import -D</userinput>
pool: dozer
    id: 
 state: DEGRADED (DESTROYED)
status: One or more devices are missing from the system.
action: The pool can be imported despite missing or damaged devices.  The
        fault tolerance of the pool may be compromised if imported.  The
        pool was destroyed, but can be imported using the '-Df' flags.
   see: http://www.sun.com/msg/ZFS-8000-2Q
config:

        dozer        DEGRADED
           raidz      ONLINE
            c1t0d0    ONLINE
            c1t1d0    ONLINE
            c1t2d0    UNAVAIL  cannot open
            c1t3d0    ONLINE
# <userinput>zpool import -Df dozer</userinput>
# <userinput>zpool status -x</userinput>
  pool: dozer
 state: DEGRADED
status: One or more devices could not be opened.  Sufficient replicas exist for
        the pool to continue functioning in a degraded state.
action: Attach the missing device and online it using 'zpool online'.
   see: http://www.sun.com/msg/ZFS-8000-D3
 scrub: resilver completed with 0 errors on Fri Mar 17 16:11:35 2006
config:

        NAME                     STATE     READ WRITE CKSUM
        dozer                    DEGRADED     0     0     0
          raidz                  ONLINE       0     0     0
            c1t0d0               ONLINE       0     0     0
            c1t1d0               ONLINE       0     0     0
            c1t2d0               UNAVAIL      0     0     0  cannot open
            c1t3d0               ONLINE       0     0     0

errors: No known data errors
# <userinput>zpool online dozer c1t2d0</userinput>
Bringing device c1t2d0 online
# <userinput>zpool status -x</userinput>
all pools are healthy</screen>
</sect2>
<sect2 xml:id="gcikw">
<title>Uaktualnianie pul nośników danych ZFS</title>
<para>W przypadku posiadania pul ZFS-a z poprzednich wersji Solarisa,
jak na przykład wydanie Solaris 10 6/06, można zaktualizować pule przy
użyciu polecenia <command>zpool upgrade</command>, żeby móc skorzystać
z dodatkowych możliwości, jakie zostały wprowadzone w wydaniu
Solaris 10 11/06. Dodatkowo polecenie <command>zpool status</command>
zostało zmodyfikowane, żeby informować, kiedy pule są w starszej
wersji. Na przykład:<indexterm xml:id="indexterm-199">
<primary>uaktualnianie</primary>
<secondary>pula nośników danych ZFS</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-200">
<primary>
<command>zpool upgrade</command>
</primary>
</indexterm>
<indexterm xml:id="indexterm-201">
<primary>pule nośników danych ZFS</primary>
<secondary>uaktualnianie</secondary>
<tertiary>opis</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool status</userinput>
  pool: test
 state: ONLINE
status: The pool is formatted using an older on-disk format.  The pool can
        still be used, but some features are unavailable.
action: Upgrade the pool using 'zpool upgrade'.  Once this is done, the
        pool will no longer be accessible on older software versions.
 scrub: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        test        ONLINE       0     0     0
          c1t27d0   ONLINE       0     0     0

errors: No known data errors</screen>
<para>Można użyć następującej składni polecenia, w celu uzyskania
dodatkowych informacji o aktualnej wersji i wspieranych wydaniach.</para>
<screen># <userinput>zpool upgrade -v</userinput>
This system is currently running ZFS version 3.

The following versions are supported:

VER  DESCRIPTION
---  --------------------------------------------------------
 1   Initial ZFS version
 2   Ditto blocks (replicated metadata)
 3   Hot spares and double parity RAID-Z

For more information on a particular version, including supported releases, see:

http://www.opensolaris.org/os/community/zfs/version/N

Where 'N' is the version number.</screen>
<para>Następnie można wykonać polecenie <command>zpool upgrade</command> w celu
uaktualnienia pul. Na przykład:</para>
<screen># <userinput>zpool upgrade -a</userinput>
</screen>
<note>

<para>W przypadku wykonania aktualizacji puli do najnowszej wersji,
nie będą one dostępne na systemach, które działają ze starszą wersją ZFS-a.</para>
</note>
</sect2>
</sect1>
</chapter>
