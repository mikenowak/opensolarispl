<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML//EN" "docbook.dtd"[
	<!ENTITY % xinclude SYSTEM "xinclude.mod">
	%xinclude;
]>

<chapter xmlns="http://docbook.org/ns/docbook" version="5.0" xml:id="gavwn">



<title>Zarządzanie pulami nośników danych ZFS</title>
<toc>
<para>W tym rozdziale opisane jest tworzenie i zarządzanie pulami nośników 
danych ZFS.</para>
<para>Podrozdziały:</para>
<itemizedlist>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gcfog">Składniki puli ZFS
</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gaypw">Tworzenie i usuwanie puli
 ZFS</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gayrd">Zarządzanie urządzeniami 
w puli ZFS</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gaynp">Sprawdzanie statusu puli 
ZFS</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gbchy">Przenoszenie puli ZFS
</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gcikw">Powiększanie puli ZFS
</olink>
</para>
</listitem>
</itemizedlist>
</toc>
<sect1 xml:id="gcfog">
<title>Składniki puli nośników danych ZFS</title>
<para>Poniższe paragrafy dostarczają dokładnych informacji o następujących 
komponentach puli nośników danych:</para>
<itemizedlist>
<listitem>
<para>Dyski</para>
</listitem>
<listitem>
<para>Pliki</para>
</listitem>
<listitem>
<para>Urządzenia wirtualne</para>
</listitem>
</itemizedlist>
<sect2 xml:id="gazdp">
<title>Używanie dysków w puli nośników danych ZFS</title>
<para>Najbardziej podstawowym elementem puli nośników danych jest fizyczny 
nośnik danych. Może nim być urządzenie blokowe o pojemności nie mniejszej niż 
128 MB. Najczęściej urządzeniem tym jest dysk twardy widoczny dla systemu 
w katalogu <filename>/dev/dsk</filename>.<indexterm xml:id="indexterm-81">
<primary>Pule nośników danych ZFS</primary>
<secondary>komponenty</secondary>
</indexterm>
<indexterm xml:id="indexterm-82">
<primary>komponenty</primary>
<secondary>pul nośników danych ZFS</secondary>
</indexterm>
</para>
<para>Nośnikiem danych może być cały dysk (<filename>c1t0d0</filename>), 
lub pojedynczy blok (ang. slice), (<filename>c0t0d0s7</filename>). Zaleca się 
używania całych dysków, w którym to przypadku dysk nie wymaga specjalnego 
formatowania. ZFS formatuje dysk używają etykiet EFI do przechowywania jednego 
bloku. W takim przyadku tablica partycji wyświetlana przez komendę <command>
format</command> wygląda podobnie:</para>
<screen>Current partition table (original):
Total disk sectors available: 71670953 + 16384 (reserved sectors)

Part      Tag    Flag     First Sector        Size        Last Sector
  0        usr    wm                34      34.18GB         71670953    
  1 unassigned    wm                 0          0              0    
  2 unassigned    wm                 0          0              0    
  3 unassigned    wm                 0          0              0    
  4 unassigned    wm                 0          0              0    
  5 unassigned    wm                 0          0              0    
  6 unassigned    wm                 0          0              0    
  7 unassigned    wm                 0          0              0    
  8   reserved    wm          71670954       8.00MB         71687337</screen>
<para>Aby użyć całego dysku, musi być nazwany zgodnie z konwencją Solarisa, na 
przykład <filename>/dev/dsk/cXtXdXsX</filename>. Niektóre sterowniki od 
niezależnych dostawców używają innych konwencji nazewniczych lub umieszczają 
dyski w innych katalogach, niż <filename>/dev/dsk</filename>. Aby użyć tych 
dysków, należy ręcznie je etykietować i udostępnić blok ZFS-owi.
<indexterm xml:id="indexterm-83">
<primary>Etykiety EFI</primary>
<secondary>opis</secondary>
</indexterm>
<indexterm xml:id="indexterm-84">
<primary>Etykiety EFI</primary>
<secondary>współpraca z ZFS</secondary>
</indexterm>
</para>
<para>ZFS nadaje etykietę EFI w momencie tworzenia puli przy użyciu całego 
dysku. Można też etykietować dyski tadycyjnym Solarisowym VTOC przy tworzeniu 
puli z użyciek bloku.</para>
<para>Bloki powinny być używane tylko w poniższych wrunkach:</para>
<itemizedlist>
<listitem>
<para>Nazwa urządzenia jest niestandardowa.</para>
</listitem>
<listitem>
<para>Pojedynczy dysk jest wpółdzielony między system plików ZFS oraz inny 
system plików, na przykład UFS.</para>
</listitem>
<listitem>
<para>Dysk jest używany na potrzeby swap bądź zrzutów (dump).</para>
</listitem>
</itemizedlist>
<para>Dyski można wskazać zarówno poprzez użycie pełnej ścieżki, na przykład 
<filename>/dev/dsk/c1t0d0</filename>, lub przez nazwę urządzenia znajdującego 
się w katalogu<filename>/dev/dsk</filename>, na przykład <filename>c1t0d0
</filename>. Poniższe są prawidłowymi nazwami dysków:
<indexterm xml:id="indexterm-85">
<primary>pule nośników danych ZFS</primary>
<secondary>używanie całego dysku</secondary>
</indexterm>
<indexterm xml:id="indexterm-86">
<primary>całe dyski</primary>
<secondary>jako komponenty pul nośników danych ZFS</secondary>
</indexterm>
<indexterm xml:id="indexterm-87">
<primary>dyski</primary>
<secondary>jako komponenty pul nośników danych ZFS</secondary>
</indexterm>
</para>
<itemizedlist>
<listitem>
<para>
<filename>c1t0d0</filename>
</para>
</listitem>
<listitem>
<para>
<filename>/dev/dsk/c1t0d0</filename>
</para>
</listitem>
<listitem>
<para>
<filename>c0t0d6s2</filename>
</para>
</listitem>
<listitem>
<para>
<filename>/dev/foo/disk</filename>
</para>
</listitem>
</itemizedlist>
<para>ZFS działa najlepiej przy użyciu całych dysków. Wprawdzie tworzenie dysków
 logicznych przy użyciu zarządców woluminów jest możliwe, na przykład Solaris 
Volume Manager (SVM), Veritas Volume Manager (VxVM), bądź sprzętowych zarządców 
woluminów (LUN-y lub sprzętowe RAID-Y), konfiguracje takie nie są zalecane. 
O ile ZFS działa z takimi urządzeniami prawidłowo, wynikiem może być 
nieoptymalna wydajność.</para>
<para>Dyski identyfikuje się za równo za pomocą ścieżki, jak i ich ID, jeśli 
jest dotępne. Metoda ta pozwala na rekonfigurowanie urządzeń w systemie bez 
uaktualniania stanu ZFS. Jesli dysk zostanie przeniesiony z kontrolera 1., 
do kontrolera 2., ZFS użyje ID urządzenia do wykrycia go i ustalenia, że powinno
 być teraz używane za pośrednictwem kontrolera 2. ID urządzenia jest unikalne 
dla firmware'u. Wprawdzie mało prawdopodobna jest zmiana ID urządzenia, jednak 
niektóre uaktualnienia firmware'u dokonywały takich zmian. W takiej sytuacji, 
ZFS może użyć urządzenia za pośrednictwem jego ścieżki systemowej i 
automatycznie uaktualnić przechowywane ID urządzenia. Jeśli dokonano zarówno 
zmiany ścieżki jak i ID urządzenia, aby możliwe było użycie puli, należy 
wyeksportować pulę, a później ją zaimportować.</para>
</sect2>
<sect2 xml:id="gazcr">
<title>Używanie plików w pulach ZFS</title>
<para>ZFS umożliwia korzystanie z plików UFS jako urządzeń w puli. Opcja ta jest
 pomyślana głównie w celach testowych i do prostych eksperymentów, nie zaś do 
użycia w środowisku produkcyjnym. Powodem jest to, że <emphasis role="strong">
spójność plików zależy od systemu plików, na których pliki są przechowywane.
</emphasis>. Jeśli zostanie utworzona pula ZFS na bazie plików przechowywanych 
na systemie plików UFS, wtedy pośrednio żąda się gwarancji semantyki 
synchornicznej oraz poprawności od systemu plików UFS.
<indexterm xml:id="indexterm-88">
<primary>Pule nośników danych ZFS</primary>
<secondary>używanie plików</secondary>
</indexterm>
<indexterm xml:id="indexterm-89">
<primary>pliki</primary>
<secondary>jako komponenty pul nośników danych ZFS</secondary>
</indexterm>
</para>
<para>Pliki mogą się jednak okazać przydatne przy pierwszych próbach z ZFS-em, 
bądź przy eksperymentowaniu z bardziej skomplikowanymi układami, kiedy nie ma 
wystarczającej ilości prawdziwych nośników. Wszystkie pliki należy wskazywać za 
pomocą pełnej ścieżki dostępu i muszą mieć przynajmniej 128MB. Jeśli plik 
zostanie przeniesiony bądź zmieniona będzie jego nazwa, pula musi zostać 
wyeksoprtowana i zaimportowana, ponieważ plikom nie przypisuje się żadnego 
unikalnego ID, po którym mogłyby być znów odnaleznione.</para>
</sect2>
<sect2 xml:id="gazca">
<title>Urządzenia wirtualne w pulach nośników danych</title>
<para>Każda pula nośników składa się z jednego lub więcej wirtualnych urządzeń. 
<emphasis>Urządzenie wirtualne</emphasis> to wewnętrzna reprezentacja puli 
nośników, opisująca układ fizycznych nośników i ich charakterystyki awaryjności.
 Wirtualne urządzenie jako takie reprezentuje urządzenia dyskowe lub pliki, 
używane do tworzenia puli. <indexterm xml:id="indexterm-90">
<primary>Pule nośników danych ZFS</primary>
<secondary>urządzenia wirtualne</secondary>
</indexterm>
<indexterm xml:id="indexterm-91">
<primary>urządzenia wirtualne</primary>
<secondary>jako komponenty pul nośników danych ZFS</secondary>
</indexterm>
</para>
<para>Dwa urządzenia wirtualne najwyższego poziomu (top level) zapewniają 
redundancję danych: mirror i RAID-Z. Urządzenia te składają się z dysków, 
bloków dysków lub plików. </para>
<para>Dyski, bloki dysków i pliki używane w pulach poza mirrorami i RAID-Z 
same funkcjonują jako urządzenia wirtualne najwyższego poziomu. </para>
<para>Pule nośników danych zawierają zazwyczaj wiele urządzeń wirtualnych 
najwyższego poziomu. ZFS dynamicznie rozdziela dane pomiędzy wszystkie 
urządzenia najwyższego poziomu w puli.</para>
</sect2>
</sect1>
<sect1 xml:id="gcfof">
<title>Opcje replikacji danych w pulach ZFS</title>
<para>ZFS zapewnia dwa poziomy reduncjanji danych w konfiguracji mirror i 
RAID-Z.<indexterm xml:id="indexterm-92">
<primary>opcje replikacji danych w ZFS</primary>
<secondary>mirror lub RAID-Z</secondary>
</indexterm>
</para>
<sect2 xml:id="gamss">
<title>Konfiguracja puli nośników danych: mirror</title>
<para>Konfiguracja lustrzana (mirrored) puli nośników danych wymaga użycia 
przynajmniej dwóch dysków, najlepiej na osobnych kontrolerach. W konfiguracji 
lustrzanej można używać wielu dysków. Dodatkowo można stworzyć więcej niż jedno 
lustro w puli. Prosta konfiguracja lustrzana wygląda podobnie do poniższej:
<indexterm xml:id="indexterm-93">
<primary>konfiguracja lustrzana</primary>
<secondary>opis</secondary>
</indexterm>
<indexterm xml:id="indexterm-94">
<primary>konfiguracja lustrzana</primary>
<secondary>opcja replikacji</secondary>
</indexterm>
<indexterm xml:id="indexterm-95">
<primary>konfiguracja lustrzana</primary>
<secondary>koncepcja</secondary>
</indexterm>
<indexterm xml:id="indexterm-96">
<primary>pule nośników danych ZFS</primary>
<secondary>konfiguracja lustrzana, opis</secondary>
</indexterm>
</para>
<screen>mirror c1t0d0 c2t0d0</screen>
<para>Bardziej skomplikowana konfiguracja będzie podobna do poniższej:</para>
<screen>mirror c1t0d0 c2t0d0 c3t0d0 mirror c4t0d0 c5t0d0 c6t0d0</screen>
<para>Więcej inforomacji o tworzeniu lustrzanej konfiguracji puli nośników 
danych w <olink targetdoc="" remap="internal" targetptr="gazhv">tworzeniu 
lustrzanych pul nośników danych</olink>.</para>
</sect2>
<sect2 xml:id="gamtu">
<title>Konfiguracja puli nośników danych RAID-Z</title>
<para>Oprócz konfiguracji lustrzanej puli nośników danych, ZFS umożliwia 
konfigurację RAID-Z. RAID-Z  jest podobne do RAID-5.
<indexterm xml:id="indexterm-97">
<primary>konfiguracja RAID-Z</primary>
<secondary>opis</secondary>
</indexterm>
<indexterm xml:id="indexterm-98">
<primary>konfiguracja RAID-Z</primary>
<secondary>opcja replikacji</secondary>
</indexterm>
<indexterm xml:id="indexterm-99">
<primary>konfiguracja RAID-Z</primary>
<secondary>koncepcja</secondary>
</indexterm>
<indexterm xml:id="indexterm-100">
<primary>pule nośników danych ZFS</primary>
<secondary>konfiguracja RAID-Z, opis</secondary>
</indexterm>
</para>
<para>Wszystkie tradycyjne algorytmy RAID-5 i podobne (RAID-4. RAID-5. RAID-6, 
RDP i PARZYSTE-NIEPARZYSTE na przykład) obarczone są problemem tak zwanej 
"luki zapisu RAID-5". Jeśli tylko fragment bloku danych RAID-5 zostanie zapisany
 i nastąpi awaria zasilania zanim wszystkie bloki zostaną zapisane na dysk, 
przystość i dane będą rozsynchronizowane, a przez to bezużyteczne na zawsze, 
chyba że następny zapis pełnego bloku nadpisze ten fragment. W RAID-Z ZFS używa 
bloków o zmiennej długości, zatem każdy zapis zapis jest zapisem pełnego bloku. 
Jest to możliwe tylko dlatego, że ZFS integruje system plików i zarządzanie 
urządzeniami w taki sposób, że metadane systemu plików zawierają wystarczająco 
dużo informacji o modelu replikacji w niższej warstwie, aby poradzić sobie z 
blokami RAID o zmiennej długości. RAID-Z to pierwszy to pierwsze na świecie 
czysto programowe rozwiązanie problemu luki zapisu RAID-5.</para>
<para>Do konfiguracji RAID-Z wymagane są co najmniej dwa dyski. Obecnie RAID-Z 
zapewnia pojedyczną parzystość. Na przykład, jeśli w puli RAID-Z są trzy dyski, 
dane parzystości zajmują miejsce równe pojemności jednego z dysków.</para>
<para>Koncepcyjnie, konfiguracja RAID-Z z trzema dyskami wygląda jak poniżej:
</para>
<screen>raidz c1t0d0 c2t0d0 c3t0d0</screen>
<para>Bardziej skomplikowana konfiguracja RAID-Z wygląda podobnie do poniższej:
</para>
<screen>raidz c1t0d0 c2t0d0 c3t0d0 c4t0d0 c5t0d0 c6t0d0 c7t0d0 raidz c8t0d0 c9t0d0 c10t0d0 c11t0d0 c12t0d0 c13t0d0 c14t0d0</screen>
<para>Jeśli tworzona jest konfiguracja z wieloma dyskami, jak w powyższym 
przykładzie, konfiguracja RAID-Z z 14 dosyami sprawdza się lepiej jako dwie 
konfiguracje RAID-Z z 7 dyskami każda. Konfiguracje RAID-Z grupami o 
jednocyfrowej liczbie dysków powinny mieć lepszą wydajność.</para>
<para>Więcej informacji o tworzeniu pul RAID-Z w 
<olink targetdoc="" remap="internal" targetptr="gazhe">Tworzenie pul RAID-Z
</olink>.</para>
</sect2>
<sect2 xml:id="gazch">
<title>Samonaprawiające się dane w konfiguracjach z replikacją</title>
<para>W konfiguracji lustrzanej lub RAID-Z ZFS zapewnia samonaprawianie się 
danych.</para>
<para>Kiedy wykryty zostanie blok z uszkodzonymi danymi, ZFS nie tylko pobiera 
poprawne dane z innej kopii, ale również naprawia uszkodzone dane zastępując je 
poprawną kopią.<indexterm xml:id="indexterm-101">
<primary>samonaprawiające się dane</primary>
<secondary>opis</secondary>
</indexterm>
</para>
</sect2>
<sect2 xml:id="gazdd">
<title>Dynamiczny striping w pulach danych</title>
<para>Dla każdego urządzenia wirtualnego dodanego do puli, ZFS dynamicznie 
zapisuje dane równomiernie na wszystkich dostępnych dyskach (stripe). Decyzja 
o tym, gdzie zapisać dane podejmowana jest w momencie zapisu, nie pojawiają się 
zatem bloki o stałej wielkości w momencie alokacji.
<indexterm xml:id="indexterm-102">
<primary>dynamiczny striping</primary>
<secondary>opis</secondary>
</indexterm>
<indexterm xml:id="indexterm-103">
<primary>dynamiczny striping</primary>
<secondary>możliwości puli nośników danych</secondary>
</indexterm>
<indexterm xml:id="indexterm-104">
<primary>Pule nośników danych ZFS</primary>
<secondary>dynamiczny striping</secondary>
</indexterm>
</para>
<para>Kiedy urządzenia wirtualne dodawane są do puli. ZFS stopniowo zapisuje 
dane na dwym urządzeniu w celu zapewnienia wydajności i zgodności z regułami 
alokacji przestrzeni dyskowej. Każde urządzenie wirtualne może być także lustrem
 lub urządzeniem RAID-Z zawierającym inne urządzenia dyskowe lub pliki. 
Konfiguracja taka pozwala na elastyczność w kontrolowaniu charakterystyk 
awaryjności w puli. Na przykład, można utworzyć następującą puę z 4 dysków:
</para>
<itemizedlist>
<listitem>
<para>Cztery dyski używające dynamicznego stripingu</para>
</listitem>
<listitem>
<para>Jedna konfiguracja RAID-Z z czterema urządzeniami</para>
</listitem>
<listitem>
<para>Dwuurządzeniowy mirror z użyciem dynamicznego stripingu</para>
</listitem>
</itemizedlist>
<para>Wprawdzie ZFS umożliwia kombinowanie różnych typów urządzeń wirtualnych w 
jednej puli, jednak praktyka taka jest niezalecana. Można, na przykład, utworzyć
 pulę z dwuurządzeniowym mirrorerm i trzyurządzeniowym RAID-Z. Odporność na 
awarie jest jednak tak dobra, jak dobre jest najgorsze urządzenie wirtualne, w 
tym przypadku RAID-Z. Zalecaną praktyką jest stosowanie takich samych urządzeń 
na tym samym poziomie replikacji w każdym urządzeniu.</para>
</sect2>
</sect1>
<sect1 xml:id="gaypw">
<title>Tworzenie i usuwanie pul nośników danych ZFS</title>
<para>Zgodnie z założeniami projektowymi, tworzenie i usuwanie pul jest proste i
 szybkie. Należy jednak zachować ostrożność przy wykonywaniu tych operacji. 
Wprawdzie ZFS stara się uniemożliwić włączenie już używanych urządzeń do nowej 
puli, nie zawsze jednak może wiedzieć, kiedy urządzenie jest już wykorzystywane 
w jakiejś puli. Usuwanie puli jest jeszcze prostsze. Komendy <command>zpool 
destroy</command> należy używać ostrożnie. To prosta komenda o poważnych 
konsekwencjach. Więcej informacji o usuwaniu pul w 
<olink targetdoc="" remap="internal" targetptr="gammr">Usuwaniu pul nosników 
danych ZFS</olink>.<indexterm xml:id="indexterm-105">
<primary>tworzenie</primary>
<secondary>pul nośników danych ZFS</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-106">
<primary>usuwanie</primary>
<secondary>pul nośników danych ZFS</secondary>
<tertiary>opis</tertiary>
</indexterm>
</para>
<sect2 xml:id="gaynr">
<title>Tworzenie puli nośników danych ZFS</title>
<para>Do tworzenie puli nośników danych służy komenda <command>zpool create
</command>. Argumentami są: nazwa puli i dowolna liczba urządzeń wirtualnych. 
Nazwa puli musi spełniać wymogi nazewnicze wyjaśnione w 
<olink targetdoc="" remap="internal" targetptr="gbcpt">Wymogi nazewnicze 
komponentów ZFS</olink>.<indexterm xml:id="indexterm-107">
<primary>tworzenie</primary>
<secondary>Puli nośników danych ZFS (<command>zpool create</command>)
</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-108">
<primary>
<command>zpool create</command>
</primary>
<secondary>prosta pula</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-109">
<primary>Pule nośników danych ZFS</primary>
<secondary>tworzenie (<command>zpool create</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<sect3 xml:id="gazgt">
<title>Tworzenie prostej puli nośników danych</title>
<para>Poniższa komenda tworzy nową pulę o nazwie <filename>tank</filename> 
składającą się z dysków <filename>c1t0d0</filename> i <filename>c1t1d0
</filename>:</para>
<screen># <userinput>zpool create tank c1t0d0 c1t1d0</userinput>
</screen>
<para>Dyski te znajdują się w katalogu <filename>/dev/dsk</filename> i zostaną 
zaetykietowane przez ZFS tak, aby zawierały jeden duży blok. Dane będą 
dynamicznie stripowane po wszystkich dyskach.</para>
</sect3>
<sect3 xml:id="gazhv">
<title>Tworzenie lustrzanej konfiguracji puli nośników danych</title>
<para>Do stworzenia lustrzanej puli służy słowo kluczowe <literal>mirror
</literal>, po którym następuje dowolna liczba dysków, z których będzie się 
składało lustro. Poprzez powtarzanie słowa kluczowego <literal>mirror</literal> 
można stworzyć wiele luster. Poniższa komenda tworzy pulę z dwoma 
dwuurządzeniowymi lustrami:</para>
<screen># <userinput>zpool create tank mirror c1d0 c2d0 mirror c3d0 c4d0
</userinput>
</screen>
<para>Drugie słowo kluczowe <literal>mirror</literal> wskazuje, że konfigurowane
 jest nowe urządzenie wirtualne najwyższej warstwy. Dane są dynamicznie 
stripowane na oba lustra z replikacją pomiędzy odpowiednimi dyskami.
<indexterm xml:id="indexterm-110">
<primary>tworzenie</primary>
<secondary>lustrzanej puli nośników danych ZFS (<command>zpool create</command>)
</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-111">
<primary>
<command>zpool create</command>
</primary>
<secondary>lustrzana konfiguracja puli nośników danych</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-112">
<primary>lustrzana konfiguracja puli nośników danych (<command>zpool create
</command>)</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-113">
<primary>Pule nośników danych ZFS</primary>
<secondary>tworzenie konfiguracji lustrzanej (<command>zpool create</command>)
</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
</sect3>
<sect3 xml:id="gazhe">
<title>Tworzenie pul nośników danych RAID-Z</title>
<para>Tworzenie puli RAID-Z jest identyczne jak tworzenie puli lustrzanej, z 
wyjątkiem słowa kluczowego <literal>raidz</literal> zamiast <literal>mirror
</literal>. Poniższy przykład pokazuje, jak stworzyć pulę z pojedynczym 
urządzeniem RAID-Z składającym się z pięciu dysków:
<indexterm xml:id="indexterm-114">
<primary>tworzenie</primary>
<secondary>pule nośników danych RAID-Z (<command>zpool create</command>)
</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-115">
<primary>konfiguracja RAID-Z</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-116">
<primary>
<command>zpool create</command>
</primary>
<secondary>pula nośników danych RAID-Z</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-117">
<primary>Pule nośników danych ZFS</primary>
<secondary>tworzenie konfiguracji RAID-Z (<command>zpool create</command>)
</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool create tank raidz c1t0d0 c2t0d0 c3t0d0 c4t0d0 /dev/dsk/c5t0d0</userinput>
</screen>
<para>Przykłąd ten pokazuje, że dyski można wskazywać ich pełnymi scieżkami. 
Urządzenie <filename>/dev/dsk/c5t0d0</filename> jest identyczne z urządzeniem 
<filename>c5t0d0</filename>.</para>
<para>Podobna konfiguracja może zostać stworzona przy pomocy bloków dysków. Na 
przykład:</para>
<screen># <userinput>zpool create tank raidz c1t0d0s0 c2t0d0s0 c3t0d0s0 c4t0d0s0 c5t0d0s0</userinput>
</screen>
<para>Dyski te muszą jednak być odpowiednio sformatowane, aby mieć właściwą 
wielkość bloku zero.</para>
<para>Więcej informacji o konfiguracji RAID-Z w 
<olink targetdoc="" remap="internal" targetptr="gamtu">Konfiguracji RAID-Z pul 
nośników danych</olink>.</para>
</sect3>
</sect2>
<sect2 xml:id="gazhs">
<title>Obsługa błedów przy tworzeniu pul nośników danych ZFS</title>
<para>Błędy przy tworzeniu puli nośników mogą wystąpić z wielu powodów. Część z 
nich jest oczywista, na przykład brak urządzenia, jednak inne błędy są bardziej 
subtelne.</para>
<sect3 xml:id="gazht">
<title>Wykrywanie używanych urządzeń</title>
<para>Przed sformatowaniem dysku, ZFS sprawdza, czy dysk jest używany przez ZFS 
lub inną część systemu operacyjnego. Jeśli dysk jest używany, może wystąpić błąd
 podobny do poniższego:</para>
<screen># <userinput>zpool create tank c1t0d0 c1t1d0</userinput>
invalid vdev specification
use '-f' to override the following errors:
/dev/dsk/c1t0d0s0 is currently mounted on /
/dev/dsk/c1t0d0s1 is currently mounted on swap
/dev/dsk/c1t1d0s0 is part of active ZFS pool 'zeepool'
Please see zpool(1M)</screen>
<para>Niektóre z tych błędów można obejść przy użyciu opcji <option>
f</option>, ale większości się nie da. Poniższych użyć nie uda się obejść za 
pomocą opcji <option>
f</option> i należy usunąć je ręcznie:<indexterm xml:id="indexterm-118">
<primary>wykrywanie</primary>
<secondary>używanych urządzeń</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-119">
<primary>używane urządzenia</primary>
<secondary>wykrywanie</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<variablelist>
<varlistentry>
<term>
<emphasis role="strong">Zamontowane systemy plików</emphasis>
</term>
<listitem>
<para>Dysk lub jeden z bloków zawiera system plików zamontowany w systemie. Błąd
 ten należy poprawić komendą <command>umount</command>.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong">System plików w /etc/vfstab</emphasis>
</term>
<listitem>
<para>Dysk zawiera system plików obecny w pliku <filename>/etc/vfstab</filename>
, ale nie jest on zamontowany. Błąd ten można poprawić usuwając lub oznaczjąć 
jako komentarz linię w pliku <filename>/etc/vfstab</filename>.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong">Urządzenie przeznaczone na zrzuty pamięci (dump)
</emphasis>
</term>
<listitem>
<para>Dysk jest używany jako urządzenie przeznaczone na zrzuty pamięci (dump). 
Błąd ten można poprawić komendą <command>dumpadm</command>.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong">Część puli ZFS</emphasis>
</term>
<listitem>
<para>Dysk lub plik jest częścią puli ZFS. Błąd ten można naprawić niszcząc pulę
 przy użyciu komendy <command>zpool</command>.</para>
</listitem>
</varlistentry>
</variablelist>
<para>Następujące testy na użycie urządzeń służą tylko jako pomocne ostrzeżenia 
i można je obejść opcją <option>
f</option> przy tworzeniu puli:</para>
<variablelist>
<varlistentry>
<term>
<emphasis role="strong">Zawiera system plików</emphasis>
</term>
<listitem>
<para>Dysk zawiera system plików, ale nie jest zamontowany i wydaje się być 
nieużywany.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong">Część woluminu</emphasis>
</term>
<listitem>
<para>Dysk jest częścią woluminu SVM.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong">Live upgrade</emphasis>
</term>
<listitem>
<para>Dysk jest używany jako alternatywne otoczenie startowe dla Solaris Live 
Upgrade.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong">Część wyeksportowanej puli ZFS</emphasis>
</term>
<listitem>
<para>Dysk jest częścią puli nośników wyeksportowanej lub ręcznie usuniętej z 
systemu. W drugim przypadku pulę przedstawia się jako <literal>potencjalnie 
aktywną</literal>, ponieważ dysk może, ale nie musi być dyskiem podłączonym 
przez sieć i używanym przez inny system. Obchodzenie potencjalnie aktywnej puli 
należy wykonywać bardzo ostrożnie.</para>
</listitem>
</varlistentry>
</variablelist>
<para>Poniższy przykład demonstruje użycie opcji <option>f</option>:</para>
<screen># <userinput>zpool create tank c1t0d0</userinput>
invalid vdev specification
use '-f' to override the following errors:
/dev/dsk/c1t0d0s0 contains a ufs filesystem
# <userinput>zpool create -f tank c1t0d0</userinput>
</screen>
<para>Najlepiej jest poprawiać błędy, niż je obchodzić przy pomocy opcji 
<option>f</option>.</para>
</sect3>
<sect3 xml:id="gazgc">
<title>Niepasujące poziomy replikacji</title>
<para>Tworzenie pul z urządzeniami wirtualnymi o różnych poziomach replikacji 
jest niezalecane. Komenda <command>zpool</command> próbuje zapobiec utworzeniu 
puli z niepasującymi poziomami replikacji. Przy próbie utworzenia takiej 
konfiguracji puli, pojawią się błedy podobne do poniższych:
<indexterm xml:id="indexterm-120">
<primary>wykrywanie</primary>
<secondary>niepasujących pozniomów replikacji</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-121">
<primary>niepasujące poziomy replikacji</primary>
<secondary>wykrywanie</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool create tank c1t0d0 mirror c2t0d0 c3t0d0</userinput>
invalid vdev specification 
use '-f' to override the following errors:
mismatched replication level: both disk and mirror vdevs are present
# <userinput>zpool create tank mirror c1t0d0 c2t0d0 mirror c3t0d0 c4t0d0 c5t0d0</userinput>
invalid vdev specification
use '-f' to override the following errors:
mismatched replication level: 2-way mirror and 3-way mirror vdevs are present
</screen>
<para>Błędy te można obejść za pomocą opcji <option>f</option>, ale postępowanie
 takie jest niezalecane. Komenda ostrzeże również o tworzeniu puli mirror lub 
RAID-Z z użyciem urządzeń o różnych rozmiarach. Konfiguracja taka jest wprawdzie
 dozwolona, niepasujące poziomy replikacji powodują marnowanie przestrzeni na 
większym urządzeniu i wymaga użycia opcji <option>f</option>, aby wyłączyć 
ostrzeżenie.</para>
</sect3>
<sect3 xml:id="gazhd">
<title>Suchy przebieg tworzenia puli nośników danych</title>
<para>Tworzenie puli może się niespodziewanie nie udać na wiele sposobów, a 
formatowanie dysków może być szkodliwą czynnością, komenda <command>zfs
create</command> ma dodatkową opcję <option>
n</option>, symulującą tworzenie puli bez zapisu danych na dysku. Opcja ta 
wykonuje testy na użyciu urządzeń i poprawność poziomów replikacji i informuje o
 wszelkich błędach. Jeśli nie ma błędów, pojawi się wynik podobny do poniższego:
<indexterm xml:id="indexterm-122">
<primary>suchy przebieg</primary>
<secondary>tworzenie puli nośników danych ZFS (<command>zpool create</command> 
<option>n</option>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-123">
<primary>
<command>zpool create</command> <option>n</option>
</primary>
<secondary>suchy przebieg</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-124">
<primary>Pue nośników danych ZFS</primary>
<secondary>wykonywanie suchego przebiegu (<command>zpool create</command> 
<option>n</option>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool create -n tank mirror c1t0d0 c1t1d0</userinput>
would create 'tank' with the following layout:

        tank
          mirror
            c1t0d0
            c1t1d0</screen>
<para>Niektórych błędów nie można wykryć bez rzeczywistego tworzenia puli. 
Najczęstszym przykłądem jest dwukrotne wskazanie tego samego urządzenia 
dwukrotnie w tej samej konfiguracji. Błędu tego nie można wykryć bez próby 
zapisu danych, dlatego komenda <command>create -n</command> może poinformowac o 
sukcesie, ale faktyczne utworznie puli może się nie udać.</para>
</sect3>
<sect3 xml:id="gbeef">
<title>Domyślny punkt montowania pul</title>
<para>Po utworzeniu puli, domyślnym punktem montowania datasetu jest katalog 
<replaceable>/nazwa-puli</replaceable>. Tego katalogu nie powinno być, a jeśli 
jest, powinien być pusty. Jeśli nnie istnieje, jest tworzony automatycznie. 
Jeśli istnieje, dataset jest automatycznie montowany w istniejącym katalogu. 
Aby utworzyć pulę z innym domyślnym punktem montowania należy użyć opcji 
<option>m</option> przy wykonywaniu komendy <command>zpool create</command>:
<indexterm xml:id="indexterm-125">
<primary>punkt montowania</primary>
<secondary>domyślny dla puli nośników danych ZFS</secondary>
</indexterm>
<indexterm xml:id="indexterm-126">
<primary>pule nośników danych ZFS</primary>
<secondary>domyślny punkt montowania</secondary>
</indexterm>
</para>
<screen># <userinput>zpool create home c1t0d0</userinput>
default mountpoint '/home' exists and is not empty
use '-m' option to specify a different default
# <userinput>zpool create -m /export/zfs home c1t0d0</userinput>
</screen>
<para>Komenda ta tworzy nową pulę <literal>home</literal> i dataset <literal>
home</literal> z punktem montowania w <filename>/export/zfs</filename>.</para>
<para>Więcej informacji o punktach montowania w 
<olink targetdoc="" remap="internal" targetptr="gaztn">Zarządzaniu punktami 
montowania ZFS</olink>.</para>
</sect3>
</sect2>
<sect2 xml:id="gammr">
<title>Niszczenie puli nośników danych ZFS</title>
<para>Pule niszczy się przy użyciu komendy <command>zpool destroy</command>. 
Niszczy ona pulę nawet wtedy, kiedy zawiera ona datasety.
<indexterm xml:id="indexterm-127">
<primary>niszczenie</primary>
<secondary>pul nośników danych ZFS (<command>zpool destroy</command>)
</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-128">
<primary>
<command>zpool destroy</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-129">
<primary>pule nośników danych ZFS</primary>
<secondary>niszczenie (<command>zpool destroy</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool destroy tank</userinput>
</screen>
<caution>

<para>Przy niszczeniu pul należy zachować daleko idącą ostrożność. Należy 
upewnić się, że niszczona jest właściwa pula i że istnieją kopie zapasowe 
danych. Po przypadkowym zniszczeniu niewłaściwej puli można spróbować ją 
odzyskać. 
Więcej informacji w <olink targetdoc="" remap="internal" targetptr="gcfhw">
Odzyskiwaniu zniszczonych pul nośników danych ZFS</olink>.</para>
</caution>
<sect3 xml:id="gazhm">
<title>Niszczenie puli z uszkodzonym urządzeniem</title>
<para>Niszczenie puli wymaga zapisu na dysku danych wskazujących, że pula 
została zniszczona. Dane te zapewniają, że przy imporcie urządzenia nie będą 
przestawiały się jako potencjalna pula. Jeśli jedno lub więcej urządzeń jest 
niedostępnych, nadal można zniszczyć pulę. Dane o usunięciu puli nie będą jednak
 zapisane na tych urządzeniach.</para>
<para>Urządzenia te po naprawieniu będą przedstawiane jako <emphasis>
potencjalnie aktywne</emphasis> przy tworzeniu nowej puli i przedstawią się jako
 prawidłowe urządzenia przy importowaniu puli. Jeśli w puli jest wystarczająco 
uszkodzonych urządzeń aby cała pula była uszkodzona (co oznacza, że urządzenie 
wirtualne najwyższego poziomu jest uszkodzone), wtedy komenda zgłosi błąd i nie
 wykona się bez użycia opcji <option>f</option>. Opcja ta jest konieczna, 
ponieważ nie mozna otowrzyć puli i nie można ustalić, czy są w niej 
przechowywane dane. Na przykład:</para>
<screen># <userinput>zpool destroy tank</userinput>
cannot destroy 'tank': pool is faulted
use '-f' to force destruction anyway
# <userinput>zpool destroy -f tank</userinput>
</screen>
<para>Więcej informacji o zdrowiu pul i urządzeń w 
<olink targetdoc="" remap="internal" targetptr="gamno">Zdrowie pul nośników 
danych ZFS</olink>.</para>
<para>Więcej informacji o importowaniu pul w 
<olink targetdoc="" remap="internal" targetptr="gazuf">Importowaniu pul nośników
 danych ZFS</olink>.</para>
</sect3>
</sect2>
</sect1>
<sect1 xml:id="gayrd">
<title>Zarządzanie urządzeniami w pulach nośników danych ZFS</title>
<para>Większość podstawowych informacji dotyczących urządzeń znajduje się w
<olink targetdoc="" remap="internal" targetptr="gcfog">Komponentach 
puli nośników danych ZFS</olink>. Po utworzeniu puli możliwych jest kilka 
akcji służących zarządzaniu fizycznym urządzeniami w puli.</para>
<sect2 xml:id="gazgw">
<title>Dodawanie urządzeń do puli nośników danych</title>
<para>Poprzez dodanie do puli nowego wirtualnego urządzenia najwyższego poziomu,
 możliwe jest dynamiczne powiększenie przestrzeni dostępnej w puli. Przestrzeń 
ta jest automatycznie dostępna dla wszystkich datasetów w puli. Nowe urządzenie 
wirtualne dodaje się do puli za pomocą komendy <command>zpool add</command>.
Na przykład:<indexterm xml:id="indexterm-130">
<primary>dodawanie</primary>
<secondary>orządzeń do puli nośników danych ZFS (<command>zpool add</command>)
</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-131">
<primary>
<command>zpool add</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-132">
<primary>pule nośników danych ZFS</primary>
<secondary>dodawanie urządzeń (<command>zpool add</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool add zeepool mirror c2t1d0 c2t2d0</userinput>
</screen>
<para>Format urządzeń wirtualnych jest taki sam jak w przypadku komendy 
<command>zpool create</command> i dotyczą go te same zasady. Wykonywane są 
testy, czy urządzenia są używane i nie można zmienić poziomu replikacji 
bez użycia opcji <option>f</option>. Komenda umożliwia skorzystanie z opcji 
<option>n</option> umożliwiającej suchy przebieg. Na przykład:</para>
<screen># <userinput>zpool add -n zeepool mirror c3t1d0 c3t2d0</userinput>
would update 'zeepool' to the following configuration:
      zeepool
        mirror
            c1t0d0
            c1t1d0
        mirror
            c2t1d0
            c2t2d0
        mirror
            c3t1d0
            c3t2d0</screen>
<para>Komenda ta dodałaby dwa mirrorowane urządzenia <literal>c3t1d0</literal> 
i <literal>c3t2d0</literal> do istniejącej konfiguracji <filename>zeepool
</filename>.</para>
<para>Więcej informacji o [FIXME] walidowaniu [/FIXME] urządzeń wirtualnych 
w <olink targetdoc="" remap="internal" targetptr="gazht">Wykrywaniu 
używanych urządzeń</olink>.</para>
</sect2>
<sect2 xml:id="gcfhe">
<title>Dołączanie i odłączanie urządzeń z puli nośników danych</title>
<para>Dodatkowo, oprócz komendy <command>zpool add</command>, istnieje komenda 
<command>zpool attach</command>, pozwalająca na dołączenie urządzenia do 
istniejącego mirrorowanego bądź niemirrorwanego urządzenia. Na przykład:
<indexterm xml:id="indexterm-133">
<primary>dołączanie</primary>
<secondary>urządzeń do puli nosników danych ZFS (<command>zpool attach
</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-134">
<primary>
<command>zpool attach</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-135">
<primary>pule nośników danych ZFS</primary>
<secondary>dołączanie urządzeń (<command>zpool attach</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool attach zeepool c1t1d0 c2t1d0</userinput>
</screen>
<para>Jeśli istniejące urządzenie jest podwójnym mirrorem, przyłączenie nowego 
urządzenia spowoduje przekształcenie w potrójny mirror i tak dalej. W każdym 
razie nowe urządzenie natychmiast zaczyna resilverować.</para>
<para>W tym przykładzie <literal>zeepool</literal> to podwójny istniejący mirror
 przekształcony w potrójny mirror przez dołączenie urządzenia <literal>c2t1d0
</literal> do istniejącego urządzenia <literal>c1t1d0</literal>.</para>
<para>Do odłączenia urządzenia można użyć komendy <command>zpool detach
</command>. Na przykład:</para>
<screen># <userinput>zpool detach zeepool c2t1d0</userinput>
</screen>
<para>ZFS nie wykona tej operacji, jeśli nie istnieje żadna prawidłowa kopia 
danych. Na przykład:<indexterm xml:id="indexterm-136">
<primary>odłączania</primary>
<secondary>urządzeń z puli nośników danych ZFS (<command>zpool detach</command>)
</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-137">
<primary>
<command>zpool detach</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-138">
<primary>pule nośników danych ZFS</primary>
<secondary>odłączanie urządzeń (<command>zpool detach</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool detach newpool c1t2d0</userinput>
cannot detach c1t2d0: only applicable to mirror and replacing vdevs</screen>
</sect2>
<sect2 xml:id="gazgm">
<title>Włączanie i wyłączanie urządzeń w puli nośników danych</title>
<para>ZFS umożliwia włączanie i wyłączanie pojedynczych urządzeń. Jeśli sprzęt 
zachowuje się niestabilnie lub niewłaściwie, ZFS nadal zapisuje i odczytuje zeń 
dane, zakładając, że usterka jest chwilowa. Jeśli tak nie jest, można nakazać 
ZFS ignorować urządzenie przez wyłączenie go. ZFS nie wysyła żadnych zapytań 
do wyłączonego urządzenia.<indexterm xml:id="indexterm-139">
<primary>włączanie i wyłączanie urządzeń</primary>
<secondary>pula nośników danych ZFS</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-140">
<primary>pula nośników danych ZFS</primary>
<secondary>włączanie i wyłączanie urządzeń</secondary>
<tertiary>opis</tertiary>
</indexterm>
</para>
<note>
<para>Nie trzeba wyłączać urządzeń, aby je wymienić.</para>
</note>
<para>Jeśli zachodzi konieczność tymczasowego odłączenia nośników, można 
użyć komendy <command>offline</command>. Na przykład, jeśli zachodzi konieczność
 odłączenia macierzy od układu przełączników Fibre Channel i podłączenia jej 
do innego układu przełączników, można wyłączyć LUN-y z macierzy używanej 
w pulach ZFS. Podłączoną i działającą już na nowych przełącznikach macież można 
aktywować włączając te same LUN-y. Dane, które pojawiły się w puli w czasie, 
kiedy LUN-y były wyłączone, automatycznie zaczną się resilverować, kiedy tylko 
LUN-y zostaną włączone.</para>
<para>Scenariusz taki jest możliwy przy założeniu, że omawiane systemy widzą 
nośniki danych po podłączeniu ich do nowych przełączników, możliwe że przez 
inne niż poprzednio kontrolery, oraz że pule są skonfigurowane jako RAID-Z 
lub mirror.</para>
<sect3 xml:id="gazfy">
<title>Wyłączanie urządzenia</title>
<para>Urządzenie można wyłączyć używając komendy <command>zpool offline
</command>.
Urządzenie można wskazać przez ścieżkę lub skrót, jeśli jest dyskiem. Na 
przykład:</para>
<screen># <userinput>zpool offline tank c1t0d0</userinput>
bringing device c1t0d0 offline</screen>
<para>Niemożliwe jest takie wyłączanie urządzeń, aby uszkodzić pulę. Nie można 
na przykład wyłączyć dwóch urządzeń w RAID-Z ani wyłączyć wirtualnego urządzenia
 najwyższego poziomu. <indexterm xml:id="indexterm-141">
<primary>wyłączanie urządzenia (<command>zpool offline</command>)</primary>
<secondary>pula nośników danych ZFS</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-142">
<primary>
<command>zpool offline</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-143">
<primary>pule nośników danych ZFS</primary>
<secondary>wyłączanie urządzenia (<command>zpool offline</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool offline tank c1t0d0</userinput>
cannot offline c1t0d0: no valid replicas</screen>
<note>

<para>Nie można obecnie wymienić wyłączonego urządzenia.</para>
</note>
<para>Wyłączone urządzenia raportowane są w statusie puli jako <literal>OFFLINE</literal>. Więcej informacji o sprawdzaniu statusu puli w <olink targetdoc="" remap="internal" targetptr="gaynp">Sprawdzanie statusu puli nośników danych ZFS</olink>.</para>
<para>Domyślnie wyłączenie jest trwałe. Urządzenie pozostaje wyłączone po restarcie systemu.</para>
<para>Tymczasowego wyłączenia można dokonać przy użyciu <command>zpool offline</command> <option>
t</option>. Na przykład:</para>
<screen># <userinput>zpool offline -t tank c1t0d0</userinput>
 bringing device 'c1t0d0' offline</screen>
<para>Po restarcie systemu urządzenie to zostanie automatycznie przywrócone do stanu online <literal>ONLINE</literal>.</para>
<para>Więcej informacji o zdrowiu urządzeń w <olink targetdoc="" remap="internal" targetptr="gamno">Zdrowiu pul nośników danych ZFS</olink>.</para>
</sect3>
<sect3 xml:id="gazgk">
<title>Włączanie urządzenia</title>
<para>Po wyłączeniu urządzenia można je włączyć komendą <command>zpool online</command>:</para>
<screen># <userinput>zpool online tank c1t0d0</userinput>
bringing device c1t0d0 online</screen>
<para>Po włączeniu urządzenia, wszelkie dane, które pojawiły się w puli są synchronizowane z nowym urządzeniem. Nie można używac wyłączania do wymiany dysków. Próba wymiany wyłączonego urządzenia spowoduje oflagowanie go jako uszkodzone przy włączaniu.<indexterm xml:id="indexterm-144">
<primary>włączanie urządzenia</primary>
<secondary>pula nośników danych ZFS (<command>zpool online</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-145">
<primary>
<command>zpool online</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-146">
<primary>pule nośników danych ZFS (<command>zpool online</command>)</primary>
<secondary>włączanie urządzenia</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<para>Przy próbie włączenia uszkodzonego urządzenia, rzy użyciu komendy <command>fmd</command> pojawi się komunikat podobny do poniższego:</para>
<screen># <userinput>zpool online tank c1t0d0</userinput>
Bringing device c1t0d0 online
# 
SUNW-MSG-ID: ZFS-8000-D3, TYPE: Fault, VER: 1, SEVERITY: Major
EVENT-TIME: Fri Mar 17 14:38:47 MST 2006
PLATFORM: SUNW,Ultra-60, CSN: -, HOSTNAME: neo
SOURCE: zfs-diagnosis, REV: 1.0
EVENT-ID: 043bb0dd-f0a5-4b8f-a52d-8809e2ce2e0a
DESC: A ZFS device failed.  Refer to http://sun.com/msg/ZFS-8000-D3 for more information.
AUTO-RESPONSE: No automated response will occur.
IMPACT: Fault tolerance of the pool may be compromised.
REC-ACTION: Run 'zpool status -x' and replace the bad device.</screen>
<para>Więcej informacji o wymianie uszkodzonych urządzeń w <olink targetdoc="" remap="internal" targetptr="gbbvb">Naprawianiu brakujących urządzeń</olink>.</para>
</sect3>
</sect2>
<sect2 xml:id="gazge">
<title>Czyszczenie pul nośników danych</title>
<para>Jeśli urządzenie zostało wyłączone z powodu uszkodzenia, które powodowało wyświetlanie się błędów w wyniku komendy <command>zpool status</command>, można wyczyścić licznik błedów komendą <command>zpool clear</command>.<indexterm xml:id="indexterm-147">
<primary>czyszczenie</primary>
<secondary>urządzenie w puli nośników danych ZFS (<command>zpool clear</command>)</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-148">
<primary>
<command>zpool clear</command>
</primary>
<secondary>opis</secondary>
</indexterm>
</para>
<para>Komenda ta wydana bez argumentów czyści wszystkie błędy wszystkich urządzeń w puli. Na przykład:</para>
<screen># <userinput>zpool clear tank</userinput>
</screen>
<para>Jeśli jako argumenty podano jedno lub więcej urządzeń, komenda wyczyści błędy powiązane tylko z tymi urządzeniami. Na przykład:<indexterm xml:id="indexterm-149">
<primary>czyszczenie urządzeń</primary>
<secondary>puna nośników danych ZFS</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-150">
<primary>
<command>zpool clear</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-151">
<primary>pule nośników danych ZFS</primary>
<secondary>czyszczenie urządzenia</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool clear tank c1t0d0</userinput>
</screen>
<para>Więcej informacji o czyszczeniu błędów <command>zpool</command> w <olink targetdoc="" remap="internal" targetptr="gbbzv">Czyszczeniu tymczasowych błędów</olink>.</para>
</sect2>
<sect2 xml:id="gazgd">
<title>Wymiana urządzeń w puli nośników danych</title>
<para>Urządzenie w puli mozna wymienić komendą <command>zpool
replace</command>.<indexterm xml:id="indexterm-152">
<primary>wymiana</primary>
<secondary>urządzenia (<command>zpool replace</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-153">
<primary>
<command>zpool replace</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-154">
<primary>pule nośników danych ZFS</primary>
<secondary>wymiana urządzenia (<command>zpool replace</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool replace tank c1t1d0 c1t2d0</userinput>
</screen>
<para>W tym przykładzie poprzednie urządzenie, <literal>c1t1d0</literal>, zostało zastąpione urządzeniem <literal>c1t2d0</literal>.</para>
<para>Pojemność urządzenia dołączanego do puli musi być nie mniejsza niż najmniejsza ze wszystkich pojemności urządzeń w konfiguracji mirror lub RAID-Z. Jeśli dołączone urządzenie jest większe, pojemność puli w konfiguracji innej niż RAID-Z i mirror zostanie powiększona po zakończeniu wymiany urządzenia.</para>
<para>Więcej informacji o wymianie urządzeń w <olink targetdoc="" remap="internal" targetptr="gbbvb">Naprawianiu brakującego urządzenia</olink> i <olink targetdoc="" remap="internal" targetptr="gbbvf">Naprawianiu uszkodzonego urządzenia</olink>.</para>
</sect2>
</sect1>
<sect1 xml:id="gaynp">
<title>Sprawdzanie stanu puli nośników danych ZFS</title>
<para>Komenda <command>zpool list</command> pozwala na sprawdzanie stanu puli na wiele sposobów. Dostępne w ten sposób informacje należą do trzech kategorii: podstawowe użycie, statystyki we/wy i zdrowie urządzeń. Wszystkie trzy są omawiane w poniższym podrozdziale.<indexterm xml:id="indexterm-155">
<primary>lista</primary>
<secondary>pule nośników danych ZFS</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-156">
<primary>
<command>zpool list</command>
</primary>
<secondary>opis</secondary>
</indexterm>
</para>
<sect2 xml:id="gamml">
<title>Podstawowe informacje o pulach nośników danych ZFS</title>
<para>Podstawowe informacje o pulach mozna wyświetlić za pomocą komendy <command>zpool list</command>.</para>
<sect3 xml:id="gazij">
<title>Informacje o wszystkich dostępnych pulach</title>
<para>Jeśli nie podano argumentów, komenda wyświetli wszystkie pola o wszystkich pulach w systemie. Na przykład:</para>
<screen># <userinput>zpool list</userinput>
NAME                    SIZE    USED   AVAIL    CAP  HEALTH     ALTROOT
tank                   80.0G   22.3G   47.7G    28%  ONLINE     -
dozer                   1.2T    384G    816G    32%  ONLINE     -</screen>
<para>Powyższy wydruk pokazuje następujące informacje:</para>
<variablelist>
<varlistentry>
<term>
<literal>NAME</literal>
</term>
<listitem>
<para>Nazwa puli.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>SIZE</literal>
</term>
<listitem>
<para>Całkowita pojemność puli, równa sumie pojemności wysztkich urządzeń wirtualnych najwyższego poziomu.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>USED</literal>
</term>
<listitem>
<para>Miejsce alokowane przez wszystkie datasety i wewnętrzne metadane. Należy zauważyć, że wielkości raportowane w tym polu różnią się od wielkości raportowanych na poziomie systemu plików.</para>
<para>Więcej o ustalaniu dostępnej przestrzeni w systemie plików w <olink targetdoc="" remap="internal" targetptr="gbchp">Obliczaniu miejsca w ZFS</olink>.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>AVAILABLE</literal>
</term>
<listitem>
<para>Wielkość niealokowanej przestrzeni w puli.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>CAPACITY</literal> (<literal>CAP</literal>)</term>
<listitem>
<para>Wielkość używanej przestrzeni, raportowana jako procent całkowitej pojemności.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>HEALTH</literal>
</term>
<listitem>
<para>Obecny poziom zdrowia puli.</para>
<para>Więcej o zdrowiu puli w <olink targetdoc="" remap="internal" targetptr="gamno">Zdrowiu pul nośników danych ZFS</olink>.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>ALTROOT</literal>
</term>
<listitem>
<para>Alternatywny główny katalog puli, jeśli jest.</para>
<para>Więcej informacji o alternatywnych głównych katalogach pul w <olink targetdoc="" remap="internal" targetptr="gbcgl">Alternatywnych głównych katalogach pul ZFS</olink>.</para>
</listitem>
</varlistentry>
</variablelist>
<para>Można również odczytać statystyki konkretnej puli przez podanie jej w argumencie komendy. Na przykład:<indexterm xml:id="indexterm-157">
<primary>lista</primary>
<secondary>pule nośników danych ZFS</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-158">
<primary>
<command>zpool list</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-159">
<primary>pule nośników danych ZFS</primary>
<secondary>lista</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool list tank</userinput>
NAME                    SIZE    USED   AVAIL    CAP  HEALTH     ALTROOT
tank                   80.0G   22.3G   47.7G    28%  ONLINE     -</screen>
</sect3>
<sect3 xml:id="gazil">
<title>Konrektne statystyki puli nośników danych</title>
<para>Konkretne statystyki mozna odczytać używając opcji <option>
o</option>.
This option allows for custom reports or a quick way to list pertinent information.
For example, to list only the name and size of each pool, you use the following
syntax:</para>
<screen># <userinput>zpool list -o name,size</userinput>
NAME                    SIZE
tank                   80.0G
dozer                   1.2T</screen>
<para>The column names correspond to the properties that are listed in <olink targetdoc="" remap="internal" targetptr="gazij">Listing Information About All Storage Pools</olink>.</para>
</sect3>
<sect3 xml:id="gazje">
<title>Scripting ZFS Storage Pool Output</title>
<para>The default output for the <command>zpool list</command> command is
designed for readability, and is not easy to use as part of a shell script.
To aid programmatic uses of the command, the <option>
H</option> option can
be used to suppress the column headings and separate fields by tabs, rather
than by spaces. For example, to request a simple list of all pool names on
the system:<indexterm xml:id="indexterm-160">
<primary>scripting</primary>
<secondary>ZFS storage pool output</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-161">
<primary>
<command>zpool list -Ho name</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-162">
<primary>pule nośników danych ZFS</primary>
<secondary>scripting storage pool output</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool list -Ho name</userinput>
tank
dozer</screen>
<para>Here is another example:</para>
<screen># <userinput>zpool list -H -o name,size</userinput>
tank   80.0G
dozer  1.2T</screen>
</sect3>
</sect2>
<sect2 xml:id="gammt">
<title>ZFS Storage Pool I/O Statistics</title>
<para>To request I/O statistics for a pool or specific virtual devices, use
the <command>zpool iostat</command> command. Similar to the <command>iostat</command> command,
this command can display a static snapshot of all I/O activity so far, as
well as updated statistics for every specified interval. The following statistics
are reported:<indexterm xml:id="indexterm-163">
<primary>displaying</primary>
<secondary>ZFS storage pool I/O statistics</secondary>
<tertiary>opis</tertiary>
</indexterm>
</para>
<variablelist>
<varlistentry>
<term>
<literal>USED CAPACITY</literal>
</term>
<listitem>
<para>The amount of data currently stored in the pool or device.
This figure differs from the amount of space available to actual file systems
by a small amount due to internal implementation details.</para>
<para>For more information about the difference between pool space and dataset
space, see <olink targetdoc="" remap="internal" targetptr="gbchp">ZFS Space Accounting</olink>.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>AVAILABLE CAPACITY</literal>
</term>
<listitem>
<para>The amount of space available in the pool or device. As with
the <literal>used</literal> statistic, this is differs from the amount of
space available to datasets by a small margin.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>READ OPERATIONS</literal>
</term>
<listitem>
<para>The number of read I/O operations sent to the pool or device,
including metadata requests.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>WRITE OPERATIONS</literal>
</term>
<listitem>
<para>The number of write I/O operations sent to the pool or device.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>READ BANDWIDTH</literal>
</term>
<listitem>
<para>The bandwidth of all read operations (including metadata),
expressed as units per second.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>WRITE BANDWIDTH</literal>
</term>
<listitem>
<para>The bandwidth of all write operations, expressed as units
per second.</para>
</listitem>
</varlistentry>
</variablelist>
<sect3 xml:id="gazng">
<title>Listing Pool-Wide Statistics</title>
<para>With no options, the <command>zpool iostat</command> command displays
the accumulated statistics since boot for all pools on the system. For example:<indexterm xml:id="indexterm-164">
<primary>displaying</primary>
<secondary>ZFS storage pool-wide I/O statistics</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-165">
<primary>
<command>zpool iostat</command>
</primary>
<secondary>pool-wide (przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-166">
<primary>pule nośników danych ZFS</primary>
<secondary>pool-wide I/O statistics</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool iostat</userinput>
               capacity     operations    bandwidth
pool         used  avail   read  write   read  write
----------  -----  -----  -----  -----  -----  -----
tank         100G  20.0G   1.2M   102K   1.2M  3.45K
dozer       12.3G  67.7G   132K  15.2K  32.1K  1.20K</screen>
<para>Because these statistics are cumulative since boot, bandwidth might
appear low if the pool is relatively idle. You can request a more accurate
view of current bandwidth usage by specifying an interval. For example:</para>
<screen># <userinput>zpool iostat tank 2</userinput>
               capacity     operations    bandwidth
pool         used  avail   read  write   read  write
----------  -----  -----  -----  -----  -----  -----
tank         100G  20.0G   1.2M   102K   1.2M  3.45K
tank         100G  20.0G    134      0  1.34K      0
tank         100G  20.0G     94    342  1.06K   4.1M</screen>
<para>In this example, the command displays usage statistics only for the
pool <literal>tank</literal> every two seconds until the you type Ctrl-C.
Alternately, you can specify an additional <literal>count</literal> parameter,
which causes the command to terminate after the specified number of iterations.
For example, <command>zpool iostat 2 3</command> would print a summary every
two seconds for three iterations, for a total of six seconds. If there is
a single pool, then the statistics is displayed on consecutive lines. If more
than one pool exists, then an additional dashed line delineates each iteration
to provide visual separation.</para>
</sect3>
<sect3 xml:id="gazne">
<title>Listing Virtual Device Statistics</title>
<para>In addition to pool-wide I/O statistics, the <command>zpool iostat</command> command
can display statistics for specific virtual devices. This command can be used
to identify abnormally slow devices, or simply to observe the distribution
of I/O generated by ZFS. To request the complete virtual device layout as
well as all I/O statistics, use the <command>zpool iostat -v</command> command.
For example:<indexterm xml:id="indexterm-167">
<primary>displaying</primary>
<secondary>ZFS storage pool vdev I/O statistics</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-168">
<primary>
<command>zpool iostat -v</command>
</primary>
<secondary>vdev (przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-169">
<primary>pule nośników danych ZFS</primary>
<secondary>vdev I/O statistics</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool iostat -v</userinput>
               capacity     operations    bandwidth
tank         used  avail   read  write   read  write
----------  -----  -----  -----  -----  -----  -----
mirror      20.4G  59.6G      0     22      0  6.00K
  c1t0d0        -      -      1    295  11.2K   148K
  c1t1d0        -      -      1    299  11.2K   148K
----------  -----  -----  -----  -----  -----  -----
total       24.5K   149M      0     22      0  6.00K</screen>
<para>Note two important things when viewing I/O statistics on a virtual device
basis.</para>
<itemizedlist>
<listitem>
<para>First, space usage is only available for top-level virtual
devices. The way in which space is allocated among mirror and RAID-Z virtual
devices is particular to the implementation and not easily expressed as a
single number. </para>
</listitem>
<listitem>
<para>Second, the numbers might not add up exactly as you would
expect them to. In particular, operations across RAID-Z and mirrored devices
will not be exactly equal. This difference is particularly noticeable immediately
after a pool is created, as a significant amount of I/O is done directly to
the disks as part of pool creation that is not accounted for at the mirror
level. Over time, these numbers should gradually equalize, although broken,
unresponsive, or offlined devices can affect this symmetry as well.</para>
</listitem>
</itemizedlist>
<para>You can use the same set of options (interval and count) when examining
virtual device statistics.</para>
</sect3>
</sect2>
<sect2 xml:id="gamno">
<title>Health Status of ZFS Storage Pools</title>
<para>ZFS provides an integrated method of examining pool and device health.
The health of a pool is determined from the state of all its devices. This
state information is displaying by using the <command>zpool status</command> command.
In addition, potential pool and device failures are reported by <command>fmd</command> and
are displayed on the system console and the <command>/var/adm/messages</command> file.
This section describes how to determine pool and device health. This chapter
does not document how to repair or recover from unhealthy pools. For more
information on troubleshooting and data recovery, see <olink targetdoc="" remap="internal" targetptr="gavwg">Chapter 9, ZFS Troubleshooting and Data Recovery</olink>.<indexterm xml:id="indexterm-170">
<primary>displaying</primary>
<secondary>health status of storage pools</secondary>
<tertiary>description of</tertiary>
</indexterm>
<indexterm xml:id="indexterm-171">
<primary>pule nośników danych ZFS</primary>
<secondary>displaying health status</secondary>
</indexterm>
</para>
<para>Each device can fall into one of the following states:</para>
<variablelist>
<varlistentry>
<term>
<literal>ONLINE</literal>
</term>
<listitem>
<para>The device is in normal working order. While some transient
errors might still occur, the device is otherwise in working order.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>DEGRADED</literal>
</term>
<listitem>
<para>The virtual device has experienced failure but is still able
to function. This state is most common when a mirror or RAID-Z device has
lost one or more constituent devices. The fault tolerance of the pool might
be compromised, as a subsequent fault in another device might be unrecoverable.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>FAULTED</literal>
</term>
<listitem>
<para>The virtual device is completely inaccessible. This status
typically indicates total failure of the device, such that ZFS is incapable
of sending or receiving data from it. If a top-level virtual device is in
this state, then the pool is completely inaccessible.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>OFFLINE</literal>
</term>
<listitem>
<para>The virtual device has been explicitly taken offline by the
administrator.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>UNAVAILABLE</literal>
</term>
<listitem>
<para>The device or virtual device cannot be opened. In some cases,
pools with <literal>UNAVAILABLE</literal> devices appear in <literal>DEGRADED</literal> mode.
If a top-level virtual device is unavailable, then nothing in the pool can
be accessed.</para>
</listitem>
</varlistentry>
</variablelist>
<para>The health of a pool is determined from the health of all its top-level
virtual devices. If all virtual devices are <literal>ONLINE</literal>, then
the pool is also <literal>ONLINE</literal>. If any one of the virtual devices
is <literal>DEGRADED</literal> or <literal>UNAVAILABLE</literal>, then the
pool is also <literal>DEGRADED</literal>. If a top-level virtual device is <literal>FAULTED</literal> or <literal>OFFLINE</literal>, then the pool is also <literal>FAULTED</literal>. A pool in the faulted state is completely inaccessible. No data
can be recovered until the necessary devices are attached or repaired. A pool
in the degraded state continues to run, but you might not achieve the same
level of data replication or data throughput if the pool were online.</para>
<sect3 xml:id="gazqw">
<title>Basic Storage Pool Health Status</title>
<para>The simplest way to request a quick overview of pool health status is
to use the <command>zpool status</command> command:<indexterm xml:id="indexterm-172">
<primary>displaying</primary>
<secondary>ZFS storage pool health status</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-173">
<primary>
<command>zpool status -x</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-174">
<primary>pule nośników danych ZFS</primary>
<secondary>displaying health status</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool status -x</userinput>
all pools are healthy</screen>
<para>Specific pools can be examined by specifying a pool name to the command.
Any pool that is not in the <literal>ONLINE</literal> state should be investigated
for potential problems, as described in the next section.</para>
</sect3>
<sect3 xml:id="gazqf">
<title>Detailed Health Status</title>
<para>You can request a more detailed health summary by using the <option>
v</option> option.
For example:</para>
<screen># <userinput>zpool status -v tank</userinput>
  pool: tank
 state: DEGRADED
status: One or more devices could not be opened.  Sufficient replicas exist 
        for the pool to continue functioning in a degraded state.
action: Attach the missing device and online it using 'zpool online'.
   see: http://www.sun.com/msg/ZFS-8000-2Q
 scrub: none requested
config:

        NAME                STATE     READ WRITE CKSUM
        tank                DEGRADED     0     0     0
          mirror            DEGRADED     0     0     0
            c1t0d0          FAULTED      0     0     0  cannot open
            c1t1d0          ONLINE       0     0     0
errors: No known data errors</screen>
<para>This output displays a complete description of why the pool is in its
current state, including a readable description of the problem and a link
to a knowledge article for more information. Each knowledge article provides
up-to-date information on the best way to recover from your current problem.
Using the detailed configuration information, you should be able to determine
which device is damaged and how to repair the pool.<indexterm xml:id="indexterm-175">
<primary>displaying</primary>
<secondary>detailed ZFS storage pool health status</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-176">
<primary>
<command>zpool status -v</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-177">
<primary>pule nośników danych ZFS</primary>
<secondary>displaying detailed health status</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<para>In the above example, the faulted device should be replaced. After the
device is replaced, use the <command>zpool online</command> command to bring
the device back online. For example:</para>
<screen># <userinput>zpool online tank c1t0d0</userinput>
Bringing device c1t0d0 online
# <userinput>zpool status -x</userinput>
all pools are healthy</screen>
<para>If a pool has an offlined device, the command output identifies the
problem pool. For example:</para>
<screen># <userinput>zpool status -x</userinput>
  pool: tank
 state: DEGRADED
status: One or more devices could not be opened.  Sufficient replicas exist for
        the pool to continue functioning in a degraded state.
action: Attach the missing device and online it using 'zpool online'.
   see: http://www.sun.com/msg/ZFS-8000-D3
 scrub: resilver completed with 0 errors on Fri Mar 17 14:38:47 2006
config:

        NAME         STATE     READ WRITE CKSUM
        tank         DEGRADED     0     0     0
          mirror     DEGRADED     0     0     0
            c1t0d0   UNAVAIL      0     0     0  cannot open
            c1t1d0   ONLINE       0     0     0</screen>
<para>The <literal>READ</literal> and <literal>WRITE</literal> columns provides
a count of I/O errors seen on the device, while the <literal>CKSUM</literal> column
provides a count of uncorrectable checksum errors that occurred on the device.
Both of these error counts likely indicate potential device failure, and some
corrective action is needed. If non-zero errors are reported for a top-level
virtual device, portions of your data might have become inaccessible. The
errors count identifies any known data errors.</para>
<para>In the example output above, the offlined device is not causing data
errors.</para>
<para>For more information about diagnosing and repairing faulted pools and
data, see <olink targetdoc="" remap="internal" targetptr="gavwg">Chapter 9, ZFS Troubleshooting and Data Recovery</olink>.</para>
</sect3>
</sect2>
</sect1>
<sect1 xml:id="gbchy">
<title>Migrating ZFS Storage Pools</title>
<para>Occasionally, you might need to move a storage pool between machines.
To do so, the storage devices must be disconnected from the original machine
and reconnected to the destination machine. This task can be accomplished
by physically recabling the devices, or by using multiported devices such
as the devices on a SAN. ZFS enables you to export the pool from one machine
and import it on the destination machine, even if the machines are of different
endianness. For information about replicating or migrating file systems between
different storage pools, which might reside on different machines, see <olink targetdoc="" remap="internal" targetptr="gbchx">Saving and Restoring ZFS Data</olink>.<indexterm xml:id="indexterm-178">
<primary>migrating ZFS storage pools</primary>
<secondary>opis</secondary>
</indexterm>
<indexterm xml:id="indexterm-179">
<primary>pule nośników danych ZFS</primary>
<secondary>migrating</secondary>
<tertiary>opis</tertiary>
</indexterm>
</para>
<sect2 xml:id="gazre">
<title>Preparing for ZFS Storage Pool Migration</title>
<para>Storage pools should be explicitly exported to indicate that they are
ready to be migrated. This operation flushes any unwritten data to disk, writes
data to the disk indicating that the export was done, and removes all knowledge
of the pool from the system.</para>
<para>If you do not explicitly export the pool, but instead remove the disks
manually, you can still import the resulting pool on another system. However,
you might lose the last few seconds of data transactions, and the pool will
appear faulted on the original machine because the devices are no longer present.
By default, the destination machine refuses to import a pool that has not
been explicitly exported. This condition is necessary to prevent accidentally
importing an active pool that consists of network attached storage that is
still in use on another system.</para>
</sect2>
<sect2 xml:id="gazqr">
<title>Exporting a ZFS Storage Pool</title>
<para>To export a pool, use the <command>zpool export</command> command. For
example:</para>
<screen># <userinput>zpool export tank</userinput>
</screen>
<para>Once this command is executed, the pool <literal>tank</literal> is no
longer visible on the system. The command attempts to unmount any mounted
file systems within the pool before continuing. If any of the file systems
fail to unmount, you can forcefully unmount them by using the <option>
f</option> option.
For example:</para>
<screen># <userinput>zpool export tank</userinput>
cannot unmount '/export/home/eschrock': Device busy
# <userinput>zpool export -f tank</userinput>
</screen>
<para>If devices are unavailable at the time of export, the disks cannot be
specified as cleanly exported. If one of these devices is later attached to
a system without any of the working devices, it appears as “potentially
active.” If emulated volumes are in use in the pool, the pool cannot
be exported, even with the <option>
f</option> option. To export a pool with
an emulated volume, first make sure that all consumers of the volume are no
longer active.<indexterm xml:id="indexterm-180">
<primary>exporting</primary>
<secondary>ZFS storage pool</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-181">
<primary>
<command>zpool export</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-182">
<primary>pule nośników danych ZFS</primary>
<secondary>exporting</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<para>For more information about emulated volumes, see <olink targetdoc="" remap="internal" targetptr="gaypf">Emulated Volumes</olink>.</para>
</sect2>
<sect2 xml:id="gazru">
<title>Determining Available Storage Pools to Import</title>
<para>Once the pool has been removed from the system (either through export
or by forcefully removing the devices), attach the devices to the target system.
Although ZFS can handle some situations in which only a portion of the devices
is available, all devices within the pool must be moved between the systems.
The devices do not necessarily have to be attached under the same device name.
ZFS detects any moved or renamed devices, and adjusts the configuration appropriately.
To discover available pools, run the <command>zpool import</command> command
with no options. For example:</para>
<screen># <userinput>zpool import</userinput>
  pool: tank
    id: 3778921145927357706
 state: ONLINE
action: The pool can be imported using its name or numeric identifier.
config:

        tank        ONLINE
          mirror    ONLINE
            c1t0d0  ONLINE
            c1t1d0  ONLINE</screen>
<para>In this example, the pool <literal>tank</literal> is available to be
imported on the target system. Each pool is identified by a name as well as
a unique numeric identifier. If multiple pools available to import have the
same name, you can use the numeric identifier to distinguish between them.<indexterm xml:id="indexterm-183">
<primary>identifying</primary>
<secondary>ZFS storage pool for import (<command>zpool import -a</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-184">
<primary>
<command>zpool import -a</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-185">
<primary>pule nośników danych ZFS</primary>
<secondary>identifying for import (<command>zpool import -a</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<para>Similar to the <command>zpool status</command> command, the <command>zpool
import</command> command refers to a knowledge article available on the web
with the most up-to-date information regarding repair procedures for this
problem. In this case, the user can force the pool to be imported. However,
importing a pool that is currently in use by another system over a storage
network can result in data corruption and panics as both systems attempt to
write to the same storage. If some devices in the pool are not available but
enough redundancy is available to have a usable pool, the pool appears in
the <literal>DEGRADED</literal> state. For example:</para>
<screen># <userinput>zpool import</userinput>
  pool: tank
    id: 3778921145927357706
 state: DEGRADED
status: One or more devices are missing from the system.
action: The pool can be imported despite missing or damaged devices.  The
        fault tolerance of the pool may be compromised if imported.
   see: http://www.sun.com/msg/ZFS-8000-2Q
config:

        tank         DEGRADED
          mirror     DEGRADED
            c1t0d0   UNAVAIL   cannot open
            c1t1d0   ONLINE</screen>
<para>In this example, the first disk is damaged or missing, though you can
still import the pool because the mirrored data is still accessible. If too
many faulted or missing devices are present, the pool cannot be imported.
For example:</para>
<screen># <userinput>zpool import</userinput>
  pool: dozer
    id: 12090808386336829175
 state: FAULTED
action: The pool cannot be imported. Attach the missing
        devices and try again.
   see: http://www.sun.com/msg/ZFS-8000-6X
config:
        raidz               FAULTED
          c1t0d0    ONLINE
          c1t1d0    FAULTED
          c1t2d0    ONLINE
          c1t3d0    FAULTED</screen>
<para>In this example, two disks are missing from a RAID-Z virtual device,
which means that sufficient replicated data is not available to reconstruct
the pool. In some cases, not enough devices are present to determine the complete
configuration. In this case, ZFS doesn't know what other devices were part
of the pool, though ZFS does report as much information as possible about
the situation. For example:</para>
<screen># <userinput>zpool import</userinput>
pool: dozer
    id: 12090808386336829175
 state: FAULTED
status: One or more devices are missing from the system.
action: The pool cannot be imported. Attach the missing
        devices and try again.
   see: http://www.sun.com/msg/ZFS-8000-6X
config:
        dozer          FAULTED   missing device
          raidz       ONLINE
            c1t0d0    ONLINE
            c1t1d0    ONLINE
            c1t2d0    ONLINE
            c1t3d0    ONLINE
        Additional devices are known to be part of this pool, though their
        exact configuration cannot be determined.</screen>
</sect2>
<sect2 xml:id="gaztk">
<title>Finding ZFS Storage Pools From Alternate Directories</title>
<para>By default, the <command>zpool import</command> command only searches
devices within the <filename>/dev/dsk</filename> directory. If devices exist
in another directory, or you are using pools backed by files, you must use
the <option>
d</option> option to search different directories. For example:</para>
<screen># <userinput>zpool create dozer /file/a /file/b</userinput>
# <userinput>zpool export dozer</userinput>
# <userinput>zpool import</userinput>
no pools available
# <userinput>zpool import -d /file</userinput>
  pool: dozer
    id: 672153753596386982
 state: ONLINE
action: The pool can be imported using its name or numeric identifier.
config:

        dozer       ONLINE
          /file/a   ONLINE
          /file/b   ONLINE
# <userinput>zpool import -d /file dozer</userinput>
</screen>
<para>If devices exist in multiple directories, you can specify multiple <option>
d</option> options.<indexterm xml:id="indexterm-186">
<primary>importing</primary>
<secondary>ZFS storage pool from alternate directories (<command>zpool import -d</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-187">
<primary>
<command>zpool import -d</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-188">
<primary>pule nośników danych ZFS</primary>
<secondary>importing from alternate directories (<command>zpool import -d</command>)</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
</sect2>
<sect2 xml:id="gazuf">
<title>Importing ZFS Storage Pools</title>
<para>Once a pool has been identified for import, you can import it by specifying
the name of the pool or its numeric identifier as an argument to the <command>zpool
import</command> command. For example:</para>
<screen># <userinput>zpool import tank</userinput>
</screen>
<para>If multiple available pools have the same name, you can specify which
pool to import using the numeric identifier. For example:</para>
<screen># <userinput>zpool import</userinput>
  pool: dozer
    id: 2704475622193776801
 state: ONLINE
action: The pool can be imported using its name or numeric identifier.
config:

        dozer       ONLINE
          c1t9d0    ONLINE

  pool: dozer
    id: 6223921996155991199
 state: ONLINE
action: The pool can be imported using its name or numeric identifier.
config:

        dozer       ONLINE
          c1t8d0    ONLINE
# <userinput>zpool import dozer</userinput>
cannot import 'dozer': more than one matching pool
import by numeric ID instead
# <userinput>zpool import 6223921996155991199</userinput>
</screen>
<para>If the pool name conflicts with an existing pool name, you can import
the pool under a different name. For example:</para>
<screen># <userinput>zpool import dozer zeepool</userinput>
</screen>
<para>This command imports the exported pool <literal>dozer</literal> using
the new name <literal>zeepool</literal>. If the pool was not cleanly exported,
ZFS requires the <option>
f</option> flag to prevent users from accidentally
importing a pool that is still in use on another system. For example:</para>
<screen># <userinput>zpool import dozer</userinput>
cannot import 'dozer': pool may be in use on another system
use '-f' to import anyway
# <userinput>zpool import -f dozer</userinput>
</screen>
<para>Pools can also be imported under an alternate root by using the <option>
R</option> option.
For more information on alternate root pools, see <olink targetdoc="" remap="internal" targetptr="gbcgl">ZFS Alternate Root Pools</olink>.<indexterm xml:id="indexterm-189">
<primary>importing</primary>
<secondary>ZFS storage pool</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-190">
<primary>
<command>zpool import</command> <replaceable>name</replaceable>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-191">
<primary>pule nośników danych ZFS</primary>
<secondary>importing</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
</sect2>
<sect2 xml:id="gcfhw">
<title>Recovering Destroyed ZFS Storage Pools</title>
<para>You can use the <command>zpool import</command> <option>
D</option> command
to recover a storage pool that has been destroyed. For example:</para>
<screen># <userinput>zpool destroy tank</userinput>
# <userinput>zpool import -D</userinput>
pool: tank
    id: 3778921145927357706
 state: ONLINE (DESTROYED)
action: The pool can be imported using its name or numeric identifier.  The
        pool was destroyed, but can be imported using the '-Df' flags.
config:

        tank        ONLINE
          mirror    ONLINE
            c1t0d0  ONLINE
            c1t1d0  ONLINE</screen>
<para>In the above <command>zpool import</command> output, you can identify
this pool as the destroyed pool because of the following state information:</para>
<screen>state: ONLINE (DESTROYED)</screen>
<para>To recover the destroyed pool, issue the <command>zpool import</command> <option>
D</option> command again with the pool to be recovered and the <option>
f</option> option.
For example:</para>
<screen># <userinput>zpool import -Df tank</userinput>
# <userinput>zpool status tank</userinput>
  pool: tank
 state: ONLINE
 scrub: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        tank        ONLINE       0     0     0
          mirror    ONLINE       0     0     0
            c1t0d0  ONLINE       0     0     0
            c1t1d0  ONLINE       0     0     0

errors: No known data errors</screen>
<para>If one of the devices in the destroyed pool is faulted or unavailable,
you might be able to recover the destroyed pool anyway. In this scenario,
import the degraded pool and then attempt to fix the device failure. For example:<indexterm xml:id="indexterm-192">
<primary>recovering</primary>
<secondary>destroyed ZFS storage pool</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-193">
<primary>
<command>zpool import -D</command>
</primary>
<secondary>(przykład)</secondary>
</indexterm>
<indexterm xml:id="indexterm-194">
<primary>pule nośników danych ZFS</primary>
<secondary>recovering a destroyed pool</secondary>
<tertiary>(przykład)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool destroy dozer</userinput>
# <userinput>zpool import -D</userinput>
pool: dozer
    id: 
 state: DEGRADED (DESTROYED)
status: One or more devices are missing from the system.
action: The pool can be imported despite missing or damaged devices.  The
        fault tolerance of the pool may be compromised if imported.  The
        pool was destroyed, but can be imported using the '-Df' flags.
   see: http://www.sun.com/msg/ZFS-8000-2Q
config:

        dozer        DEGRADED
           raidz      ONLINE
            c1t0d0    ONLINE
            c1t1d0    ONLINE
            c1t2d0    UNAVAIL  cannot open
            c1t3d0    ONLINE
# <userinput>zpool import -Df dozer</userinput>
# <userinput>zpool status -x</userinput>
  pool: dozer
 state: DEGRADED
status: One or more devices could not be opened.  Sufficient replicas exist for
        the pool to continue functioning in a degraded state.
action: Attach the missing device and online it using 'zpool online'.
   see: http://www.sun.com/msg/ZFS-8000-D3
 scrub: resilver completed with 0 errors on Fri Mar 17 16:11:35 2006
config:

        NAME                     STATE     READ WRITE CKSUM
        dozer                    DEGRADED     0     0     0
          raidz                  ONLINE       0     0     0
            c1t0d0               ONLINE       0     0     0
            c1t1d0               ONLINE       0     0     0
            c1t2d0               UNAVAIL      0     0     0  cannot open
            c1t3d0               ONLINE       0     0     0

errors: No known data errors
# <userinput>zpool online dozer c1t2d0</userinput>
Bringing device c1t2d0 online
# <userinput>zpool status -x</userinput>
all pools are healthy</screen>
</sect2>
<sect2 xml:id="gcikw">
<title>Upgrading ZFS Storage Pools</title>
<para>In future ZFS releases, it might be necessary to upgrade your pools
to a newer version to take advantage of the features in the newer version.
The <command>zpool upgrade</command> command is available for this process.
In addition, the <command>zpool status</command> command has been modified
to notify you when your pools are running older versions. For example:<indexterm xml:id="indexterm-195">
<primary>upgrading</primary>
<secondary>ZFS storage pool</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-196">
<primary>
<command>zpool upgrade</command>
</primary>
</indexterm>
<indexterm xml:id="indexterm-197">
<primary>pule nośników danych ZFS</primary>
<secondary>upgrading</secondary>
<tertiary>opis</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool status</userinput>
  pool: test
 state: ONLINE
status: The pool is formatted using an older on-disk format.  The pool can
        still be used, but some features are unavailable.
action: Upgrade the pool using 'zpool upgrade'.  Once this is done, the
        pool will no longer be accessible on older software versions.
 scrub: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        test        ONLINE       0     0     0
          c1t27d0   ONLINE       0     0     0

errors: No known data errors</screen>
<para>In this ZFS release, running the <command>zpool upgrade</command> command
to upgrade pools should be unnecessary. Currently, this command displays the
initial ZFS version information.</para>
<screen># <userinput>zpool upgrade</userinput>
This system is currently running ZFS version 1.

All pools are formatted using this version.</screen>
<para>In future ZFS releases, you can use the following syntax to identify
additional information about a particular version and supported releases.</para>
<screen># <userinput>zpool upgrade -v</userinput>
This system is currently running ZFS version 1.

The following versions are supported:

VER  DESCRIPTION
---  --------------------------------------------------------
 1   Initial ZFS version.

For more information on a particular version, including supported releases, see:

http://www.opensolaris.org/os/community/zfs/version/N

Where 'N' is the version number.</screen>
<para>More information about the pool upgrade process will be provided in
future versions of this guide.</para>
</sect2>
</sect1>
</chapter>
