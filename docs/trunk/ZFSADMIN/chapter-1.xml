<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML//EN" "docbook.dtd"[
	<!ENTITY % xinclude SYSTEM "xinclude.mod">
	%xinclude;
]>

<chapter xmlns="http://docbook.org/ns/docbook" version="5.0" xml:id="zfsover-1">



<title>System plików ZFS (Wprowadzenie)</title>
<toc>
<para>Ten rozdział zawiera przegląd możliwości oraz korzyści związanych
z systemem plików ZFS. Wyjaśnia także podstawową terminologię
używaną przez resztę tej książki.</para>
<para>W tym rozdziale znajdują się następujące sekcje:</para>
<itemizedlist>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gbscy">Co nowego w ZFS-ie?</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="zfsover-2">Czym jest ZFS?</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="ftyue">Terminologia</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gbcpt">Konwencje nazewnictwa w ZFS-ie</olink>
</para>
</listitem>
</itemizedlist>
</toc>
<sect1 xml:id="gbscy">
<title>Co nowego w ZFS-ie?</title>
<para>Sekcja ta zestawia nowe cechy, które zostały dodane do systemu plików ZFS już po początkowym
wypuszczeniu go w dystrybucji Solaris Express w grudniu 2005 r.</para>
<itemizedlist>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gcsxk">Używanie ZFS-a do powielania stref innych niż globalna i inne ulepszenia</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gciui">Komendy ZFS-a do backupu i odtwarzania systemu plików mają zmienioną nazwę</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gcitn">Odzyskiwanie zniszczonych puli</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gcfhy">ZFS został zintegrowany z "Fault Manager"</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gcfiw">Nowe polecenie <command>zpool clear</command></olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gcajn">Compact NFSv4 ACL Format</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gcakl">Narzędzie do monitorowania systemów plików (<command>fsstat</command>)</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gbsbp">Zarządzanie ZFS przez interfejs WWW</olink>
</para>
</listitem>
</itemizedlist>
<sect2 xml:id="gcsxk">
<title>Używanie ZFS-a do powielania stref innych niż globalna i inne ulepszenia</title>
<para>
<emphasis role="strong">OpenSolaris, build 39:</emphasis> Jeżeli wzorcowy parametr <literal>zonepath</literal> i docelowy <literal>zonepath</literal> znajdują się na ZFS-ie i
dodatkowo w tej samej puli ZFS-owej, polecenie <command>zoneadm clone</command> automatycznie użyje
funkcjonalności klonowania systemu plików ZFS w celu utworzenia sklonowanej zony.
Oznacza to, że polecenie <command>zoneadm clone</command> robi obraz (snapshot) systemu
plików wskazywanego przez źródłowy parametr <literal>zonepath</literal> i ustawia go jako docelowy.
Obraz ten jest nazywany w notacji <literal>SUNWzoneX</literal>,
gdzie <literal>X</literal> jest unikatowym identyfikatorem używanym do rozróżnienia poszczególnych
obrazów. Parametr <literal>zonepath</literal> docelowej zony jest używany jako nazwa sklonowanego
systemu plików ZFS. Tworzona jest programowa inspekcja poprawności obrazu, aby mógł on zostać poźniej użytym przez system. Zauważ, że nadal posiadasz możliwość skopiowania parametru źródłowego ZFS <literal>zonepath</literal> zamiast klonu ZFS, jeśli zajdzie taka potrzeba.</para>
<para>Aby umożliwić kilkukrotnie klonowanie zon, dodano nowy paramter polecenia <command>zoneadm</command> pozwalający na określenie, iż powinieć zostać użyty istniejący obraz.
System dokonuje inspekcji poprawności obrazu, aby mógł on zostać poźniej użyty przez system. Dodatkowo, proces instalacyjny zony, ma teraz zdolność detekcji kiedy system plików ZFS może być stworzony dla zony, a proces deinstalacyjny potrafi sprawdzić czy system plików ZFS w zonie może zostać usunięty. Kroki te są wykonywane automatycznie przez polecenie <command>zoneadm</command> .</para> 
<para>Nie należy wykorzystywać opcji tworzenie obrazów ZFS w celu klonowania zony.</para>
<para>W celu uzyskanie większej ilości informacji, zobacz <olink targetdoc="819-2450" remap="external">
<citetitle remap="book">System Administration Guide: Solaris Containers-Resource Management
and Solaris Zones</citetitle>
</olink>.</para>
</sect2>
<sect2 xml:id="gciui">
<title>Komendy ZFS-a do backupu i odtwarzania systemu plików mają zmienioną nazwę</title>
<para>
<emphasis role="strong">OpenSolaris, build 38:</emphasis> W tym wydaniu Solarisa
polecenia <command>zfs backup</command> oraz <command>zfs restore</command>, zostały
przemianowane na <command>zfs send</command> i <command>zfs receive</command>, żeby
bardziej precyzyjnie opisywały swoją funkcję. Rolą tych poleceń jest zapis
i odtworzenie strumienia danych ZFS-a.</para>
<para>W celu uzyskania większej ilości informacji o tych poleceniach, zobacz
<olink targetdoc="" remap="internal" targetptr="gbchx">Saving and Restoring ZFS Data</olink>.</para>
</sect2>
<sect2 xml:id="gcitn">
<title>Odzyskiwanie zniszczonych puli</title>
<para>
<emphasis role="strong">OpenSolaris, build 37:</emphasis> To wydanie wprowadziło polecenie <command>zpool import</command> <option>
D</option>, które umożliwia odtworzenie puli,
które zostały wcześniej zniszczone przez <command>zpool destroy</command>.</para>
<para>W celu uzyskanie większej ilości informacji, zobacz <olink targetdoc="" remap="internal" targetptr="gcfhw">Recovering Destroyed ZFS Storage Pools</olink>.</para>
</sect2>
<sect2 xml:id="gcfhy">
<title>ZFS został zintegrowany z "Fault Manager"</title>
<para>
<emphasis role="strong">OpenSolaris, build 36:</emphasis> Wydanie to zawiera zintegrowany silnik diagnostyczny ZFS, który jest zdolny do diagnozowania oraz raportowania awarii puli i urządzeń. Sumy kontrolne, I/O, błedy puli i urządzeń związane z awariami urządzeń i puli są również raportowane.</para>
<para>Silnik diagnostyczny nie prognozuje na podstawie analizy sum kontrolnych oraz błędów I/O, jednakże zdolność prognozowania na podstawie analizy błędu.</para>
<para>W przypadku awarii ZFS, być może zobaczysz komunikat podobny to następującego<command>fmd</command>:</para>
<screen>SUNW-MSG-ID: ZFS-8000-D3, TYPE: Fault, VER: 1, SEVERITY: Major
EVENT-TIME: Fri Mar 10 11:09:06 MST 2006
PLATFORM: SUNW,Ultra-60, CSN: -, HOSTNAME: neo
SOURCE: zfs-diagnosis, REV: 1.0
EVENT-ID: b55ee13b-cd74-4dff-8aff-ad575c372ef8
DESC: A ZFS device failed.  Refer to http://sun.com/msg/ZFS-8000-D3 for more information.
AUTO-RESPONSE: No automated response will occur.
IMPACT: Fault tolerance of the pool may be compromised.
REC-ACTION: Run 'zpool status -x' and replace the bad device.</screen>
<para>Poprzez zapoznanie się z rekomendowanymi krokami, które będą przestawione przez polecenie <command>zpool status</command>, będziesz mógł szybko zidentyfikować i naprawić awarię.</para>
<para>W celu zapoznania się z przykładami rozwiązywanie problemów raportowanych przez ZFS, zobacz <olink targetdoc="" remap="internal" targetptr="gbbvb">Repairing a Missing Device</olink>.</para>
</sect2>
<sect2 xml:id="gcfiw">
<title>Nowe polecenie <command>zpool clear</command></title>

<para>
<emphasis role="strong">OpenSolaris, build 36:</emphasis> Wydanie to zawiera komendę <command>zpool clear</command>, służącą do zerownia licznika błędów związanych z urządzeniem lub pulą. Poprzednio, licznik błędów był zerowany, kiedy urządzenie w puli było włączane ponownie za pomocą polecenia <command>zpool online</command>.
W celu uzyskanie większej ilości informacji, zobacz <olink targetdoc="819-2240" remap="external" targetptr="zpool-1m">
<citerefentry>
<refentrytitle>zpool</refentrytitle>
<manvolnum>
1M
</manvolnum>
</citerefentry>
</olink> and <olink targetdoc="" remap="internal" targetptr="gazge">Clearing Storage Pool Devices</olink>.</para>
</sect2>
<sect2 xml:id="gcajn">
<title>Compact NFSv4 ACL Format</title>
<para>
<emphasis role="strong">OpenSolaris, build 34:</emphasis> W wydniu tym, są dostępne trzy formaty NFSv4 ACL: informacyjny (verbose), pozycyjny i upakowany. Nowe formaty ACL upakowany i pozycyjny są dostępny, aby ustawiać i wyświetlać ACL. Możesz używać polecenia <command>chmod</command>, aby ustawić wszystkie 3 formaty ACL. Możesz używać polecenia <command>ls</command> <option>V</option>, aby wyświetlić upakowany i pozycyjny format ACL i polecenia <command>ls</command> <option> v</option>, aby wyświetlić format informacyjny (verbose) ACL.</para>
<para>W celu uzyskanie większej ilości informacji, zobacz <olink targetdoc="" remap="internal" targetptr="gbchf">Setting and Displaying ACLs on ZFS Files in Compact Format</olink>, <olink targetdoc="819-2239" remap="external" targetptr="chmod-1">
<citerefentry>
<refentrytitle>chmod</refentrytitle>
<manvolnum>
1
</manvolnum>
</citerefentry>
</olink>, and <olink targetdoc="819-2239" remap="external" targetptr="ls-1">
<citerefentry>
<refentrytitle>ls</refentrytitle>
<manvolnum>
1
</manvolnum>
</citerefentry>
</olink>.</para>
</sect2>
<sect2 xml:id="gcakl">
<title>Narzędzie Monitorowania Systemu Plików (File System Monitoring Tool) (<command>fsstat</command>)</title>
<para>
<emphasis role="strong">OpenSolaris, build 34:</emphasis> Nowe narzędzie do monitorowania systemu plików, <command>fsstat</command>, jest dostępne do raportowania operacji na systemie plików. Aktywność może być raportowana według punktu montowania systemu plików, bądź według jego typu. Poniższy przykład przedstawia ogólne statystyki aktywności systemu plików ZFS.</para>
<screen>% <userinput>fsstat zfs</userinput>
 new  name   name  attr  attr lookup rddir  read read  write write
 file remov  chng   get   set    ops   ops   ops bytes   ops bytes
 729K  488K  282K 79.8M  266K   333M 5.33M 24.8M  115G 2.36M 27.8G zfs</screen>
<para>W celu uzyskanie większej ilości informacji, zobacz <citerefentry>
<refentrytitle>fsstat</refentrytitle>
<manvolnum>
1M
</manvolnum>
</citerefentry>.</para>
</sect2>
<sect2 xml:id="gbsbp">
<title>Zarządzanie ZFS przez interfejs WWW</title>

<para>
<emphasis role="strong">OpenSolaris, build 28:</emphasis> Zostało udostępnione narzędzie administracyjne ZFS zbudowane w oparciu o interfejs web. Za pomocą tego narzędzia można wykonywac następujące czynności:</para>
<itemizedlist>
<listitem>
<para>Tworzenie nowej puli nośników danych.</para>
</listitem>
<listitem>
<para>Dodawanie przestrzeni dla danych do istniejącej puli.</para>
</listitem>
<listitem>
<para>Przenoszenie (export) puli nośników danych do innego systemu.</para>
</listitem>
<listitem>
<para>Import uprzednio exportowanych puli nośników danych, celem udostępnienia ich w innym systemie.</para>
</listitem>
<listitem>
<para>Podględ informacji o puli nośników danych.</para>
</listitem>
<listitem>
<para>Tworzenie systemu plików.</para>
</listitem>
<listitem>
<para>Tworzenie woluminu.</para>
</listitem>
<listitem>
<para>Tworzenie obrazu systemu plików lub woluminu.</para>
</listitem>
<listitem>
<para>Przywracanie systemu plików do poprzedniego obrazu.</para>
</listitem>
</itemizedlist>
<para>Możesz uzyskać dostęp do konsoli Administracj ZFS, poprzez bezpieczne połącznie przy pomocy przeglądarki web pod następującym adresem URL:</para>
<screen>https://<replaceable>system-name</replaceable>:6789/zfs</screen>
<para>Jeśli wpisałeś poprawny adres URL i nie jesteś w stanie połączyć sie z konsolą Administracji ZFS, znaczy to iż serwer prawdopodobnie nie jest uruchomiony. Aby go uruchomić, uruchom następujące polecenie:</para>
<screen># /usr/sbin/smcwebserver start</screen>
<para>Jeśli chcesz, aby serwer uruchamiał się automatycznie podczas start systemu, wykonaj następujące polecenie:</para>
<screen># /usr/sbin/smcwebserver enable</screen>
</sect2>
</sect1>
<sect1 xml:id="zfsover-2">
<title>What Is ZFS?</title>
<para>The Solaris ZFS file system is a revolutionary new file system that
fundamentally changes the way file systems are administered, with features
and benefits not found in any other file system available today. ZFS has been
designed to be robust, scalable, and simple to administer.<indexterm xml:id="indexterm-1">
<primary>ZFS file systems</primary>
<secondary>description</secondary>
</indexterm>
</para>
<sect2 xml:id="gaypk">
<title>ZFS Pooled Storage</title>
<para>ZFS uses the concept of <emphasis>storage pools</emphasis> to manage
physical storage. Historically, file systems were constructed on top of a
single physical device. To address multiple devices and provide for data redundancy,
the concept of a <emphasis>volume manager</emphasis> was introduced to provide
the image of a single device so that file systems would not have to be modified
to take advantage of multiple devices. This design added another layer of
complexity and ultimately prevented certain file system advances, because
the file system had no control over the physical placement of data on the
virtualized volumes.<indexterm xml:id="indexterm-2">
<primary>ZFS file systems</primary>
<secondary>pooled storage</secondary>
<tertiary>description</tertiary>
</indexterm>
<indexterm xml:id="indexterm-3">
<primary>pooled storage</primary>
<secondary>description</secondary>
</indexterm>
</para>
<para>ZFS eliminates the volume management altogether. Instead of forcing
you to create virtualized volumes, ZFS aggregates devices into a storage pool.
The storage pool describes the physical characteristics of the storage (device
layout, data redundancy, and so on,) and acts as an arbitrary data store from
which file systems can be created. File systems are no longer constrained
to individual devices, allowing them to share space with all file systems
in the pool. You no longer need to predetermine the size of a file system,
as file systems grow automatically within the space allocated to the storage
pool. When new storage is added, all file systems within the pool can immediately
use the additional space without additional work. In many ways, the storage
pool acts as a virtual memory system. When a memory DIMM is added to a system,
the operating system doesn't force you to invoke some commands to configure
the memory and assign it to individual processes. All processes on the system
automatically use the additional memory.</para>
</sect2>
<sect2 xml:id="gaypi">
<title>Transactional Semantics</title>
<para>ZFS is a transactional file system, which means that the file system
state is always consistent on disk. Traditional file systems overwrite data
in place, which means that if the machine loses power, for example, between
the time a data block is allocated and when it is linked into a directory,
the file system will be left in an inconsistent state. Historically, this
problem was solved through the use of the <command>fsck</command> command.
This command was responsible for going through and verifying file system state,
making an attempt to repair any inconsistencies in the process. This problem
caused great pain to administrators and was never guaranteed to fix all possible
problems. More recently, file systems have introduced the concept of <emphasis>journaling</emphasis>. The journaling process records action in a separate journal,
which can then be replayed safely if a system crash occurs. This process introduces
unnecessary overhead, because the data needs to be written twice, and often
results in a new set of problems, such as when the journal can't be replayed
properly.<indexterm xml:id="indexterm-4">
<primary>ZFS file systems</primary>
<secondary>transactional semantics</secondary>
<tertiary>description</tertiary>
</indexterm>
<indexterm xml:id="indexterm-5">
<primary>transactional semantics</primary>
<secondary>description</secondary>
</indexterm>
</para>
<para>With a transactional file system, data is managed using <emphasis>copy
on write</emphasis> semantics. Data is never overwritten, and any sequence
of operations is either entirely committed or entirely ignored. This mechanism
means that the file system can never be corrupted through accidental loss
of power or a system crash. So, no need for a <command>fsck</command> equivalent
exists. While the most recently written pieces of data might be lost, the
file system itself will always be consistent. In addition, synchronous data
(written using the <varname>O_DSYNC</varname> flag) is always guaranteed to
be written before returning, so it is never lost.</para>
</sect2>
<sect2 xml:id="gaypb">
<title>Checksums and Self-Healing Data</title>
<para>With ZFS, all data and metadata is checksummed using a user-selectable
algorithm. Traditional file systems that do provide checksumming have performed
it on a per-block basis, out of necessity due to the volume management layer
and traditional file system design. The traditional design means that certain
failure modes, such as writing a complete block to an incorrect location,
can result in properly checksummed data that is actually incorrect. ZFS checksums
are stored in a way such that these failure modes are detected and can be
recovered from gracefully. All checksumming and data recovery is done at the
file system layer, and is transparent to applications.<indexterm xml:id="indexterm-6">
<primary>ZFS file systems</primary>
<secondary>checksummed data</secondary>
<tertiary>description</tertiary>
</indexterm>
<indexterm xml:id="indexterm-7">
<primary>checksummed data</primary>
<secondary>description</secondary>
</indexterm>
</para>
<para>In addition, ZFS provides for self-healing data. ZFS supports storage
pools with varying levels of data redundancy, including mirroring and a variation
on RAID-5. When a bad data block is detected, ZFS fetches the correct data
from another replicated copy, and repairs the bad data, replacing it with
the good copy.</para>
</sect2>
<sect2 xml:id="gayou">
<title>Unparalleled Scalability</title>
<para>ZFS has been designed from the ground up to be the most scalable file
system, ever. The file system itself is 128-bit, allowing for 256 quadrillion
zettabytes of storage. All metadata is allocated dynamically, so no need exists
to pre-allocate inodes or otherwise limit the scalability of the file system
when it is first created. All the algorithms have been written with scalability
in mind. Directories can have up to 2<superscript>48</superscript> (256 trillion)
entries, and no limit exists on the number of file systems or number of files
that can be contained within a file system.</para>
</sect2>
<sect2 xml:id="gbcbn">
<title>ZFS Snapshots</title>
<para>A <emphasis>snapshot</emphasis> is a read-only copy of a file system
or volume. Snapshots can be created quickly and easily. Initially, snapshots
consume no additional space within the pool.</para>
<para>As data within the active dataset changes, the snapshot consumes space
by continuing to reference the old data. As a result, the snapshot prevents
the data from being freed back to the pool.</para>
</sect2>
<sect2 xml:id="gayoc">
<title>Simplified Administration</title>
<para>Most importantly, ZFS provides a greatly simplified administration model.
Through the use of hierarchical file system layout, property inheritance,
and automanagement of mount points and NFS share semantics, ZFS makes it easy
to create and manage file systems without needing multiple commands or editing
configuration files. You can easily set quotas or reservations, turn compression
on or off, or manage mount points for numerous file systems with a single
command. Devices can be examined or repaired without having to understand
a separate set of volume manager commands. You can take an unlimited number
of instantaneous snapshots of file systems. You can backup and restore individual
file systems.<indexterm xml:id="indexterm-8">
<primary>ZFS file systems</primary>
<secondary>simplified administration</secondary>
<tertiary>description</tertiary>
</indexterm>
<indexterm xml:id="indexterm-9">
<primary>simplified administration</primary>
<secondary>description</secondary>
</indexterm>
</para>
<para>ZFS manages file systems through a hierarchy that allows for this simplified
management of properties such as quotas, reservations, compression, and mount
points. In this model, file systems become the central point of control. File
systems themselves are very cheap (equivalent to a new directory), so you
are encouraged to create a file system for each user, project, workspace,
and so on. This design allows you to define fine-grained management points.</para>
</sect2>
</sect1>
<sect1 xml:id="ftyue">
<title>ZFS Terminology</title>
<para>This section describes the basic terminology used throughout this book:</para>
<variablelist>
<varlistentry>
<term>checksum</term>
<listitem>
<para>A 256-bit hash of the data in a file system block. The checksum
capability can range from the simple and fast fletcher2 (the default) to cryptographically
strong hashes such as SHA256.<indexterm xml:id="indexterm-10">
<primary>ZFS file systems</primary>
<secondary>checksum</secondary>
<tertiary>definition</tertiary>
</indexterm>
<indexterm xml:id="indexterm-11">
<primary>checksum</primary>
<secondary>definition</secondary>
</indexterm>
<indexterm xml:id="indexterm-12">
<primary>terminology</primary>
<secondary>checksum</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
<varlistentry>
<term>clone</term>
<listitem>
<para>A file system whose initial contents are identical to the
contents of a snapshot.</para>
<para>For information about clones, see <olink targetdoc="" remap="internal" targetptr="gbcxz">ZFS Clones</olink>.<indexterm xml:id="indexterm-13">
<primary>ZFS file systems</primary>
<secondary>clones</secondary>
<tertiary>definition</tertiary>
</indexterm>
<indexterm xml:id="indexterm-14">
<primary>clone</primary>
<secondary>definition</secondary>
</indexterm>
<indexterm xml:id="indexterm-15">
<primary>terminology</primary>
<secondary>clone</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
<varlistentry>
<term>dataset</term>
<listitem>
<para>A generic name for the following ZFS entities: clones, file
systems, snapshots, or volumes.</para>
<para>Each dataset is identified by a unique name in the ZFS namespace. Datasets
are identified using the following format:</para>
<para>
<replaceable>pool</replaceable>/<replaceable>path</replaceable>[<replaceable>@snapshot</replaceable>]</para>
<variablelist>
<varlistentry>
<term>
<replaceable>pool</replaceable>
</term>
<listitem>
<para>Identifies the name of the storage pool that contains the
dataset</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<replaceable>path</replaceable>
</term>
<listitem>
<para>Is a slash-delimited path name for the dataset object</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<replaceable>snapshot</replaceable>
</term>
<listitem>
<para>Is an optional component that identifies a snapshot of a dataset</para>
</listitem>
</varlistentry>
</variablelist>
<para>For more information about datasets, see <olink targetdoc="" remap="internal" targetptr="gavwq">Chapter 5, Managing ZFS File Systems</olink>.<indexterm xml:id="indexterm-16">
<primary>ZFS file systems</primary>
<secondary>dataset</secondary>
<tertiary>definition</tertiary>
</indexterm>
<indexterm xml:id="indexterm-17">
<primary>dataset</primary>
<secondary>definition</secondary>
</indexterm>
<indexterm xml:id="indexterm-18">
<primary>terminology</primary>
<secondary>dataset</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
<varlistentry>
<term>file system</term>
<listitem>
<para>A dataset that contains a standard POSIX file system.</para>
<para>For more information about file systems, see <olink targetdoc="" remap="internal" targetptr="gavwq">Chapter 5, Managing ZFS File Systems</olink>.<indexterm xml:id="indexterm-19">
<primary>ZFS file systems</primary>
<secondary>file system</secondary>
<tertiary>definition</tertiary>
</indexterm>
<indexterm xml:id="indexterm-20">
<primary>file system</primary>
<secondary>definition</secondary>
</indexterm>
<indexterm xml:id="indexterm-21">
<primary>terminology</primary>
<secondary>file system</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
<varlistentry>
<term>mirror</term>
<listitem>
<para>A virtual device that stores identical copies of data on two
or more disks. If any disk in a mirror fails, any other disk in that mirror
can provide the same data.<indexterm xml:id="indexterm-22">
<primary>ZFS storage pools</primary>
<secondary>mirror</secondary>
<tertiary>definition</tertiary>
</indexterm>
<indexterm xml:id="indexterm-23">
<primary>mirror</primary>
<secondary>definition</secondary>
</indexterm>
<indexterm xml:id="indexterm-24">
<primary>terminology</primary>
<secondary>mirror</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
<varlistentry>
<term>pool</term>
<listitem>
<para>A logical group of devices describing the layout and physical
characteristics of the available storage. Space for datasets is allocated
from a pool.</para>
<para>For more information about storage pools, see <olink targetdoc="" remap="internal" targetptr="gavwn">Chapter 4, Managing ZFS Storage Pools</olink>.<indexterm xml:id="indexterm-25">
<primary>ZFS storage pools</primary>
<secondary>pool</secondary>
<tertiary>definition</tertiary>
</indexterm>
<indexterm xml:id="indexterm-26">
<primary>pool</primary>
<secondary>definition</secondary>
</indexterm>
<indexterm xml:id="indexterm-27">
<primary>terminology</primary>
<secondary>pool</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
<varlistentry>
<term>RAID-Z</term>
<listitem>
<para>A virtual device that stores data and parity on multiple disks,
similar to RAID-5. <indexterm xml:id="indexterm-28">
<primary>ZFS storage pools</primary>
<secondary>RAID-Z</secondary>
<tertiary>definition</tertiary>
</indexterm>
<indexterm xml:id="indexterm-29">
<primary>RAID-Z</primary>
<secondary>definition</secondary>
</indexterm>
<indexterm xml:id="indexterm-30">
<primary>terminology</primary>
<secondary>RAID-Z</secondary>
</indexterm>For more information about RAID-Z, see <olink targetdoc="" remap="internal" targetptr="gamtu">RAID-Z Storage Pool Configuration</olink>.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>resilvering</term>
<listitem>
<para>The process of transferring data from one device to another
device is known as <emphasis>resilvering</emphasis>. For example, if a mirror
component is replaced or taken offline, the data from the up-to-date mirror
component is copied to the newly restored mirror component. This process is
referred to as <emphasis>mirror resynchronization</emphasis> in traditional
volume management products.</para>
<para>For more information about ZFS resilvering, see <olink targetdoc="" remap="internal" targetptr="gbcus">Viewing Resilvering Status</olink>.<indexterm xml:id="indexterm-31">
<primary>ZFS storage pools</primary>
<secondary>resilvering</secondary>
<tertiary>definition</tertiary>
</indexterm>
<indexterm xml:id="indexterm-32">
<primary>resilvering</primary>
<secondary>definition</secondary>
</indexterm>
<indexterm xml:id="indexterm-33">
<primary>terminology</primary>
<secondary>resilvering</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
<varlistentry>
<term>snapshot</term>
<listitem>
<para>A read-only image of a file system or volume at a given point
in time.</para>
<para>For more information about snapshots, see <olink targetdoc="" remap="internal" targetptr="gbciq">ZFS Snapshots</olink>.<indexterm xml:id="indexterm-34">
<primary>ZFS file systems</primary>
<secondary>snapshot</secondary>
<tertiary>definition</tertiary>
</indexterm>
<indexterm xml:id="indexterm-35">
<primary>snapshot</primary>
<secondary>definition</secondary>
</indexterm>
<indexterm xml:id="indexterm-36">
<primary>terminology</primary>
<secondary>snapshot</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
<varlistentry>
<term>virtual device</term>
<listitem>
<para>A logical device in a pool, which can be a physical device,
a file, or a collection of devices.</para>
<para>For more information about virtual devices, see <olink targetdoc="" remap="internal" targetptr="gazca">Virtual Devices in a Storage Pool</olink>.<indexterm xml:id="indexterm-37">
<primary>ZFS storage pools</primary>
<secondary>virtual device</secondary>
<tertiary>definition</tertiary>
</indexterm>
<indexterm xml:id="indexterm-38">
<primary>virtual device</primary>
<secondary>definition</secondary>
</indexterm>
<indexterm xml:id="indexterm-39">
<primary>terminology</primary>
<secondary>virtual device</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
<varlistentry>
<term>volume</term>
<listitem>
<para>A dataset used to emulate a physical device in order to support
legacy file systems.</para>
<para>For more information about emulated volumes, see <olink targetdoc="" remap="internal" targetptr="gaypf">Emulated Volumes</olink>.<indexterm xml:id="indexterm-40">
<primary>ZFS file systems</primary>
<secondary>volume</secondary>
<tertiary>definition</tertiary>
</indexterm>
<indexterm xml:id="indexterm-41">
<primary>volume</primary>
<secondary>definition</secondary>
</indexterm>
<indexterm xml:id="indexterm-42">
<primary>terminology</primary>
<secondary>volume</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
</variablelist>
</sect1>
<sect1 xml:id="gbcpt">
<title>ZFS Component Naming Requirements</title>
<para>Each ZFS component must be named according to the following rules:<indexterm xml:id="indexterm-43">
<primary>ZFS file systems</primary>
<secondary>component naming requirements</secondary>
</indexterm>
<indexterm xml:id="indexterm-44">
<primary>components of ZFS</primary>
<secondary>naming requirements</secondary>
</indexterm>
<indexterm xml:id="indexterm-45">
<primary>naming requirements</primary>
<secondary>ZFS components</secondary>
</indexterm>
</para>
<itemizedlist>
<listitem>
<para>Empty components are not allowed.</para>
</listitem>
<listitem>
<para>Each component can only contain alphanumeric characters in
addition to the following four special characters:</para>
<itemizedlist>
<listitem>
<para>Underscore (_)</para>
</listitem>
<listitem>
<para>Hyphen (-)</para>
</listitem>
<listitem>
<para>Colon (:)</para>
</listitem>
<listitem>
<para>Period (.)</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Pool names must begin with a letter, except that the beginning
sequence c[0-9] is not allowed. In addition, pool names that begin with <literal>mirror</literal>, <literal>raidz</literal>, or <literal>spare</literal> are
not allowed as these name are reserved.</para>
</listitem>
<listitem>
<para>Dataset names must begin with an alphanumeric character.</para>
</listitem>
</itemizedlist>
</sect1>
</chapter>
