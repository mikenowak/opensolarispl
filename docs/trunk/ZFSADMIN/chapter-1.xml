<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML//EN" "docbook.dtd"[
	<!ENTITY % xinclude SYSTEM "xinclude.mod">
	%xinclude;
]>

<chapter xmlns="http://docbook.org/ns/docbook" version="5.0" xml:id="zfsover-1">



<title>System plików ZFS (Wprowadzenie)</title>
<toc>
<para>Ten rozdział zawiera przegląd możliwości oraz korzyści związanych z systemem plików ZFS. Wyjaśnia także podstawową terminologię pojawiającą się w następnych rozdziałach.</para>
<para>W tym rozdziale znajdują się następujące podrozdziały:</para>
<itemizedlist>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-1.xml" targetptr="gbscy">Co nowego w ZFS-ie?</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-1.xml" targetptr="zfsover-2">Czym jest ZFS?</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-1.xml" targetptr="ftyue">Terminologia</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-1.xml" targetptr="gbcpt">Nazewnictwo elementów składowych ZFS-a</olink>
</para>
</listitem>
</itemizedlist>
</toc>
<sect1 xml:id="gbscy">
<title>Co nowego w ZFS-ie?</title>
<para>Nowe cechy dodane do systemu plików ZFS.</para>
<itemizedlist>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-1.xml" targetptr="gevnp">Rekursywna zmiana nazw obrazów ZFS (<command>zfs rename</command> <option>r</option>)</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-1.xml" targetptr="gevpi">Kompresja GZIP dla systemu plików ZFS</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-1.xml" targetptr="gevpg">Zachowywanie wielokrotnych kopii danych użytkownika</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-1.xml" targetptr="gebws">Poprawione wyjście polecenia <command>zpool status</command></olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-1.xml" targetptr="gebwq">Poprawienie funkcjonalności ZFS oraz Solaris iSCSI</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-1.xml" targetptr="gebwp">Rozszerzenia w udostępnianiu systemów plików ZFS</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-1.xml" targetptr="gdswe">Historia poleceń ZFS (<command>zpool history</command>)</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-1.xml" targetptr="gdswf">Rozszerzenie parametrów ZFS</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-1.xml" targetptr="gdsvh">Drukowanie wszystkich informacji o systemie plików ZFS</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-1.xml" targetptr="gdsup">Nowa opcja <command>zfs receive</command> <option>
F</option> </olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-1.xml" targetptr="gdfdt">Rekursywne obrazy ZFS</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-1.xml" targetptr="gcviu">RAID-Z o podwójnej przystości (<literal>raidz2</literal>)</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-1.xml" targetptr="gcvdm">Hot Spares w pulach nośników danych ZFS</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-1.xml" targetptr="gcvgc">Podmiana systemu plików ZFS na jego klon (<command>zfs promote</command>)</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-1.xml" targetptr="gcvit">Uaktualnianie pul nośników danych ZFS (<command>zpool upgrade</command>)</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-1.xml" targetptr="gcsxk">Używanie ZFS do klonowanie nieglobalnych stref oraz inne dodatki</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-1.xml" targetptr="gciui">Zmieniono nazwy komend ZFS Backup i Restore</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-1.xml" targetptr="gcitn">Odzyskiwanie zniszczonych pul nośników danych</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-1.xml" targetptr="gcfhy">ZFS został zintegrowany z zarządzaniem awariami</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-1.xml" targetptr="gcfiw">Nowa komenda <command>zpool clear</command> </olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-1.xml" targetptr="gcajn">Skrócony format ACL-i NFSv4</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-1.xml" targetptr="gcakl">Narzędzie do monitorowania systemu plików (<command>fsstat</command>)</olink>
</para>
</listitem>
<listitem>
<para>
<olink remap="internal" targetdoc="chapter-1.xml" targetptr="gbsbp">Zarządzanie ZFS przez WWW</olink>
</para>
</listitem>
</itemizedlist>
<sect2 xml:id="gevnp">
<title>Rekursywna zmiana nazw obrazów ZFS (<command>zfs rename</command> <option>
r</option>)</title>
<para>
<emphasis role="strong">Solaris Express Developer Edition 5/07:</emphasis> 
Korzystająć z komendy <command>zfs rename</command> <option>
r</option> można rekursywnie zmienić nazwy obrazów ZFS (ang. snapshot). </para>
<para>Przykładowo wykonany zostanie obraz systemów plików ZFS.</para>
<screen># <userinput>zfs snapshot -r users/home@today</userinput>
# <userinput>zfs list</userinput>
NAME                     USED  AVAIL  REFER  MOUNTPOINT
users                    216K  16.5G    20K  /users
users/home                76K  16.5G    22K  /users/home
users/home@today            0      -    22K  -
users/home/markm          18K  16.5G    18K  /users/home/markm
users/home/markm@today      0      -    18K  -
users/home/marks          18K  16.5G    18K  /users/home/marks
users/home/marks@today      0      -    18K  -
users/home/neil           18K  16.5G    18K  /users/home/neil
users/home/neil@today       0      -    18K  -</screen>
<para>Następnie zmienione zostaną nazwy.</para>
<screen># <userinput>zfs rename -r users/home@today @yesterday</userinput>
# <userinput>zfs list</userinput>
NAME                         USED  AVAIL  REFER  MOUNTPOINT
users                        216K  16.5G    20K  /users
users/home                    76K  16.5G    22K  /users/home
users/home@yesterday            0      -    22K  -
users/home/markm              18K  16.5G    18K  /users/home/markm
users/home/markm@yesterday      0      -    18K  -
users/home/marks              18K  16.5G    18K  /users/home/marks
users/home/marks@yesterday      0      -    18K  -
users/home/neil               18K  16.5G    18K  /users/home/neil
users/home/neil@yesterday       0      -    18K  -</screen>
<para>Nazwy można zmienić rekursywnie tylko obrazom. </para>
<para>Więcej informacji o obrazach ZFS w <olink remap="external" targetdoc="chapter-6.xml" targetptr="gbciq">Przegląd obrazów ZFS</olink> oraz ten wpis w blogu:</para>
<para>
<link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:type="url" xlink:href="http://blogs.sun.com/mmusante/entry/rolling_snapshots_made_easy">http://blogs.sun.com/mmusante/entry/rolling_snapshots_made_easy</link>
</para>
</sect2>
<sect2 xml:id="gevpi">
<title>Kompresja GZIP w ZFS</title>
<para>
<emphasis role="strong">Solaris Express Developer Edition 5/07:</emphasis> In
this Solaris release, you can set <literal>gzip</literal> compression on ZFS
file systems in addition to <literal>lzjb</literal> compression. You can specify
compression as <literal>gzip</literal>, the default, or <literal>gzip-</literal>
<replaceable>N</replaceable>, where <replaceable>N</replaceable> equals 1 through 9. For
example:</para>
<screen># zfs create -o compression=gzip users/home/snapshots
# zfs get compression users/home/snapshots
NAME                  PROPERTY     VALUE            SOURCE
users/home/snapshots  compression  gzip             local
# zfs create -o compression=gzip-9 users/home/oldfiles
# zfs get compression users/home/oldfiles
NAME                  PROPERTY     VALUE           SOURCE
users/home/oldfiles   compression  gzip-9          local</screen>
<para>For more information about setting ZFS properties, see <olink remap="external" targetdoc="chapter-5.xml" targetptr="gazsp">Setting ZFS Properties</olink>.</para>
</sect2>
<sect2 xml:id="gevpg">
<title>Storing Multiple Copies of ZFS User Data</title>
<para>
<emphasis role="strong">Solaris Express Developer Edition 5/07:</emphasis> As
a reliability feature, ZFS file system metadata is automatically stored multiple
times across different disks, if possible. This feature is known as <emphasis>ditto
blocks</emphasis>.</para>
<para>In this Solaris release, you can specify that multiple copies of user
data is also stored per file system by using the <command>zfs set copies</command> command.
For example:</para>
<screen># <userinput>zfs set copies=2 users/home</userinput>
# <userinput>zfs get copies users/home</userinput>
NAME        PROPERTY  VALUE       SOURCE
users/home  copies    2           local</screen>
<para>Available values are 1, 2, or 3. The default value is 1. These copies
are in addition to any pool-level redundancy, such as in a mirrored or RAID-Z
configuration.</para>
<para>The benefits of storing multiple copies of ZFS user data are as follows:</para>
<itemizedlist>
<listitem>
<para>Improves data retention by allowing recovery from unrecoverable
block read faults, such as media faults (bit rot) for all ZFS configurations.</para>
</listitem>
<listitem>
<para>Provides data protection even in the case where only a single
disk is available.</para>
</listitem>
<listitem>
<para>Allows you to select data protection policies on a per-file
system basis, beyond the capabilities of the storage pool.</para>
</listitem>
</itemizedlist>
<para>Depending on the allocation of the ditto blocks in the storage pool,
multiple copies might be placed on a single disk. A subsequent full disk failure
might cause all ditto blocks to be unavailable.</para>
<para>You might consider using ditto blocks when you accidentally create a
non-redundant pool and when you need to set data retention policies.</para>
<para>For a detailed description of how setting copies on a system with a
single-disk pool or a multiple-disk pool might impact overall data protection,
see this blog:</para>
<para>
<link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:type="text" xlink:href="http://blogs.sun.com/relling/entry/zfs_copies_and_data_protection">http://blogs.sun.com/relling/entry/zfs_copies_and_data_protection</link>
</para>
<para>For more information about setting ZFS properties, see <olink remap="external" targetdoc="chapter-5.xml" targetptr="gazsp">Setting ZFS Properties</olink>.</para>
</sect2>
<sect2 xml:id="gebws">
<title>Improved <command>zpool status</command> Output</title>
<para>
<emphasis role="strong">Solaris Express 1/07:</emphasis> You
can use the <command>zpool status</command> <option>
v</option> command to
display a list of files with persistent errors. Previously, you had to use
the <command>find</command> <option>
inum</option> command to identify the
filenames from the list of displayed inodes.</para>
<para>For more information about displaying a list of files with persistent
errors, see <olink remap="external" targetdoc="chapter-9.xml" targetptr="gbctx">Repairing a Corrupted File or Directory</olink>.</para>
</sect2>
<sect2 xml:id="gebwq">
<title>ZFS and Solaris iSCSI Improvements</title>
<para>
<emphasis role="strong">Solaris Express, Developer Edition 2/07:</emphasis> In
this Solaris release, you can create a ZFS volume as a Solaris iSCSI target
device by setting the <literal>shareiscsi</literal> property on the ZFS volume.
This method is a convenient way to quickly set up a Solaris iSCSI target.
For example:</para>
<screen># zfs create -V 2g tank/volumes/v2
# zfs set shareiscsi=on tank/volumes/v2
# iscsitadm list target
Target: tank/volumes/v2
    iSCSI Name: iqn.1986-03.com.sun:02:984fe301-c412-ccc1-cc80-cf9a72aa062a
    Connections: 0</screen>
<para>After the iSCSI target is created, set up the iSCSI initiator. For information
about setting up a Solaris iSCSI initiator, see <olink remap="external" targetdoc="819-2723" targetptr="fmvcd">Chapter 14, <citetitle remap="chapter">Configuring Solaris
iSCSI Targets and Initiators (Tasks),</citetitle> in <citetitle remap="book">System
Administration Guide: Devices and File Systems</citetitle>
</olink>.</para>
<para>For more information about managing a ZFS volume as an iSCSI target,
see <olink remap="external" targetdoc="chapter-8.xml" targetptr="gechv">Using a ZFS Volume as a Solaris iSCSI Target</olink>.</para>
</sect2>
<sect2 xml:id="gebwp">
<title>Sharing ZFS File System Enhancements</title>
<para>
<emphasis role="strong">Solaris Express, Developer Edition 2/07:</emphasis> In
this Solaris release, the process of sharing file systems has been improved.
Although modifying system configuration files, such as <filename>/etc/dfs/dfstab</filename>,
is unnecessary for sharing ZFS file systems, you can use the <command>sharemgr</command> command
to manage ZFS share properties. The <command>sharemgr</command> command enables
you to set and manage share properties on share groups. ZFS shares are automatically
designated in the <literal>zfs</literal> share group.</para>
<para>As in previous releases, you can set the ZFS <literal>sharenfs</literal> property
on a ZFS file system to share a ZFS file system. For example:</para>
<screen># zfs set sharenfs=on tank/home</screen>
<para>Or, you can use the new <command>sharemgr</command> <command>add-share</command> subcommand
to share a ZFS file system in the <literal>zfs</literal> share group. For
example:</para>
<screen># sharemgr add-share -s tank/data zfs
# sharemgr show -vp zfs
zfs nfs=()
    zfs/tank/data
          /tank/data
          /tank/data/1
          /tank/data/2
          /tank/data/3</screen>
<para>Then, you can use the <command>sharemgr</command> command to manage
ZFS shares. The following example shows how to use <literal>sharemgr</literal> to
set the <literal>nosuid</literal> property on the shared ZFS file systems.
You must preface ZFS share paths with <literal>/zfs</literal> designation.</para>
<screen># sharemgr set -P nfs -p nosuid=true zfs/tank/data
# sharemgr show -vp zfs
zfs nfs=()
    zfs/tank/data nfs=(nosuid="true")
          /tank/data
          /tank/data/1
          /tank/data/2
          /tank/data/3</screen>
<para>For more information, see <olink remap="external" targetdoc="819-2240" targetptr="sharemgr-1m">
<citerefentry>
<refentrytitle>sharemgr</refentrytitle>
<manvolnum>
1M
</manvolnum>
</citerefentry>
</olink>.</para>
</sect2>
<sect2 xml:id="gdswe">
<title>ZFS Command History (<command>zpool history</command>)</title>
<para>
<emphasis role="strong">Solaris Express 12/06:</emphasis> In
this Solaris release, ZFS automatically logs successful <command>zfs</command> and <command>zpool</command> commands that modify pool state information. For example:</para>
<screen># <userinput>zpool history</userinput>
History for 'newpool':
2007-04-25.11:37:31 zpool create newpool mirror c0t8d0 c0t10d0
2007-04-25.11:37:46 zpool replace newpool c0t10d0 c0t9d0
2007-04-25.11:38:04 zpool attach newpool c0t9d0 c0t11d0
2007-04-25.11:38:09 zfs create newpool/user1
2007-04-25.11:38:15 zfs destroy newpool/user1

History for 'tank':
2007-04-25.11:46:28 zpool create tank mirror c1t0d0 c2t0d0 mirror c3t0d0 c4t0d0</screen>
<para>This features enables you or Sun support personnel to identify the <emphasis>exact</emphasis> set of ZFS commands that was executed to troubleshoot an
error scenario.</para>
<para>You can identify a specific storage pool with the <command>zpool history</command> command.
For example:</para>
<screen># <userinput>zpool history newpool</userinput>
History for 'newpool':
History for 'newpool':
2007-04-25.11:37:31 zpool create newpool mirror c0t8d0 c0t10d0
2007-04-25.11:37:46 zpool replace newpool c0t10d0 c0t9d0
2007-04-25.11:38:04 zpool attach newpool c0t9d0 c0t11d0
2007-04-25.11:38:09 zfs create newpool/user1
2007-04-25.11:38:15 zfs destroy newpool/user1</screen>
<para>The features of the history log are as follows:</para>
<itemizedlist>
<listitem>
<para>The log cannot be disabled.</para>
</listitem>
<listitem>
<para>The log is saved persistently on disk, which means the log
is saved across system reboots.</para>
</listitem>
<listitem>
<para>The log is implemented as a ring buffer. The minimum size
is 128 Kbytes. The maximum size is 32 Mbytes.</para>
</listitem>
<listitem>
<para>For smaller pools, the maximum size is capped at 1% of the
pool size, where <replaceable>size</replaceable> is determined at pool creation
time.</para>
</listitem>
<listitem>
<para>Requires no administration, which means tuning the size of
the log or changing the location of the log is unnecessary.</para>
</listitem>
</itemizedlist>
<para>Currently, the <command>zpool history</command> command does not record
 <replaceable>user-ID</replaceable>, <replaceable>hostname</replaceable>,
or <replaceable>zone-name</replaceable>.</para>
<para>For more information about troubleshooting ZFS problems, see <olink remap="external" targetdoc="chapter-9.xml" targetptr="gbbuw">Identifying Problems in ZFS</olink>.</para>
</sect2>
<sect2 xml:id="gdswf">
<title>ZFS Property Improvements</title>
<sect3 xml:id="gebwr">
<title>ZFS <literal>xattr</literal> Property</title>
<para>
<emphasis role="strong">Solaris Express 1/07:</emphasis> You
can use the <literal>xattr</literal> property to disable or enable extended
attributes for a specific ZFS file system. The default value is on. For a
description of ZFS properties, see <olink remap="external" targetdoc="chapter-5.xml" targetptr="gazss">Introducing ZFS Properties</olink>.</para>
</sect3>
<sect3 xml:id="gdswd">
<title>ZFS <command>canmount</command> Property</title>
<para>
<emphasis role="strong">Solaris Express 10/06:</emphasis> The
new <command>canmount</command> property allows you to specify whether a dataset
can be mounted by using the <command>zfs mount</command> command. For more
information, see <olink remap="external" targetdoc="chapter-5.xml" targetptr="gdrcf">The <command>canmount</command> Property</olink>.</para>
</sect3>
<sect3 xml:id="gdsus">
<title>ZFS User Properties</title>
<para>
<emphasis role="strong">Solaris Express 10/06:</emphasis> In
addition to the standard native properties that can either export internal
statistics or control ZFS file system behavior, ZFS supports user properties.
User properties have no effect on ZFS behavior, but you can use them to annotate
datasets with information that is meaningful in your environment.</para>
<para>For more information, see <olink remap="external" targetdoc="chapter-5.xml" targetptr="gdrcw">ZFS User Properties</olink>.</para>
</sect3>
<sect3 xml:id="gdsvx">
<title>Setting Properties When Creating ZFS File Systems</title>
<para>
<emphasis role="strong">Solaris Express 10/06:</emphasis> In
this Solaris release, you can set properties when you create a file system,
in addition to setting properties after the file system is created.</para>
<para>The following examples illustrate equivalent syntax:</para>
<screen># <userinput>zfs create tank/home</userinput>
# <userinput>zfs set mountpoint=/export/zfs tank/home</userinput>
# <userinput>zfs set sharenfs=on tank/home</userinput>
# <userinput>zfs set compression=on tank/home</userinput>
</screen>
<screen># <userinput>zfs create -o mountpoint=/export/zfs -o sharenfs=on -o compression=on tank/home</userinput>
</screen>
</sect3>
</sect2>
<sect2 xml:id="gdsvh">
<title>Displaying All ZFS File System Information</title>
<para>
<emphasis role="strong">Solaris Express 10/06:</emphasis> In
this Solaris release, you can use various forms of the <command>zfs get</command> command
to display information about all datasets if you do not specify a dataset.
In previous releases, all dataset information was not retrievable with the <command>zfs get</command> command.</para>
<para>For example:</para>
<screen># <userinput>zfs get -s local all</userinput>
tank/home               atime          off                    local
tank/home/bonwick       atime          off                    local
tank/home/marks         quota          50G                    local</screen>
</sect2>
<sect2 xml:id="gdsup">
<title>New <command>zfs receive</command> <option>
F</option> Option</title>
<para>
<emphasis role="strong">Solaris Express 10/06:</emphasis> In
this Solaris release, you can use the new <option>
F</option> option to the <command>zfs receive</command> command to force a rollback of the file system to the
most recent snapshot before doing the receive. Using this option might be
necessary when the file system is modified between the time a rollback occurs
and the receive is initiated.</para>
<para>For more information, see <olink remap="external" targetdoc="chapter-6.xml" targetptr="gbimy">Restoring a ZFS Snapshot</olink>.</para>
</sect2>
<sect2 xml:id="gdfdt">
<title>Recursive ZFS Snapshots</title>
<para>
<emphasis role="strong">Solaris Express 8/06:</emphasis> When
you use the <command>zfs snapshot</command> command to create a file system
snapshot, you can use the <option>
r</option> option to recursively create
snapshots for all descendent file systems. In addition, using the <option>
r</option> option
recursively destroys all descendent snapshots when a snapshot is destroyed.</para>
<para>Recursive ZFS snapshots are created quickly as one atomic operation.
The snapshots are created together (all at once) or not created at all. The
benefit of atomic snapshots operations is that the snapshot data is always
taken at one consistent time, even across descendent file systems.</para>
<para>For more information, see <olink remap="external" targetdoc="chapter-6.xml" targetptr="gbcya">Creating and Destroying ZFS Snapshots</olink>.</para>
</sect2>
<sect2 xml:id="gcviu">
<title>Double Parity RAID-Z (<literal>raidz2</literal>)</title>
<para>
<emphasis role="strong">Solaris Express 7/06:</emphasis> A
redundant RAID-Z configuration can now have either single- or double-parity,
which means that one or two device failures can be sustained respectively,
without any data loss. You can specify the <literal>raidz2</literal> keyword
for a double-parity RAID-Z configuration. Or,  you can specify the <literal>raidz</literal> or <literal>raidz1</literal> keyword for a single-parity RAID-Z
configuration.</para>
<para>For more information, see <olink remap="external" targetdoc="chapter-4.xml" targetptr="gcvjg">Creating RAID-Z Storage Pools</olink> or <olink remap="external" targetdoc="819-2240" targetptr="zpool-1m">
<citerefentry>
<refentrytitle>zpool</refentrytitle>
<manvolnum>
1M
</manvolnum>
</citerefentry>
</olink>.</para>
</sect2>
<sect2 xml:id="gcvdm">
<title>Hot Spares for ZFS Storage Pool Devices</title>
<para>
<emphasis role="strong">Solaris Express 7/06:</emphasis> The
ZFS hot spares feature enables you to identify disks that could be used to
replace a failed or faulted device in one or more storage pools. Designating
a device as a <emphasis>hot spare</emphasis> means that if an active device
in the pool fails, the hot spare automatically replaces the failed device.
Or, you can manually replace a device in a storage pool with a hot spare.</para>
<para>For more information, see <olink remap="external" targetdoc="chapter-4.xml" targetptr="gcvcw">Designating Hot Spares in Your Storage Pool</olink> and <olink remap="external" targetdoc="819-2240" targetptr="zpool-1m">
<citerefentry>
<refentrytitle>zpool</refentrytitle>
<manvolnum>
1M
</manvolnum>
</citerefentry>
</olink>.</para>
</sect2>
<sect2 xml:id="gcvgc">
<title>Replacing a ZFS File System With a ZFS
Clone (<command>zfs promote</command>)</title>
<para>
<emphasis role="strong">Solaris Express 7/06:</emphasis> The <command>zfs promote</command> command enables you to replace an existing ZFS file
system with a clone of that file system. This feature is helpful when you
want to run tests on an alternative version of a file system and then, make
that alternative version of the file system the active file system.</para>
<para>For more information, see <olink remap="external" targetdoc="chapter-6.xml" targetptr="gcvfl">Replacing a ZFS File System With a ZFS Clone</olink> and <olink remap="external" targetdoc="819-2240" targetptr="zfs-1m">
<citerefentry>
<refentrytitle>zfs</refentrytitle>
<manvolnum>
1M
</manvolnum>
</citerefentry>
</olink>.</para>
</sect2>
<sect2 xml:id="gcvit">
<title>Upgrading ZFS Storage Pools (<command>zpool
upgrade</command>)</title>
<para>
<emphasis role="strong">Solaris Express 6/06:</emphasis> You
can upgrade your storage pools to a newer version to take advantage of the
latest features by using the <command>zpool upgrade</command> command. In
addition, the <command>zpool status</command> command has been modified to
notify you when your pools are running older versions.</para>
<para>For more information, see <olink remap="external" targetdoc="chapter-4.xml" targetptr="gcikw">Upgrading ZFS Storage Pools</olink> and <olink remap="external" targetdoc="819-2240" targetptr="zpool-1m">
<citerefentry>
<refentrytitle>zpool</refentrytitle>
<manvolnum>
1M
</manvolnum>
</citerefentry>
</olink>.</para>
<para>If you want to use the ZFS Administration console on a system with a
pool from a previous Solaris release, make sure you upgrade your pools before
using the ZFS Administration console. To see if your pools need to be upgraded,
use the <command>zpool status</command> command. For information about the
ZFS Administration console, see <olink remap="internal" targetdoc="chapter-1.xml" targetptr="gbsbp">ZFS Web-Based Management</olink>.</para>
</sect2>
<sect2 xml:id="gcsxk">
<title>Używanie ZFS-a do powielania stref (ang. zone) innych niż globalna i inne ulepszenia</title>
<para>
<emphasis role="strong">Solaris Express 6/06:</emphasis> Jeżeli wzorcowy parametr <literal>zonepath</literal> i docelowy <literal>zonepath</literal> znajdują się na ZFS-ie i dodatkowo w tej samej puli ZFS-owej, polecenie <command>zoneadm clone</command> automatycznie użyje funkcjonalności klonowania systemu plików ZFS w celu utworzenia sklonowanej strefy. Oznacza to, że polecenie <command>zoneadm clone</command> robi obraz (snapshot) systemu plików wskazywanego przez źródłowy parametr <literal>zonepath</literal> i ustawia go jako docelowy.
Obraz ten jest nazywany w notacji <literal>SUNWzoneX</literal>, gdzie <literal>X</literal> jest unikatowym identyfikatorem używanym do rozróżnienia poszczególnych obrazów. Parametr <literal>zonepath</literal> docelowej strefy jest używany jako nazwa sklonowanego systemu plików ZFS. Tworzona jest programowa inspekcja poprawności obrazu, aby mógł on zostać później użyty przez system. Nadal istnieje możliwość skopiowania parametru źródłowego ZFS <literal>zonepath</literal> zamiast klonu ZFS, jeśli zajdzie taka potrzeba.</para>
<para>Aby umożliwić kilkukrotnie klonowanie stref dodano nowy parametr polecenia <command>zoneadm</command>, pozwalający na określenie, iż powinien zostać użyty istniejący obraz. Wykonywana jest inspekcja poprawności obrazu, aby mógł on zostać później użyty przez system. Dodatkowo proces instalacyjny strefy umie teraz wykrywać kiedy system plików ZFS może być stworstrefy dla strefy, a proces deinstalacyjny potrafi sprawdzić, czy system plików ZFS w strefie może zostać usunięty. Kroki te są wykonywane automatycznie przez polecenie <command>zoneadm</command>.</para> 
<para>Nie należy wykorzystywać opcji tworzenie obrazów ZFS w celu klonowania strefy.</para>
<para>Więcej informacji w <olink remap="external" targetdoc="819-2450">
<citetitle remap="book">System Administration Guide: Solaris Containers-Resource Management
and Solaris Zones</citetitle>
</olink>.</para>
</sect2>
<sect2 xml:id="gciui">
<title>Komendy ZFS-a do tworzenia kopii zapasowych i odtwarzania systemu plików mają zmienioną nazwę</title>
<para>
<emphasis role="strong">Solaris Express 5/06:</emphasis> W tym wydaniu Solarisa polecenia <command>zfs backup</command> oraz <command>zfs restore</command> zostały przemianowane na <command>zfs send</command> i <command>zfs receive</command>, żeby bardziej precyzyjnie opisywały swoją funkcję. Rolą tych poleceń jest zapis i odtworzenie strumienia danych ZFS-a.</para>
<para>Więcej informacji o tych poleceniach w <olink remap="external" targetdoc="chapter-6.xml" targetptr="gbchx">Zapisywanie i odzyskiwanie danych ZFS</olink>.</para>
</sect2>
<sect2 xml:id="gcitn">
<title>Odzyskiwanie zniszczonych pul</title>
<para>
<emphasis role="strong">Solaris Express 5/06:</emphasis> To wydanie wprowadziło polecenie <command>zpool import</command> <option>D</option>, które umożliwia odtworzenie pul zniszczonych wcześniej poleceniem <command>zpool destroy</command>.</para>
<para>Więcej informacji w <olink remap="external" targetdoc="chapter-4.xml" targetptr="gcfhw">Odzyskiwanie zniszczonych danych ZFS</olink>.</para>
</sect2>
<sect2 xml:id="gcfhy">
<title>ZFS został zintegrowany z "Fault Manager"</title>
<para>
<emphasis role="strong">Solaris Express 4/06:</emphasis> Wydanie to zawiera zintegrowany silnik diagnostyczny ZFS, który jest zdolny do diagnozowania oraz raportowania awarii pul i urządzeń. Sumy kontrolne, I/O, błędy puli i urządzeń związane z awariami urządzeń i pul są również raportowane.</para>
<para>Silnik diagnostyczny nie prognozuje na podstawie analizy sum kontrolnych oraz błędów I/O, umie jednak prognozować na podstawie analizy błędu.</para>
<para>W przypadku awarii ZFS może pojawić się błąd podobny do poniższego <command>fmd</command>:</para>
<screen remap="wide">SUNW-MSG-ID: ZFS-8000-D3, TYPE: Fault, VER: 1, SEVERITY: Major
EVENT-TIME: Fri Mar 10 11:09:06 MST 2006
PLATFORM: SUNW,Ultra-60, CSN: -, HOSTNAME: neo
SOURCE: zfs-diagnosis, REV: 1.0
EVENT-ID: b55ee13b-cd74-4dff-8aff-ad575c372ef8
DESC: A ZFS device failed.  Refer to http://sun.com/msg/ZFS-8000-D3 for more information.
AUTO-RESPONSE: No automated response will occur.
IMPACT: Fault tolerance of the pool may be compromised.
REC-ACTION: Run 'zpool status -x' and replace the bad device.</screen>
<para>Poprzez zapoznanie się z rekomendowanymi krokami, które będą przedstawione przez polecenie <command>zpool status</command>, szybko będzie można zidentyfikować i naprawić awarię.</para>
<para>Przykłady rozwiązywania problemów raportowanych przez ZFS w <olink remap="external" targetdoc="chapter-9.xml" targetptr="gbbvb">Naprawa brakującego urządzenia</olink>.</para>
</sect2>
<sect2 xml:id="gcfiw">
<title>Nowe polecenie <command>zpool clear</command></title>
<para>
<emphasis role="strong">Solaris Express 4/06:</emphasis> Wydanie to zawiera komendę <command>zpool clear</command>, służącą do zerownia licznika błędów związanych z urządzeniem lub pulą. Poprzednio licznik błędów był zerowany, kiedy urządzenie w puli było włączane ponownie za pomocą polecenia <command>zpool online</command>.
Więcej informacji w <olink remap="external" targetdoc="819-2240" targetptr="zpool-1m">
<citerefentry>
<refentrytitle>zpool</refentrytitle>
<manvolnum>
1M
</manvolnum>
</citerefentry>
</olink> oraz <olink remap="external" targetdoc="chapter-4.xml" targetptr="gazge">Czyszczenie urządzeń puli nośników danych</olink>.</para>
</sect2>
<sect2 xml:id="gcajn">
<title>Skrócony format ACL-i NFSv4</title>
<para>
<emphasis role="strong">Solaris Express 4/06:</emphasis> W wydaniu tym są dostępne trzy formaty ACLI- NFSv4: informacyjny (verbose), pozycyjny i skrócony. Nowe formaty ACL: skrócony i pozycyjny są dostępne, aby ustawiać i wyświetlać ACL-e. Możesz używać polecenia <command>chmod</command>, aby ustawić wszystkie 3 formaty ACL-i. Możesz używać polecenia <command>ls</command> <option>V</option>, aby wyświetlić skrócony i pozycyjny format ACL-i oraz polecenia <command>ls</command> <option> v</option>, aby wyświetlić format informacyjny (verbose) ACL-i.</para>
<para>Więcej informacji w <olink remap="external" targetdoc="chapter-7.xml" targetptr="gbchf">Ustawianie i drukowanie ACL-i dla plików ZFs-a w formacie skróconym</olink>, <olink remap="external" targetdoc="819-2239" targetptr="chmod-1">
<citerefentry>
<refentrytitle>chmod</refentrytitle>
<manvolnum>
1
</manvolnum>
</citerefentry>
</olink> oraz <olink remap="external" targetdoc="819-2239" targetptr="ls-1">
<citerefentry>
<refentrytitle>ls</refentrytitle>
<manvolnum>
1
</manvolnum>
</citerefentry>
</olink>.</para>
</sect2>
<sect2 xml:id="gcakl">
<title>Narzędzie Monitorowania Systemu Plików (File System Monitoring Tool) (<command>fsstat</command>)</title>
<para>
<emphasis role="strong">Solaris Express 4/06:</emphasis> Nowe narzędzie do monitorowania systemu plików, <command>fsstat</command>, jest dostępne do raportowania operacji na systemie plików. Aktywność może być raportowana według punktu montowania systemu plików bądź według jego typu. Poniższy przykład przedstawia ogólne statystyki aktywności systemu plików ZFS.</para>
<screen>$ <userinput>fsstat zfs</userinput>
 new  name   name  attr  attr lookup rddir  read read  write write
 file remov  chng   get   set    ops   ops   ops bytes   ops bytes
7.82M 5.92M 2.76M 1.02G 3.32M  5.60G 87.0M  363M 1.86T 20.9M  251G zfs</screen>
<para>Więcej informacji w <olink remap="external" targetdoc="819-2240" targetptr="fsstat-1m">
<citerefentry>
<refentrytitle>fsstat</refentrytitle>
<manvolnum>
1M
</manvolnum>
</citerefentry>
</olink>.</para>
</sect2>
<sect2 xml:id="gbsbp">
<title>Zarządzanie ZFS-em przez interfejs WWW</title>
<para>
<emphasis role="strong">Solaris Express 1/06:</emphasis> Zostało udostępnione narzędzie administracyjne ZFS zbudowane w oparciu o interfejs web. Za pomocą tego narzędzia można wykonywać następujące czynności:</para>
<itemizedlist>
<listitem>
<para>Tworzenie nowej puli nośników danych.</para>
</listitem>
<listitem>
<para>Dodawanie przestrzeni dla danych do istniejącej puli.</para>
</listitem>
<listitem>
<para>Przenoszenie (eksport) puli nośników danych do innego systemu.</para>
</listitem>
<listitem>
<para>Import uprzednio eksportowanych pul nośników danych celem udostępnienia ich w innym systemie.</para>
</listitem>
<listitem>
<para>Podgląd informacji o puli nośników danych.</para>
</listitem>
<listitem>
<para>Tworzenie systemu plików.</para>
</listitem>
<listitem>
<para>Tworzenie woluminu.</para>
</listitem>
<listitem>
<para>Tworzenie obrazu systemu plików lub woluminu.</para>
</listitem>
<listitem>
<para>Przywracanie systemu plików do poprzedniego obrazu.</para>
</listitem>
</itemizedlist>
<para>Dostęp do konsoli Administracji ZFS możliwe jest przez bezpieczne połączenie przy pomocy przeglądarki web pod następującym adresem URL:</para>
<screen>https://<replaceable>system-name</replaceable>:6789/zfs</screen>
<para>Jeśli po wpisaniu poprawengo adresu URL nie pojawia się konsola administracyjna, oznacza to najprawdopodobniej, że serwer jest nieuruchomiony. Włącza się go następującym poleceniem:</para>
<screen># /usr/sbin/smcwebserver start</screen>
<para>Automatyczne uruchamianie serwera podczas startu systemu włączane jest następującą komendą:</para>
<screen># /usr/sbin/smcwebserver enable</screen>
<note>

<para>Nie można użyć Konsoli Zarządzania Solarisem (<command>smc</command>)
do zarządzania pulami ZFS lub systemami plików.</para>
</note>
<para>Nie można zarządzać systemami plików ZFS zdalnie przy użyciu konsoli administracji
ZFS-em, ponieważ w ostatnich wydaniach Solaris Express wyłączono część serwisów sieciowych
automatycznie. Należy użyć następującego polecenia w celu włączenia tych serwisów:</para>
<screen># <userinput>netservices open</userinput>
</screen>
</sect2>
</sect1>
<sect1 xml:id="zfsover-2">
<title>Czym jest ZFS?</title>
<para>Solarisowy system plików ZFS jest rewolucyjnym, nowym systemem plików, który zmienia sposób administracji systemem plików oraz którego funkcjonalność oraz korzyści z niej płynące są niespotykane dzisiaj w żadnym innym systemie plików. ZFS został zaprojektowany by być mocnym, skalowalnym oraz prostym w administracji.<indexterm xml:id="indexterm-1">
<primary>System plików ZFS</primary>
<secondary>opis</secondary></indexterm>
</para>
<sect2 xml:id="gaypk">
<title>Pula danych ZFS</title>
<para>ZFS używa koncepcji <emphasis>pul nośników danych</emphasis>, aby zarządzać urządzeniami fizycznymi. Uprzednio systemy plików były konstruowane na pojedynczych, fizycznych urządzeniach. Aby zaadresować wiele urządzeń oraz zapewnić redundancję została przedstawiona koncepcja <emphasis>managera woluminów</emphasis>, prezentującego obraz pojedynczego urządzenia, który nie musiał być modyfikowany, aby korzystać z zalet wykorzystania kilku urządzeń. Taki schemat stworzył dodatkowy poziom i ostatecznie uniemożliwił wykorzystanie pewnych zalet systemu plików, ponieważ system plików nie miał kontroli nad fizyczną lokalizacją danych w wirtualnych woluminach.<indexterm xml:id="indexterm-2">
<primary>System plików ZFS</primary>
<secondary>pula nośników danych</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-3">
<primary>pula nośników danych</primary>
<secondary>opis</secondary>
</indexterm>
</para>
<para>ZFS eliminuje całkowicie managera woluminów. Zamiast zmuszać do tworzenia wirtualnych woluminów, ZFS grupuje urządzenia w pule nośników danych. Pula nośników danych opisuje fizyczną charakterystykę zbioru danych (podział logiczny urządzenia, sposób redundancji danych, itd.) i działa jako magazyn danych, z którego systemy plików mogą być tworzone. System plików nie zawiera już indywidualnych urządzeń, co pozwala im na dzielenie się przestrzenią dyskową ze wszystkimi systemami plików w puli. Nie trzeba już dłużej z góry definiować wielkości systemu plików, rośnie on automatycznie w przestrzeni dyskowej, jaką zadedykowano na potrzeby puli. W momencie gdy zostają dodane nowe urządzenia, wszystkie systemy plików potrafią używać dodatkowej przestrzeni, bez potrzeby ingerencji. W pewien sposób pule nośników danych zachowują się podobnie do systemu pamięć wirtualnej. Kiedy dodawane są pamięci DIMM do systemu, system operacyjny nie wymusza wykonania jakichkolwiek poleceń, aby skonfigurować pamięć, by przypisać ją do indywidualnego procesu. Wszystkie procesy w systemie automatycznie wykorzystują dodatkową pamięć.</para>
</sect2>
<sect2 xml:id="gaypi">
<title>Terminologia transakcyjna</title>
<para>ZFS jest transakcyjnym systemem plików, co znaczy że stan systemu plików na dysku jest zawsze spójny. Tradycyjny system plików nadpisuje dane, co oznacza iż jeśli zdarzy się utrata zasilania, przykładowo, pomiędzy czasem kiedy blok danych zostanie zaalokowany a kiedy zostanie połącstrefy wewnątrz katalogu, system plików pozostanie w stanie nieustalonym (niespójnym). Wcześniej problem ten był rozwiązywany poprzez użycie polecenia <command>fsck</command>. Było ono odpowiedzialne za przeczytanie i zweryfikowanie stanu systemu plików oraz wykonanie prób naprawy każdej napotkanej niespójności. Problem ten było dokuczliwy i nigdy nie gwarantował rozwiązania wszystkich problemów. Dopiero niedawno przedstawiono koncepcję <emphasis>indeksowania</emphasis> (journaling). Polega ona na zapisywaniu wszystkich działań w oddzielnym indeksie, który może być odtworstrefy bezpiecznie, jeśli system ulegnie awarii. Taki proces tworzy niepotrzebną nadmiarowość, ponieważ dane muszą być zapisywane podwójnie, a czasem wynikiem są dodatkowe problemy, takie jak wtedy, gdy indeksowania nie mogą być odtworzone poprawnie.<indexterm xml:id="indexterm-4">
<primary>System plików ZFS</primary>
<secondary>terminologia transakcyjna</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-5">
<primary>terminologia transakcyjna</primary>
<secondary>opis</secondary>
</indexterm>
</para>
<para>W transakcyjnym systemie plików, dane zarządzane są sposobem <emphasis>copy on write</emphasis>. Dane nie są nigdy nadpisywane, a żadna z sekwencji operacji nie jest zarówno nigdy całkowicie zatwierdzana, ani całkowicie ignorowana. Mechanizm ten oznacza, iż system plików nigdy nie zostanie popsuty poprzez przypadkowy zanik zasilania lub awarię systemu. Zatem nie ma potrzeby, by istniał odpowiednik polecenia <command>fsck</command>. Kiedy ostatnio zapisywane części danych mogły zostać utracone, system plików zawsze pozostaje spójnym. Ponadto zsynchronizowane dane (zapisywane z flagą <varname>O_DSYNC</varname>), są zawsze zapisywane zanim zostaną zwrócone, dlatego istnieje gwarancja iż nie ma możliwości ich utraty.</para>
</sect2>
<sect2 xml:id="gaypb">
<title>Sumy kontrolne i samonaprawa danych </title>
<para>W ZFS-ie, z wszystkich danych zlicza się sumy kontrolne przy wykorzystanie algorytmu wybranego przez użytkownika. Tradycyjne systemy plików pozwalały na przeprowadzanie sum kontrolnych na poziomie bloków danych, poza poziomem managera woluminów i tradycyjnego systemu plików. Ten tradycyjny schemat oznacza, że pewne rodzaje błędów, takie jak zapis całego bloku do nieprawidłowej lokacji, może skutkować poprawną sumą kontrolną w rzeczywistości niepoprawnych danych. Sumy kontrolne ZFS są przechowywane w taki sposób, iż takiego rodzaju awarie są wykrywane i mogą być odzyskane bez problemów. Całe obliczanie sum kontrolnych i odzyskiwanie danych, wykonywane jest na warstwie systemu plików i jest nie widoczna dla aplikacji.<indexterm xml:id="indexterm-6">
<primary>System plików ZFS</primary>
<secondary>checksummed data</secondary>
<tertiary>description</tertiary>
</indexterm>
<indexterm xml:id="indexterm-7">
<primary>checksummed data</primary>
<secondary>description</secondary>
</indexterm>
</para>
<para>Dodatkowo, ZFS posiada zdolność samodzielnej naprawy danych. ZFS wspiera pule nośników danych wraz z różnymi poziomami redundancji danych, włączając w to mirroring i wariacje RAID-5. Gdy zostanie wykryty zły blok danych, ZFS importuje poprawne dane z innej kopii, naprawia złe dane, zamieniając je na dobrą kopię.</para>
</sect2>
<sect2 xml:id="gayou">
<title>Niezrównana skalowalność</title>

<para>ZFS został zaprojektowany od podstaw, aby być najbardziej skalowalnym systemem plików. System sam w sobie jest 128-bitowy, pozwalając na 2<superscript>128</superscript> bajtów danych. Wszystkie dane są alokowane dynamicznie, zatem nie ma potrzeby wcześniejszego alokowania danych, innymi słowy ograniczenia skalowalności systemu plików, kiedy jest on tworstrefy na początku. Całość algorytmów została napisana z myślą o skalowalności. Katalogi mogą zawierać do 2<superscript>48</superscript> (256 trylionów) wpisów oraz nie ma limitu dla ilości systemów plików jak i ilości plików jakie może on zawierać.</para>
</sect2>
<sect2 xml:id="gbcbn">
<title>Obrazy ZFS</title>

<para><emphasis>Obraz</emphasis> jest kopią tylko do odczytu systemu plików lub woluminu. Obrazy mogą być tworzone szybko i łatwo. Początkowo obraz nie zajmuje dodatkowej przestrzeni dyskowej wewnątrz puli.</para>
<para>W trakcie gdy dane wewnątrz aktywnego dataset'u ulegają zmianom, obraz zaczyna zajmować miejsce jako kontynuacja referencji do starych danych. Jako rezultat tego, obraz zapobiega uwolnieniu danych z powrotem do puli.</para>
</sect2>
<sect2 xml:id="gayoc">
<title>Uproszczona Administracja</title>

<para>Co najważniejsze, ZFS posiada znacznie uproszcstrefy model administrowania. Poprzez użycie hierarchicznego wyglądu systemu plików, dziedziczenie właściwości, samozarządzalność punktów montowania i semantykę udostępniania NFS, ZFS umożliwia tworzenie i zarządzanie systemem plików bez bez potrzeby stosowania wielu poleceń lub edytowań plików konfiguracyjnych. Możesz łatwo ustawić quota'y lub rezerwacje, włączyć / wyłączyć kompresję lub zarządzać punktami montowań dla kilku systemu plików za pomocą jednego polecenia. Urządzenia mogą być przetestowane lub naprawione bez potrzeby znajomości oddzielnego zestawu poleceń dla managera woluminów. Możliwym jest też stworzenie nieograniczonej ilości obrazów systemu plików w trybie natychmiastowym. Można tworzyć kopie zapasowe i odtwarzać dane indywidualnych systemów plików.<indexterm xml:id="indexterm-8">
<primary>System plików ZFS</primary>
<secondary>uproszczona administracja</secondary>
<tertiary>opis</tertiary>
</indexterm>
<indexterm xml:id="indexterm-9">
<primary>uproszczona administracja</primary>
<secondary>opis</secondary>
</indexterm>
</para>
<para>ZFS zarządza systemem plików poprzez hierarchię dlatego umożliwia to uproszczenie zarządzania właściwościami takimi jak quota'y, rezerwacje, kompresja i punkty montowań. W tym modelu, system plików staje się centralnym punktem kontrolnym. Sam system plików staje się tani (w porównaniu do nowego katalogu), zatem zachęcającym jest tworzenie systemu plików dla każdego użytkownika, projektu, obszaru działania, itd. Ten model pozwala na definiowanie dopasowanych punktów zarządzania.</para>
</sect2>
</sect1>
<sect1 xml:id="ftyue">
<title>Terminologia ZFS</title>
<para>Ta sekcja opisuje podstawową terminologię wykorzystaną w tej książce:</para>
<variablelist>
<varlistentry>
<term>suma kontrolna</term>
<listitem>
<para>256-bitowe mieszanie danych w blokach systemu plików. Suma kontrolna może zawierać się w zakresie od prostego i szybkiego fletcher2 (domyślnie) do kryptograficznie mocnego mieszania jak SHA256.<indexterm xml:id="indexterm-10">
<primary>System plików ZFS</primary>
<secondary>suma kontrolna</secondary>
<tertiary>definicja</tertiary>
</indexterm>
<indexterm xml:id="indexterm-11">
<primary>suma kontrolna</primary>
<secondary>definicja</secondary>
</indexterm>
<indexterm xml:id="indexterm-12">
<primary>terminologia</primary>
<secondary>suma kontrolna</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
<varlistentry>
<term>klon</term>
<listitem>
<para>Jest to system plików, którego zawartość początkowa jest identyczna z zawartością obrazu.</para>
<para>W celu uzyskania dodatkowych informacji, zobacz <olink remap="external" targetdoc="chapter-6.xml" targetptr="gbcxz">Wprowadzenie do klonów ZFS</olink>.<indexterm xml:id="indexterm-13">
<primary>System plików ZFS</primary>
<secondary>klony</secondary>
<tertiary>definicja</tertiary>
</indexterm>
<indexterm xml:id="indexterm-14">
<primary>klon</primary>
<secondary>definicja</secondary>
</indexterm>
<indexterm xml:id="indexterm-15">
<primary>terminologia</primary>
<secondary>klony</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
<varlistentry>
<term>dataset</term>
<listitem>
<para>Ogólna nazwa dla następujących elementów ZFS: klony, systemy plików, obrazy lub woluminy.</para>
<para>Każdy dataset jest identyfikowany poprzez unikalną nazwę w przestrzeni nazw ZFS. Dataset-y są identyfikowane przez użycie następującego formatu:</para>
<para>
<replaceable>pula</replaceable>/<replaceable>ścieżka</replaceable>[<replaceable>@obraz</replaceable>]</para>
<variablelist>
<varlistentry>
<term>
<replaceable>pula</replaceable>
</term>
<listitem>
<para>Identyfikuje nazwę puli nośników danych, która zawiera dataset</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<replaceable>ścieżka</replaceable>
</term>
<listitem>
<para>Jest to separowana ukośnikiem nazwa ścieżki do obiektu dataset</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<replaceable>obraz</replaceable>
</term>
<listitem>
<para>Jest komponentem opcjonalnym, który identyfikuje obraz dataset-u</para>
</listitem>
</varlistentry>
</variablelist>
<para>W celu uzyskania dodatkowych informacji, zobacz <olink remap="external" targetdoc="chapter-5.xml" targetptr="gavwq">Rozdział 5, Zarządzanie systemem plików ZFS</olink>.<indexterm xml:id="indexterm-16">
<primary>System plików ZFS</primary>
<secondary>dataset</secondary>
<tertiary>definicja</tertiary>
</indexterm>
<indexterm xml:id="indexterm-17">
<primary>dataset</primary>
<secondary>definicja</secondary>
</indexterm>
<indexterm xml:id="indexterm-18">
<primary>terminologia</primary>
<secondary>dataset</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
<varlistentry>
<term>system plików</term>
<listitem>
<para>Dataset, który zawiera standardowy system plików POSIX.</para>
<para>W celu uzyskania dodatkowych informacji, zobacz <olink remap="external" targetdoc="chapter-5.xml" targetptr="gavwq">Rozdział 5, Zarządzanie systemem plików ZFS</olink>.<indexterm xml:id="indexterm-19">
<primary>System plików ZFS</primary>
<secondary>system plików</secondary>
<tertiary>definicja</tertiary>
</indexterm>
<indexterm xml:id="indexterm-20">
<primary>system plików</primary>
<secondary>definicja</secondary>
</indexterm>
<indexterm xml:id="indexterm-21">
<primary>terminologia</primary>
<secondary>system plików</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
<varlistentry>
<term>mirror</term>
<listitem>
<para>Urządzenie wirtualne, która przechowuje identyczne kopie danych na dwóch lub więcej, dyskach. Jeśli którykolwiek z dysków w konfiguracji mirror`a popsuje się, inny dysk może udostępniając te same dane.<indexterm xml:id="indexterm-22">
<primary>Pule nośników danych ZFS</primary>
<secondary>mirror</secondary>
<tertiary>definicja</tertiary>
</indexterm>
<indexterm xml:id="indexterm-23">
<primary>mirror</primary>
<secondary>definicja</secondary>
</indexterm>
<indexterm xml:id="indexterm-24">
<primary>terminologia</primary>
<secondary>mirror</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
<varlistentry>
<term>pula</term>
<listitem>
<para>Logiczna grupa urządzeń opisujących wygląd i charakterystyki fizyczne dostępnych nośników danych. Przestrzeń dla dataset-ów jest poświęcana z puli.</para>
<para>W celu uzyskania dodatkowych informacji, zobacz <olink remap="external" targetdoc="chapter-4.xml" targetptr="gavwn">Rozdział 4, Zarządzanie pulami nośników danych ZFS</olink>.<indexterm xml:id="indexterm-25">
<primary>pule nośników danych ZFS</primary>
<secondary>pula</secondary>
<tertiary>definicja</tertiary>
</indexterm>
<indexterm xml:id="indexterm-26">
<primary>pula</primary>
<secondary>definicja</secondary>
</indexterm>
<indexterm xml:id="indexterm-27">
<primary>terminologia</primary>
<secondary>pula</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
<varlistentry>
<term>RAID-Z</term>
<listitem>
<para>Wirtualne urządzenie, które przechowuje dane i dane parzystości na wielu dyskach, zbliżone do RAID-5. <indexterm xml:id="indexterm-28">
<primary>pule nośników danych ZFS</primary>
<secondary>RAID-Z</secondary>
<tertiary>definicja</tertiary>
</indexterm>
<indexterm xml:id="indexterm-29">
<primary>RAID-Z</primary>
<secondary>definicja</secondary>
</indexterm>
<indexterm xml:id="indexterm-30">
<primary>terminologia</primary>
<secondary>RAID-Z</secondary>
</indexterm>W celu uzyskania dodatkowych informacji, zobacz <olink remap="external" targetdoc="chapter-4.xml" targetptr="gamtu">Konfiguracja puli nośników danych RAID-Z</olink>.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>resilvering</term>
<listitem>
<para>Proces transferu danych z jednego urządzenia na drugie jest znany jako <emphasis>resilvering</emphasis>. Przykładowo, jeśli mirror zostanie wymieniony lub zostanie wyłącstrefy, dane z mirror'a który jest aktualny są kopiowane do nowo przywróconego komponentu mirror'u. Ten proces odpowiada <emphasis>resynchronizacji mirror'a</emphasis> w tradycyjnym managerze woluminów.</para>
<para>W celu uzyskania dodatkowych informacji, zobacz <olink remap="external" targetdoc="chapter-9.xml" targetptr="gbcus">Podgląd postępu resilvering'u</olink>.<indexterm xml:id="indexterm-31">
<primary>pule nośników danych ZFS</primary>
<secondary>resilvering</secondary>
<tertiary>definicja</tertiary>
</indexterm>
<indexterm xml:id="indexterm-32">
<primary>resilvering</primary>
<secondary>definicja</secondary>
</indexterm>
<indexterm xml:id="indexterm-33">
<primary>terminologia</primary>
<secondary>resilvering</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
<varlistentry>
<term>obraz</term>
<listitem>
<para>Obraz systemu plików lub woluminu tylko do odczytu z danego punktu czasowego.</para>
<para>W celu uzyskania dodatkowych informacji, zobacz <olink remap="external" targetdoc="chapter-6.xml" targetptr="gbciq">Wprowadzenie do obrazów ZFS</olink>.<indexterm xml:id="indexterm-34">
<primary>System plików ZFS</primary>
<secondary>obraz</secondary>
<tertiary>definicja</tertiary>
</indexterm>
<indexterm xml:id="indexterm-35">
<primary>snapshot</primary>
<secondary>definicja</secondary>
</indexterm>
<indexterm xml:id="indexterm-36">
<primary>terminologia</primary>
<secondary>obraz</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
<varlistentry>
<term>urządzenie wirtualne</term>
<listitem>
<para>Logiczne urządzenie w puli, które może być fizycznym urządzeniem, plikiem lub zbiorem urzadzeń.</para>
<para>W celu uzyskania dodatkowych informacji, zobacz <olink remap="external" targetdoc="chapter-4.xml" targetptr="gazca">Rozpoznawanie wirtualnych urządzeń w pulach nośników danych</olink>.<indexterm xml:id="indexterm-37">
<primary>pule nośników danych ZFS</primary>
<secondary>urządzenie wirtualne</secondary>
<tertiary>definicja</tertiary>
</indexterm>
<indexterm xml:id="indexterm-38">
<primary>urządzenie wirtualne</primary>
<secondary>definicja</secondary>
</indexterm>
<indexterm xml:id="indexterm-39">
<primary>terminologia</primary>
<secondary>urządzenie wirtualne</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
<varlistentry>
<term>wolumin</term>
<listitem>
<para>Dataset użyty w celu emulacji fizycznego urządzenia. Na przykład można użyć emulowanego woluminu w celu stworzenia urządzenia wymiany danych (swap device).</para>
<para>W celu uzyskania dodatkowych informacji, zobacz <olink remap="external" targetdoc="chapter-8.xml" targetptr="gaypf">Woluminy ZFS</olink>.<indexterm xml:id="indexterm-40">
<primary>System plików ZFS</primary>
<secondary>wolumin</secondary>
<tertiary>definicja</tertiary>
</indexterm>
<indexterm xml:id="indexterm-41">
<primary>wolumin</primary>
<secondary>definicja</secondary>
</indexterm>
<indexterm xml:id="indexterm-42">
<primary>terminologia</primary>
<secondary>wolumin</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
</variablelist>
</sect1>
<sect1 xml:id="gbcpt">
<title>Wymagania nazewnictwa komponentów ZFS</title>
<para>Każdy komponent ZFS musi posiadać nazwę zgodną z następującymi zasadami:<indexterm xml:id="indexterm-43">
<primary>System plików ZFS</primary>
<secondary>wymagania nazewnictwa komponentów</secondary>
</indexterm>
<indexterm xml:id="indexterm-44">
<primary>komponenty ZFS</primary>
<secondary>wymagania nazewnictwa</secondary>
</indexterm>
<indexterm xml:id="indexterm-45">
<primary>wymagania nazewnictwa</primary>
<secondary>komponenty ZFS</secondary>
</indexterm>
</para>
<itemizedlist>
<listitem>
<para>Puste komponenty są niedozwolone.</para>
</listitem>
<listitem>
<para>Każdy komponent może zawierać tylko alfanumeryczne znaki, dodatkowo cztery następujące znaki:</para>
<itemizedlist>
<listitem>
<para>Podkreślnik (_)</para>
</listitem>
<listitem>
<para>Pauza (-)</para>
</listitem>
<listitem>
<para>Dwukropek (:)</para>
</listitem>
<listitem>
<para>Kropka (.)</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Nazwy puli muszą zaczynać się literą, z wyjątkiem sekwencji c[0-9], która jest niedozwolona. Także nazwy puli zaczynające się <literal>mirror</literal>, <literal>raidz</literal> lub <literal>spare</literal> są niedozwolone, gdyż te nazwy są już zajęte.</para>
</listitem>
<listitem>
<para>Nazwy dataset-ów muszą zaczynać się znakiem alfanumerycznym.</para>
</listitem>
</itemizedlist>
</sect1>
</chapter>
