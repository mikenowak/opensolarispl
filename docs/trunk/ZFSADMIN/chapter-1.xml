<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML//EN" "docbook.dtd"[
	<!ENTITY % xinclude SYSTEM "xinclude.mod">
	%xinclude;
]>

<chapter xmlns="http://docbook.org/ns/docbook" version="5.0" xml:id="zfsover-1">



<title>System plików ZFS (Wprowadzenie)</title>
<toc>
<para>Ten rozdział zawiera przegląd możliwości oraz korzyści związanych z systemem plików ZFS. Wyjaśnia także podstawową terminologię używaną przez resztę tej książki.</para>
<para>W tym rozdziale znajdują się następujące sekcje:</para>
<itemizedlist>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gbscy">Co nowego w ZFS-ie?</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="zfsover-2">Czym jest ZFS?</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="ftyue">Terminologia</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gbcpt">Nazewnictwo elementów składowych ZFS-ie</olink>
</para>
</listitem>
</itemizedlist>
</toc>
<sect1 xml:id="gbscy">
<title>Co nowego w ZFS-ie?</title>
<para>Sekcja ta zestawia nowe cechy, które zostały dodane do systemu plików ZFS już po początkowym
wypuszczeniu go w dystrybucji Solaris Express w grudniu 2005 r.</para>
<itemizedlist>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gcsxk">Używanie ZFS-a do powielania zon innych niż globalna i inne ulepszenia</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gciui">Komendy ZFS-a do backupu i odtwarzania systemu plików mają zmienioną nazwę</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gcitn">Odzyskiwanie zniszczonych puli</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gcfhy">ZFS został zintegrowany z "Fault Manager"</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gcfiw">Nowe polecenie <command>zpool clear</command></olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gcajn">Compact NFSv4 ACL Format</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gcakl">Narzędzie do monitorowania systemów plików (<command>fsstat</command>)</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gbsbp">Zarządzanie ZFS przez interfejs WWW</olink>
</para>
</listitem>
</itemizedlist>
<sect2 xml:id="gcsxk">
<title>Używanie ZFS-a do powielania zon innych niż globalna i inne ulepszenia</title>
<para>
<emphasis role="strong">OpenSolaris, build 39:</emphasis> Jeżeli wzorcowy parametr <literal>zonepath</literal> i docelowy <literal>zonepath</literal> znajdują się na ZFS-ie i dodatkowo w tej samej puli ZFS-owej, polecenie <command>zoneadm clone</command> automatycznie użyje funkcjonalności klonowania systemu plików ZFS w celu utworzenia sklonowanej zony. Oznacza to, że polecenie <command>zoneadm clone</command> robi obraz (snapshot) systemu plików wskazywanego przez źródłowy parametr <literal>zonepath</literal> i ustawia go jako docelowy.
Obraz ten jest nazywany w notacji <literal>SUNWzoneX</literal>, gdzie <literal>X</literal> jest unikatowym identyfikatorem używanym do rozróżnienia poszczególnych obrazów. Parametr <literal>zonepath</literal> docelowej zony jest używany jako nazwa sklonowanego systemu plików ZFS. Tworzona jest programowa inspekcja poprawności obrazu, aby mógł on zostać poźniej użytym przez system. Zauważ, że nadal posiadasz możliwość skopiowania parametru źródłowego ZFS <literal>zonepath</literal> zamiast klonu ZFS, jeśli zajdzie taka potrzeba.</para>
<para>Aby umożliwić kilkukrotnie klonowanie zon, dodano nowy paramter polecenia <command>zoneadm</command> pozwalający na określenie, iż powinieć zostać użyty istniejący obraz. System dokonuje inspekcji poprawności obrazu, aby mógł on zostać poźniej użyty przez system. Dodatkowo, proces instalacyjny zony, ma teraz zdolność detekcji kiedy system plików ZFS może być stworzony dla zony, a proces deinstalacyjny potrafi sprawdzić czy system plików ZFS w zonie może zostać usunięty. Kroki te są wykonywane automatycznie przez polecenie <command>zoneadm</command> .</para> 
<para>Nie należy wykorzystywać opcji tworzenie obrazów ZFS w celu klonowania zony.</para>
<para>W celu uzyskanie większej ilości informacji, zobacz <olink targetdoc="819-2450" remap="external">
<citetitle remap="book">System Administration Guide: Solaris Containers-Resource Management
and Solaris Zones</citetitle>
</olink>.</para>
</sect2>
<sect2 xml:id="gciui">
<title>Komendy ZFS-a do backupu i odtwarzania systemu plików mają zmienioną nazwę</title>
<para>
<emphasis role="strong">OpenSolaris, build 38:</emphasis> W tym wydaniu Solarisa polecenia <command>zfs backup</command> oraz <command>zfs restore</command>, zostały przemianowane na <command>zfs send</command> i <command>zfs receive</command>, żeby bardziej precyzyjnie opisywały swoją funkcję. Rolą tych poleceń jest zapis i odtworzenie strumienia danych ZFS-a.</para>
<para>W celu uzyskania większej ilości informacji o tych poleceniach, zobacz <olink targetdoc="" remap="internal" targetptr="gbchx">Saving and Restoring ZFS Data</olink>.</para>
</sect2>
<sect2 xml:id="gcitn">
<title>Odzyskiwanie zniszczonych puli</title>
<para>
<emphasis role="strong">OpenSolaris, build 37:</emphasis> To wydanie wprowadziło polecenie <command>zpool import</command> <option>D</option>, które umożliwia odtworzenie puli, które zostały wcześniej zniszczone przez <command>zpool destroy</command>.</para>
<para>W celu uzyskanie większej ilości informacji, zobacz <olink targetdoc="" remap="internal" targetptr="gcfhw">Recovering Destroyed ZFS Storage Pools</olink>.</para>
</sect2>
<sect2 xml:id="gcfhy">
<title>ZFS został zintegrowany z "Fault Manager"</title>
<para>
<emphasis role="strong">OpenSolaris, build 36:</emphasis> Wydanie to zawiera zintegrowany silnik diagnostyczny ZFS, który jest zdolny do diagnozowania oraz raportowania awarii puli i urządzeń. Sumy kontrolne, I/O, błedy puli i urządzeń związane z awariami urządzeń i puli są również raportowane.</para>
<para>Silnik diagnostyczny nie prognozuje na podstawie analizy sum kontrolnych oraz błędów I/O, jednakże zdolność prognozowania na podstawie analizy błędu.</para>
<para>W przypadku awarii ZFS, być może zobaczysz komunikat podobny to następującego<command>fmd</command>:</para>
<screen>SUNW-MSG-ID: ZFS-8000-D3, TYPE: Fault, VER: 1, SEVERITY: Major
EVENT-TIME: Fri Mar 10 11:09:06 MST 2006
PLATFORM: SUNW,Ultra-60, CSN: -, HOSTNAME: neo
SOURCE: zfs-diagnosis, REV: 1.0
EVENT-ID: b55ee13b-cd74-4dff-8aff-ad575c372ef8
DESC: A ZFS device failed.  Refer to http://sun.com/msg/ZFS-8000-D3 for more information.
AUTO-RESPONSE: No automated response will occur.
IMPACT: Fault tolerance of the pool may be compromised.
REC-ACTION: Run 'zpool status -x' and replace the bad device.</screen>
<para>Poprzez zapoznanie się z rekomendowanymi krokami, które będą przestawione przez polecenie <command>zpool status</command>, będziesz mógł szybko zidentyfikować i naprawić awarię.</para>
<para>W celu zapoznania się z przykładami rozwiązywanie problemów raportowanych przez ZFS, zobacz <olink targetdoc="" remap="internal" targetptr="gbbvb">Repairing a Missing Device</olink>.</para>
</sect2>
<sect2 xml:id="gcfiw">
<title>Nowe polecenie <command>zpool clear</command></title>
<para>
<emphasis role="strong">OpenSolaris, build 36:</emphasis> Wydanie to zawiera komendę <command>zpool clear</command>, służącą do zerownia licznika błędów związanych z urządzeniem lub pulą. Poprzednio, licznik błędów był zerowany, kiedy urządzenie w puli było włączane ponownie za pomocą polecenia <command>zpool online</command>.
W celu uzyskanie większej ilości informacji, zobacz <olink targetdoc="819-2240" remap="external" targetptr="zpool-1m">
<citerefentry>
<refentrytitle>zpool</refentrytitle>
<manvolnum>
1M
</manvolnum>
</citerefentry>
</olink> and <olink targetdoc="" remap="internal" targetptr="gazge">Clearing Storage Pool Devices</olink>.</para>
</sect2>
<sect2 xml:id="gcajn">
<title>Compact NFSv4 ACL Format</title>
<para>
<emphasis role="strong">OpenSolaris, build 34:</emphasis> W wydniu tym, są dostępne trzy formaty NFSv4 ACL: informacyjny (verbose), pozycyjny i upakowany. Nowe formaty ACL upakowany i pozycyjny są dostępne, aby ustawiać i wyświetlać ACL. Możesz używać polecenia <command>chmod</command>, aby ustawić wszystkie 3 formaty ACL. Możesz używać polecenia <command>ls</command> <option>V</option>, aby wyświetlić upakowany i pozycyjny format ACL oraz polecenia <command>ls</command> <option> v</option>, aby wyświetlić format informacyjny (verbose) ACL.</para>
<para>W celu uzyskanie większej ilości informacji, zobacz <olink targetdoc="" remap="internal" targetptr="gbchf">Setting and Displaying ACLs on ZFS Files in Compact Format</olink>, <olink targetdoc="819-2239" remap="external" targetptr="chmod-1">
<citerefentry>
<refentrytitle>chmod</refentrytitle>
<manvolnum>
1
</manvolnum>
</citerefentry>
</olink>, and <olink targetdoc="819-2239" remap="external" targetptr="ls-1">
<citerefentry>
<refentrytitle>ls</refentrytitle>
<manvolnum>
1
</manvolnum>
</citerefentry>
</olink>.</para>
</sect2>
<sect2 xml:id="gcakl">
<title>Narzędzie Monitorowania Systemu Plików (File System Monitoring Tool) (<command>fsstat</command>)</title>
<para>
<emphasis role="strong">OpenSolaris, build 34:</emphasis> Nowe narzędzie do monitorowania systemu plików, <command>fsstat</command>, jest dostępne do raportowania operacji na systemie plików. Aktywność może być raportowana według punktu montowania systemu plików, bądź według jego typu. Poniższy przykład przedstawia ogólne statystyki aktywności systemu plików ZFS.</para>
<screen>% <userinput>fsstat zfs</userinput>
 new  name   name  attr  attr lookup rddir  read read  write write
 file remov  chng   get   set    ops   ops   ops bytes   ops bytes
 729K  488K  282K 79.8M  266K   333M 5.33M 24.8M  115G 2.36M 27.8G zfs</screen>
<para>W celu uzyskanie większej ilości informacji, zobacz <citerefentry>
<refentrytitle>fsstat</refentrytitle>
<manvolnum>
1M
</manvolnum>
</citerefentry>.</para>
</sect2>
<sect2 xml:id="gbsbp">
<title>Zarządzanie ZFS przez interfejs WWW</title>
<para>
<emphasis role="strong">OpenSolaris, build 28:</emphasis> Zostało udostępnione narzędzie administracyjne ZFS zbudowane w oparciu o interfejs web. Za pomocą tego narzędzia można wykonywac następujące czynności:</para>
<itemizedlist>
<listitem>
<para>Tworzenie nowej puli nośników danych.</para>
</listitem>
<listitem>
<para>Dodawanie przestrzeni dla danych do istniejącej puli.</para>
</listitem>
<listitem>
<para>Przenoszenie (export) puli nośników danych do innego systemu.</para>
</listitem>
<listitem>
<para>Import uprzednio exportowanych puli nośników danych, celem udostępnienia ich w innym systemie.</para>
</listitem>
<listitem>
<para>Podgląd informacji o puli nośników danych.</para>
</listitem>
<listitem>
<para>Tworzenie systemu plików.</para>
</listitem>
<listitem>
<para>Tworzenie woluminu.</para>
</listitem>
<listitem>
<para>Tworzenie obrazu systemu plików lub woluminu.</para>
</listitem>
<listitem>
<para>Przywracanie systemu plików do poprzedniego obrazu.</para>
</listitem>
</itemizedlist>
<para>Możesz uzyskać dostęp do konsoli Administracji ZFS, poprzez bezpieczne połącznie, przy pomocy przeglądarki web pod następującym adresem URL:</para>
<screen>https://<replaceable>system-name</replaceable>:6789/zfs</screen>
<para>Jeśli wpisałeś poprawny adres URL i nie jesteś w stanie połączyć sie z konsolą Administracji ZFS, znaczy to iż serwer prawdopodobnie nie jest uruchomiony. Aby go uruchomić, uruchom następujące polecenie:</para>
<screen># /usr/sbin/smcwebserver start</screen>
<para>Jeśli chcesz, aby serwer uruchamiał się automatycznie podczas startu systemu, wykonaj następujące polecenie:</para>
<screen># /usr/sbin/smcwebserver enable</screen>
</sect2>
</sect1>
<sect1 xml:id="zfsover-2">
<title>Czym jest ZFS?</title>
<para>Solarisowy system plików ZFS jest rewolucyjnym, nowym systemem plików, który zmienia zmienia sposoby administracji systemem plików oraz którego funkcjonalność oraz korzyści z niej płynące są niespotykane dzisiaj w żadnym innym systemie plików. ZFS został zaprojektowany by być mocnym, skalowalnym oraz prostym w administracji.<indexterm xml:id="indexterm-1">
<primary>ZFS file systems</primary>
<secondary>description</secondary></indexterm>
</para>
<sect2 xml:id="gaypk">
<title>Pula danych ZFS</title>
<para>ZFS używa koncepcji <emphasis>puli nośników danych</emphasis>, aby zarządzać urządzeniami fizycznymi. Uprzednio systemy plików były konstruowane na pojedynczych, fizycznych urządzeniach. Aby zaadresować wiele urządzeń oraz zapewnienić redundancję, został przedstawiony koncept <emphasis>managera woluminów</emphasis>, który prezentował obraz pojedynczego urządzenia, który nie musiał być modyfikowany aby korzystać z zalet wykorzystania kilku urządzeń. Taki schemat stworzył dodatkowy poziom i ostatecznie uniemożliwił wykorzystanie pewnych zalet systemu plików, ponieważ system plików nie miał kontroli nad fizyczną lokalizacją danych w wirtualnych woluminach.<indexterm xml:id="indexterm-2">
<primary>ZFS file systems</primary>
<secondary>pooled storage</secondary>
<tertiary>description</tertiary>
</indexterm>
<indexterm xml:id="indexterm-3">
<primary>pooled storage</primary>
<secondary>description</secondary>
</indexterm>
</para>
<para>ZFS eliminuje całkowicie managera woluminów. Zamiast zmuszać  do tworzenia wirtualnych woluminów, ZFS grupuje urządzenia w pule nośników danych. Pula nośników danych opisuje fizyczną charakterystykę zbioru danych (podział logiczny urządzenia, sposób redundancji danych, etc.) i działa jako magazyn danych, z którego systemy plików mogą zostać stworzone. Systemy plików nie zawierą już indywidualnych urzadzeń, co pozwala im na dzielenie się przestrzenią dyskową ze wszystkimi systemami plików w puli. Nie trzeba już dłużej z góry definiować wielkości systemu plików, rośnie on automaycznie w przestrzeni dyskowej, jaką zadedykowano na potrzeby puli. W momencie gdy zostają dodane nowe urządznia, wszystkie systemy plików potrafią zaczą używać dodatkowej przestrzeni, bez potrzeby ingerencji. W pewien sposób, pule nośników danych zachowują się podobnie do systemu pamięć wirtualnej. Kiedy dodawane są pamięci DIMM do systemu, system operacyjny nie wymusza wykonania jakichkolwiek poleceń, aby skonfigurować pamięć a przypisać ją do indywidaulanego procesu. Wszystkie procesy w systemie automatycznie wykorzystują dodatkową pamięć.</para>
</sect2>
<sect2 xml:id="gaypi">
<title>Terminologia transakcyjna</title>

<para>ZFS jest transakcyjnym systemem plików, co znaczy że stan systemu plików na dysku jest zawsze spójny. Tradycyjny system plików nadpisuje dane, co oznacza iż jeśli zdarzy się utrata zasilania, przykładowo, pomiędzy czasem kiedy blok danych zostanei zaalokowany a kiedy zostanie połączony wewnątrz katalogu, system plików pozostanie w stanie nieustalonym (niespójnym). Wcześniej problem ten był rozwiązywany poprzez użycie polecenia <command>fsck</command>. Było ono odpowiadzialne za przeczytanie i zweryfikowanie stanu systemu plików oraz wykonanie prób naprawy każdej napotkanej niespojności. Problem ten było dokuczliwy i nigdy nie gwarantował rozwiązanie wszystkich problemów. Dopiero niedawno przedstawiono koncepcję <emphasis>indeksowania</emphasis> (journaling). Polega ona na zapisywaniu wszystkich działań w oddzielnym indeksie, który może być odtworzony bezpiecznie, jeśli system ulegnie awarii. Taki proces tworzy niepotrzebną nadmiarowość, ponieważ dane muszą być zapisywane podwójnie, a czasem wynikiem są dodatkowe problemy, takie jak gdy indeksowania nie mogą być odtworzone poprawnie.<indexterm xml:id="indexterm-4">
<primary>ZFS file systems</primary>
<secondary>transactional semantics</secondary>
<tertiary>description</tertiary>
</indexterm>
<indexterm xml:id="indexterm-5">
<primary>transactional semantics</primary>
<secondary>description</secondary>
</indexterm>
</para>
<para>W transakcyjnym systemie plików, dane zarządzanie są sposobu <emphasis>copy on write</emphasis>. Dane nie są nigdy nadpisywane, a żadna z sekwencji operacji nie jest zarówno nigdy całkowicie zatwierdzana, ani całkowiecie ignorowana. Mechanizm ten oznacza, iż system plików niegdy nie zostanie popsuty poprzez przypadkowy zanik zasilania lub awarię systemu. Zatem nie ma potrzeby, by istniał odpowiednik polecenia <command>fsck</command>. Kiedy ostatnio zapisywane częsci danych mogły zostać utracone, system plików zawsze pozostaje spójnym. Ponadto zsynchronizowane dane (zapisywane z flagą <varname>O_DSYNC</varname>), są zawsze zapisywane zanim zostaną zwrócone, dlatego istnieje gwarancja iż nie istnieje możliwość ich utraty.</para>
</sect2>
<sect2 xml:id="gaypb">
<title>Sumy kontrolne i samonaprawa danych </title>

<para>W ZFS, z wszystkich danych zlicza się sumy kontrolne przy wykorzystanie algorytmu wybranego przez użytkownika. Tradycyjne systemy plików pozwalały na przeprowadzanie sum kontrolnych na poziomie bloków danych, poza poziomem managera woluminów i tradycyjnego systemu plików. Ten tradycyjny schemat oznacza, że pewne rodzaje błędów, takie jak zapis całego bloku do nieprawidłowej lokacji, może skutkować poprawną sumą kontrolną w rzczywistość niepoprawnych danych. Sumy kontrolne ZFS sa przechowywane w taki sposób, iż takiego rodzaju awarie są wykrywane i mogą być odzyskane bez problemów. Cały obliczanie sum kontrolnych i odzyskiwanie danych, wykonywane jest na warstwie systemu plików i jest nie widoczna dla aplikacji.<indexterm xml:id="indexterm-6">
<primary>ZFS file systems</primary>
<secondary>checksummed data</secondary>
<tertiary>description</tertiary>
</indexterm>
<indexterm xml:id="indexterm-7">
<primary>checksummed data</primary>
<secondary>description</secondary>
</indexterm>
</para>
<para>In addition, ZFS provides for self-healing data. ZFS supports storage
pools with varying levels of data redundancy, including mirroring and a variation
on RAID-5. When a bad data block is detected, ZFS fetches the correct data
from another replicated copy, and repairs the bad data, replacing it with
the good copy.</para>
</sect2>
<sect2 xml:id="gayou">
<title>Unparalleled Scalability</title>
<para>ZFS has been designed from the ground up to be the most scalable file
system, ever. The file system itself is 128-bit, allowing for 256 quadrillion
zettabytes of storage. All metadata is allocated dynamically, so no need exists
to pre-allocate inodes or otherwise limit the scalability of the file system
when it is first created. All the algorithms have been written with scalability
in mind. Directories can have up to 2<superscript>48</superscript> (256 trillion)
entries, and no limit exists on the number of file systems or number of files
that can be contained within a file system.</para>
</sect2>
<sect2 xml:id="gbcbn">
<title>ZFS Snapshots</title>
<para>A <emphasis>snapshot</emphasis> is a read-only copy of a file system
or volume. Snapshots can be created quickly and easily. Initially, snapshots
consume no additional space within the pool.</para>
<para>As data within the active dataset changes, the snapshot consumes space
by continuing to reference the old data. As a result, the snapshot prevents
the data from being freed back to the pool.</para>
</sect2>
<sect2 xml:id="gayoc">
<title>Simplified Administration</title>
<para>Most importantly, ZFS provides a greatly simplified administration model.
Through the use of hierarchical file system layout, property inheritance,
and automanagement of mount points and NFS share semantics, ZFS makes it easy
to create and manage file systems without needing multiple commands or editing
configuration files. You can easily set quotas or reservations, turn compression
on or off, or manage mount points for numerous file systems with a single
command. Devices can be examined or repaired without having to understand
a separate set of volume manager commands. You can take an unlimited number
of instantaneous snapshots of file systems. You can backup and restore individual
file systems.<indexterm xml:id="indexterm-8">
<primary>ZFS file systems</primary>
<secondary>simplified administration</secondary>
<tertiary>description</tertiary>
</indexterm>
<indexterm xml:id="indexterm-9">
<primary>simplified administration</primary>
<secondary>description</secondary>
</indexterm>
</para>
<para>ZFS manages file systems through a hierarchy that allows for this simplified
management of properties such as quotas, reservations, compression, and mount
points. In this model, file systems become the central point of control. File
systems themselves are very cheap (equivalent to a new directory), so you
are encouraged to create a file system for each user, project, workspace,
and so on. This design allows you to define fine-grained management points.</para>
</sect2>
</sect1>
<sect1 xml:id="ftyue">
<title>ZFS Terminology</title>
<para>This section describes the basic terminology used throughout this book:</para>
<variablelist>
<varlistentry>
<term>checksum</term>
<listitem>
<para>A 256-bit hash of the data in a file system block. The checksum
capability can range from the simple and fast fletcher2 (the default) to cryptographically
strong hashes such as SHA256.<indexterm xml:id="indexterm-10">
<primary>ZFS file systems</primary>
<secondary>checksum</secondary>
<tertiary>definition</tertiary>
</indexterm>
<indexterm xml:id="indexterm-11">
<primary>checksum</primary>
<secondary>definition</secondary>
</indexterm>
<indexterm xml:id="indexterm-12">
<primary>terminology</primary>
<secondary>checksum</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
<varlistentry>
<term>clone</term>
<listitem>
<para>A file system whose initial contents are identical to the
contents of a snapshot.</para>
<para>For information about clones, see <olink targetdoc="" remap="internal" targetptr="gbcxz">ZFS Clones</olink>.<indexterm xml:id="indexterm-13">
<primary>ZFS file systems</primary>
<secondary>clones</secondary>
<tertiary>definition</tertiary>
</indexterm>
<indexterm xml:id="indexterm-14">
<primary>clone</primary>
<secondary>definition</secondary>
</indexterm>
<indexterm xml:id="indexterm-15">
<primary>terminology</primary>
<secondary>clone</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
<varlistentry>
<term>dataset</term>
<listitem>
<para>A generic name for the following ZFS entities: clones, file
systems, snapshots, or volumes.</para>
<para>Each dataset is identified by a unique name in the ZFS namespace. Datasets
are identified using the following format:</para>
<para>
<replaceable>pool</replaceable>/<replaceable>path</replaceable>[<replaceable>@snapshot</replaceable>]</para>
<variablelist>
<varlistentry>
<term>
<replaceable>pool</replaceable>
</term>
<listitem>
<para>Identifies the name of the storage pool that contains the
dataset</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<replaceable>path</replaceable>
</term>
<listitem>
<para>Is a slash-delimited path name for the dataset object</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<replaceable>snapshot</replaceable>
</term>
<listitem>
<para>Is an optional component that identifies a snapshot of a dataset</para>
</listitem>
</varlistentry>
</variablelist>
<para>For more information about datasets, see <olink targetdoc="" remap="internal" targetptr="gavwq">Chapter 5, Managing ZFS File Systems</olink>.<indexterm xml:id="indexterm-16">
<primary>ZFS file systems</primary>
<secondary>dataset</secondary>
<tertiary>definition</tertiary>
</indexterm>
<indexterm xml:id="indexterm-17">
<primary>dataset</primary>
<secondary>definition</secondary>
</indexterm>
<indexterm xml:id="indexterm-18">
<primary>terminology</primary>
<secondary>dataset</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
<varlistentry>
<term>file system</term>
<listitem>
<para>A dataset that contains a standard POSIX file system.</para>
<para>For more information about file systems, see <olink targetdoc="" remap="internal" targetptr="gavwq">Chapter 5, Managing ZFS File Systems</olink>.<indexterm xml:id="indexterm-19">
<primary>ZFS file systems</primary>
<secondary>file system</secondary>
<tertiary>definition</tertiary>
</indexterm>
<indexterm xml:id="indexterm-20">
<primary>file system</primary>
<secondary>definition</secondary>
</indexterm>
<indexterm xml:id="indexterm-21">
<primary>terminology</primary>
<secondary>file system</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
<varlistentry>
<term>mirror</term>
<listitem>
<para>A virtual device that stores identical copies of data on two
or more disks. If any disk in a mirror fails, any other disk in that mirror
can provide the same data.<indexterm xml:id="indexterm-22">
<primary>ZFS storage pools</primary>
<secondary>mirror</secondary>
<tertiary>definition</tertiary>
</indexterm>
<indexterm xml:id="indexterm-23">
<primary>mirror</primary>
<secondary>definition</secondary>
</indexterm>
<indexterm xml:id="indexterm-24">
<primary>terminology</primary>
<secondary>mirror</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
<varlistentry>
<term>pool</term>
<listitem>
<para>A logical group of devices describing the layout and physical
characteristics of the available storage. Space for datasets is allocated
from a pool.</para>
<para>For more information about storage pools, see <olink targetdoc="" remap="internal" targetptr="gavwn">Chapter 4, Managing ZFS Storage Pools</olink>.<indexterm xml:id="indexterm-25">
<primary>ZFS storage pools</primary>
<secondary>pool</secondary>
<tertiary>definition</tertiary>
</indexterm>
<indexterm xml:id="indexterm-26">
<primary>pool</primary>
<secondary>definition</secondary>
</indexterm>
<indexterm xml:id="indexterm-27">
<primary>terminology</primary>
<secondary>pool</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
<varlistentry>
<term>RAID-Z</term>
<listitem>
<para>A virtual device that stores data and parity on multiple disks,
similar to RAID-5. <indexterm xml:id="indexterm-28">
<primary>ZFS storage pools</primary>
<secondary>RAID-Z</secondary>
<tertiary>definition</tertiary>
</indexterm>
<indexterm xml:id="indexterm-29">
<primary>RAID-Z</primary>
<secondary>definition</secondary>
</indexterm>
<indexterm xml:id="indexterm-30">
<primary>terminology</primary>
<secondary>RAID-Z</secondary>
</indexterm>For more information about RAID-Z, see <olink targetdoc="" remap="internal" targetptr="gamtu">RAID-Z Storage Pool Configuration</olink>.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>resilvering</term>
<listitem>
<para>The process of transferring data from one device to another
device is known as <emphasis>resilvering</emphasis>. For example, if a mirror
component is replaced or taken offline, the data from the up-to-date mirror
component is copied to the newly restored mirror component. This process is
referred to as <emphasis>mirror resynchronization</emphasis> in traditional
volume management products.</para>
<para>For more information about ZFS resilvering, see <olink targetdoc="" remap="internal" targetptr="gbcus">Viewing Resilvering Status</olink>.<indexterm xml:id="indexterm-31">
<primary>ZFS storage pools</primary>
<secondary>resilvering</secondary>
<tertiary>definition</tertiary>
</indexterm>
<indexterm xml:id="indexterm-32">
<primary>resilvering</primary>
<secondary>definition</secondary>
</indexterm>
<indexterm xml:id="indexterm-33">
<primary>terminology</primary>
<secondary>resilvering</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
<varlistentry>
<term>snapshot</term>
<listitem>
<para>A read-only image of a file system or volume at a given point
in time.</para>
<para>For more information about snapshots, see <olink targetdoc="" remap="internal" targetptr="gbciq">ZFS Snapshots</olink>.<indexterm xml:id="indexterm-34">
<primary>ZFS file systems</primary>
<secondary>snapshot</secondary>
<tertiary>definition</tertiary>
</indexterm>
<indexterm xml:id="indexterm-35">
<primary>snapshot</primary>
<secondary>definition</secondary>
</indexterm>
<indexterm xml:id="indexterm-36">
<primary>terminology</primary>
<secondary>snapshot</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
<varlistentry>
<term>virtual device</term>
<listitem>
<para>A logical device in a pool, which can be a physical device,
a file, or a collection of devices.</para>
<para>For more information about virtual devices, see <olink targetdoc="" remap="internal" targetptr="gazca">Virtual Devices in a Storage Pool</olink>.<indexterm xml:id="indexterm-37">
<primary>ZFS storage pools</primary>
<secondary>virtual device</secondary>
<tertiary>definition</tertiary>
</indexterm>
<indexterm xml:id="indexterm-38">
<primary>virtual device</primary>
<secondary>definition</secondary>
</indexterm>
<indexterm xml:id="indexterm-39">
<primary>terminology</primary>
<secondary>virtual device</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
<varlistentry>
<term>volume</term>
<listitem>
<para>A dataset used to emulate a physical device in order to support
legacy file systems.</para>
<para>For more information about emulated volumes, see <olink targetdoc="" remap="internal" targetptr="gaypf">Emulated Volumes</olink>.<indexterm xml:id="indexterm-40">
<primary>ZFS file systems</primary>
<secondary>volume</secondary>
<tertiary>definition</tertiary>
</indexterm>
<indexterm xml:id="indexterm-41">
<primary>volume</primary>
<secondary>definition</secondary>
</indexterm>
<indexterm xml:id="indexterm-42">
<primary>terminology</primary>
<secondary>volume</secondary>
</indexterm>
</para>
</listitem>
</varlistentry>
</variablelist>
</sect1>
<sect1 xml:id="gbcpt">
<title>ZFS Component Naming Requirements</title>
<para>Each ZFS component must be named according to the following rules:<indexterm xml:id="indexterm-43">
<primary>ZFS file systems</primary>
<secondary>component naming requirements</secondary>
</indexterm>
<indexterm xml:id="indexterm-44">
<primary>components of ZFS</primary>
<secondary>naming requirements</secondary>
</indexterm>
<indexterm xml:id="indexterm-45">
<primary>naming requirements</primary>
<secondary>ZFS components</secondary>
</indexterm>
</para>
<itemizedlist>
<listitem>
<para>Empty components are not allowed.</para>
</listitem>
<listitem>
<para>Each component can only contain alphanumeric characters in
addition to the following four special characters:</para>
<itemizedlist>
<listitem>
<para>Underscore (_)</para>
</listitem>
<listitem>
<para>Hyphen (-)</para>
</listitem>
<listitem>
<para>Colon (:)</para>
</listitem>
<listitem>
<para>Period (.)</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Pool names must begin with a letter, except that the beginning
sequence c[0-9] is not allowed. In addition, pool names that begin with <literal>mirror</literal>, <literal>raidz</literal>, or <literal>spare</literal> are
not allowed as these name are reserved.</para>
</listitem>
<listitem>
<para>Dataset names must begin with an alphanumeric character.</para>
</listitem>
</itemizedlist>
</sect1>
</chapter>
