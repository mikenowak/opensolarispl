<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML//EN" "docbook.dtd"[
	<!ENTITY % xinclude SYSTEM "xinclude.mod">
	%xinclude;
]>

<chapter xmlns="http://docbook.org/ns/docbook" version="5.0" xml:id="gavwn">



<title>Managing ZFS Storage Pools</title>
<toc>
<para>This chapter describes how to create and administer ZFS storage pools.</para>
<para>The following sections are provided in this chapter:</para>
<itemizedlist>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gcfog">Components of a ZFS Storage Pool</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gaypw">Creating and Destroying ZFS Storage Pools</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gayrd">Managing Devices in ZFS Storage Pools</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gaynp">Querying ZFS Storage Pool Status</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gbchy">Migrating ZFS Storage Pools</olink>
</para>
</listitem>
<listitem>
<para>
<olink targetdoc="" remap="internal" targetptr="gcikw">Upgrading ZFS Storage Pools</olink>
</para>
</listitem>
</itemizedlist>
</toc>
<sect1 xml:id="gcfog">
<title>Components of a ZFS Storage Pool</title>
<para>This section provides detailed information about the following storage
pool components:</para>
<itemizedlist>
<listitem>
<para>Disks</para>
</listitem>
<listitem>
<para>Files</para>
</listitem>
<listitem>
<para>Virtual devices</para>
</listitem>
</itemizedlist>
<sect2 xml:id="gazdp">
<title>Using Disks in a ZFS Storage Pool</title>
<para>The most basic element of a storage pool is a piece of physical storage.
Physical storage can be any block device of at least 128 Mbytes in size. Typically,
this device is a hard drive that is visible to the system in the <filename>/dev/dsk</filename> directory.<indexterm xml:id="indexterm-81">
<primary>ZFS storage pools</primary>
<secondary>components</secondary>
</indexterm>
<indexterm xml:id="indexterm-82">
<primary>components of</primary>
<secondary>ZFS storage pool</secondary>
</indexterm>
</para>
<para>A storage device can be a whole disk (<filename>c1t0d0</filename>) or
an individual slice (<filename>c0t0d0s7</filename>). The recommended mode
of operation is to use an entire disk, in which case the disk does not need
to be specially formatted. ZFS formats the disk using an EFI label to contain
a single, large slice. When used in this way, the partition table that is
displayed by the <command>format</command> command appears similar to the
following:</para>
<screen>Current partition table (original):
Total disk sectors available: 71670953 + 16384 (reserved sectors)

Part      Tag    Flag     First Sector        Size        Last Sector
  0        usr    wm                34      34.18GB         71670953    
  1 unassigned    wm                 0          0              0    
  2 unassigned    wm                 0          0              0    
  3 unassigned    wm                 0          0              0    
  4 unassigned    wm                 0          0              0    
  5 unassigned    wm                 0          0              0    
  6 unassigned    wm                 0          0              0    
  7 unassigned    wm                 0          0              0    
  8   reserved    wm          71670954       8.00MB         71687337</screen>
<para>To use whole disks, the disks must be named using the standard Solaris
convention, such as <filename>/dev/dsk/cXtXdXsX</filename>. Some third-party
drivers use a different naming convention or place disks in a location other
than the <filename>/dev/dsk</filename> directory. To use these disks, you
must manually label the disk and provide a slice to ZFS. <indexterm xml:id="indexterm-83">
<primary>EFI label</primary>
<secondary>description</secondary>
</indexterm>
<indexterm xml:id="indexterm-84">
<primary>EFI label</primary>
<secondary>interaction with ZFS</secondary>
</indexterm>
</para>
<para>ZFS applies an EFI label when you create a storage pool with whole disks.
Disks can be labeled with a traditional Solaris VTOC label when you create
a storage pool with a disk slice.</para>
<para>Slices should only be used under the following conditions:</para>
<itemizedlist>
<listitem>
<para>The device name is nonstandard.</para>
</listitem>
<listitem>
<para>A single disk is shared between ZFS and another file system,
such as UFS.</para>
</listitem>
<listitem>
<para>A disk is used as a swap or a dump device.</para>
</listitem>
</itemizedlist>
<para>Disks can be specified by using either the full path, such as <filename>/dev/dsk/c1t0d0</filename>, or a shorthand name that consists of the device name within the <filename>/dev/dsk</filename> directory, such as <filename>c1t0d0</filename>. For example,
the following are valid disk names:<indexterm xml:id="indexterm-85">
<primary>ZFS storage pools</primary>
<secondary>using whole disks</secondary>
</indexterm>
<indexterm xml:id="indexterm-86">
<primary>whole disks</primary>
<secondary>as components of ZFS storage pools</secondary>
</indexterm>
<indexterm xml:id="indexterm-87">
<primary>disks</primary>
<secondary>as components of ZFS storage pools</secondary>
</indexterm>
</para>
<itemizedlist>
<listitem>
<para>
<filename>c1t0d0</filename>
</para>
</listitem>
<listitem>
<para>
<filename>/dev/dsk/c1t0d0</filename>
</para>
</listitem>
<listitem>
<para>
<filename>c0t0d6s2</filename>
</para>
</listitem>
<listitem>
<para>
<filename>/dev/foo/disk</filename>
</para>
</listitem>
</itemizedlist>
<para>ZFS works best when given whole physical disks. Although constructing
logical devices using a volume manager, such as Solaris Volume Manager (SVM),
Veritas Volume Manager (VxVM), or a hardware volume manager (LUNs or hardware
RAID) is possible, these configurations are not recommended. While ZFS functions
properly on such devices, less-than-optimal performance might be the result.</para>
<para>Disks are identified both by their path and by their device ID, if available.
This method allows devices to be reconfigured on a system without having to
update any ZFS state. If a disk is switched between controller 1 and controller
2, ZFS uses the device ID to detect that the disk has moved and should now
be accessed using controller 2. The device ID is unique to the drive's firmware.
While unlikely, some firmware updates have been known to change device IDs.
If this situation happens, ZFS can still access the device by path and update
the stored device ID automatically. If you inadvertently change both the path
and the ID of the device, then export and re-import the pool in order to use
it.</para>
</sect2>
<sect2 xml:id="gazcr">
<title>Using Files in a ZFS Storage Pool</title>
<para>ZFS also allows you to use UFS files as virtual devices in your storage
pool. This feature is aimed primarily at testing and enabling simple experimentation,
not for production use. The reason is that <emphasis role="strong">any use
of files relies on the underlying file system for consistency</emphasis>.
If you create a ZFS pool backed by files on a UFS file system, then you are
implicitly relying on UFS to guarantee correctness and synchronous semantics.<indexterm xml:id="indexterm-88">
<primary>ZFS storage pools</primary>
<secondary>using files</secondary>
</indexterm>
<indexterm xml:id="indexterm-89">
<primary>files</primary>
<secondary>as components of ZFS storage pools</secondary>
</indexterm>
</para>
<para>However, files can be quite useful when you are first trying out ZFS
or experimenting with more complicated layouts when not enough physical devices
are present. All files must be specified as complete paths and must be at
least 128 Mbytes in size. If a file is moved or renamed, the pool must be
exported and re-imported in order to use it, as no device ID is associated
with files by which they can be located.</para>
</sect2>
<sect2 xml:id="gazca">
<title>Virtual Devices in a Storage Pool</title>
<para>Each storage pool is comprised of one or more virtual devices. A <emphasis>virtual device</emphasis> is an internal representation of the storage pool
that describes the layout of physical storage and its fault characteristics.
As such, a virtual device represents the disk devices or files that are used
to create the storage pool. <indexterm xml:id="indexterm-90">
<primary>ZFS storage pools</primary>
<secondary>virtual devices</secondary>
</indexterm>
<indexterm xml:id="indexterm-91">
<primary>virtual devices</primary>
<secondary>as components of ZFS storage pools</secondary>
</indexterm>
</para>
<para>Two top-level virtual devices provide data redundancy: mirror and RAID-Z
virtual devices. These virtual devices consist of disks, disk slices, or files.</para>
<para>Disks, disk slices, or files that are used in pools outside of mirrors
and RAID-Z virtual devices, function as top-level virtual devices themselves.</para>
<para>Storage pools typically contain multiple top-level virtual devices.
ZFS dynamically stripes data among all of the top-level virtual devices in
a pool.</para>
</sect2>
</sect1>
<sect1 xml:id="gcfof">
<title>Replication Features of a ZFS Storage Pool</title>
<para>ZFS provides two levels of data redundancy in a mirrored and a RAID-Z
configuration.<indexterm xml:id="indexterm-92">
<primary>replication features of ZFS</primary>
<secondary>mirrored or RAID-Z</secondary>
</indexterm>
</para>
<sect2 xml:id="gamss">
<title>Mirrored Storage Pool Configuration</title>
<para>A mirrored storage pool configuration requires at least two disks, preferrably
on separate controllers. Many disks can be used in a mirrored configuration.
In addition, you can create more than one mirror in each pool. Conceptually,
a simple mirrored configuration would look similar to the following:<indexterm xml:id="indexterm-93">
<primary>mirrored configuration</primary>
<secondary>description</secondary>
</indexterm>
<indexterm xml:id="indexterm-94">
<primary>mirrored configuration</primary>
<secondary>replication feature</secondary>
</indexterm>
<indexterm xml:id="indexterm-95">
<primary>mirrored configuration</primary>
<secondary>conceptual view</secondary>
</indexterm>
<indexterm xml:id="indexterm-96">
<primary>ZFS storage pools</primary>
<secondary>mirrored configuration, description of</secondary>
</indexterm>
</para>
<screen>mirror c1t0d0 c2t0d0</screen>
<para>Conceptually, a more complex mirrored configuration would look similar
to the following:</para>
<screen>mirror c1t0d0 c2t0d0 c3t0d0 mirror c4t0d0 c5t0d0 c6t0d0</screen>
<para>For information about creating a mirrored storage pool, see <olink targetdoc="" remap="internal" targetptr="gazhv">Creating a Mirrored Storage Pool</olink>.</para>
</sect2>
<sect2 xml:id="gamtu">
<title>RAID-Z Storage Pool Configuration</title>
<para>In addition to a mirrored storage pool configuration, ZFS provides a
RAID-Z configuration. RAID-Z is similar to RAID-5.<indexterm xml:id="indexterm-97">
<primary>RAID-Z configuration</primary>
<secondary>description</secondary>
</indexterm>
<indexterm xml:id="indexterm-98">
<primary>RAID-Z configuration</primary>
<secondary>replication feature</secondary>
</indexterm>
<indexterm xml:id="indexterm-99">
<primary>RAID-Z configuration</primary>
<secondary>conceptual view</secondary>
</indexterm>
<indexterm xml:id="indexterm-100">
<primary>ZFS storage pools</primary>
<secondary>RAID-Z configuration, description of</secondary>
</indexterm>
</para>
<para>All traditional RAID-5-like algorithms (RAID-4. RAID-5. RAID-6, RDP,
and EVEN-ODD, for example) suffer from a problem known as the “RAID-5
write hole.” If only part of a RAID-5 stripe is written, and power is
lost before all blocks have made it to disk, the parity will remain out of
sync with the data, and therefore useless, forever (unless a subsequent full-stripe
write overwrites it). In RAID-Z, ZFS uses variable-width RAID stripes so that
all writes are full-stripe writes. This design is only possible because ZFS
integrates file system and device management in such a way that the file system's
metadata has enough information about the underlying data replication model
to handle variable-width RAID stripes. RAID-Z is the world's first software-only
solution to the RAID-5 write hole.</para>
<para>You need at least two disks for a RAID-Z configuration. Otherwise, no
special hardware is required to create a RAID-Z configuration. Currently,
RAID-Z provides single parity. For example, if you have three disks in a RAID-Z
configuration, parity data occupies space equal to one of the three disks.</para>
<para>Conceptually, RAID-Z configuration with three disks would look similar
to the following:</para>
<screen>raidz c1t0d0 c2t0d0 c3t0d0</screen>
<para>A more complex conceptual RAID-Z configuration would look similar to
the following:</para>
<screen>raidz c1t0d0 c2t0d0 c3t0d0 c4t0d0 c5t0d0 c6t0d0 c7t0d0 raidz c8t0d0 c9t0d0 c10t0d0 c11t0d0 c12t0d0 c13t0d0 c14t0d0</screen>
<para>If you are creating a RAID-Z configuration with many disks, as in this
example, a RAID-Z configuration with 14 disks is better split into a two 7-disk
groupings. RAID-Z configurations with single-digit groupings of disks should
perform better.</para>
<para>For information about creating a RAID-Z storage pool, see <olink targetdoc="" remap="internal" targetptr="gazhe">Creating a RAID-Z Storage Pool</olink>.</para>
</sect2>
<sect2 xml:id="gazch">
<title>Self-Healing Data in a Replicated Configuration</title>
<para>ZFS provides for self-healing data in a mirrored or RAID-Z configuration.</para>
<para>When a bad data block is detected, not only does ZFS fetch the correct
data from another replicated copy, but it also repairs the bad data by replacing
it with the good copy.<indexterm xml:id="indexterm-101">
<primary>self-healing data</primary>
<secondary>description</secondary>
</indexterm>
</para>
</sect2>
<sect2 xml:id="gazdd">
<title>Dynamic Striping in a Storage Pool</title>
<para>For each virtual device that is added to the pool, ZFS dynamically stripes
data across all available devices. The decision about where to place data
is done at write time, so no fixed width stripes are created at allocation
time.<indexterm xml:id="indexterm-102">
<primary>dynamic striping</primary>
<secondary>description</secondary>
</indexterm>
<indexterm xml:id="indexterm-103">
<primary>dynamic striping</primary>
<secondary>storage pool feature</secondary>
</indexterm>
<indexterm xml:id="indexterm-104">
<primary>ZFS storage pools</primary>
<secondary>dynamic striping</secondary>
</indexterm>
</para>
<para>When virtual devices are added to a pool, ZFS gradually allocates data
to the new device in order to maintain performance and space allocation policies.
Each virtual device can also be a mirror or a RAID-Z device that contains
other disk devices or files. This configuration allows for flexibility in
controlling the fault characteristics of your pool. For example, you could
create the following configurations out of 4 disks:</para>
<itemizedlist>
<listitem>
<para>Four disks using dynamic striping</para>
</listitem>
<listitem>
<para>One four-way RAID-Z configuration</para>
</listitem>
<listitem>
<para>Two two-way mirrors using dynamic striping</para>
</listitem>
</itemizedlist>
<para>While ZFS supports combining different types of virtual devices within
the same pool, this practice is not recommended. For example, you can create
a pool with a two-way mirror and a three-way RAID-Z configuration. However,
your fault tolerance is as good as your worst virtual device, RAID-Z in this
case. The recommended practice is to use top-level virtual devices of the
same type with the same replication level in each device.</para>
</sect2>
</sect1>
<sect1 xml:id="gaypw">
<title>Creating and Destroying ZFS Storage Pools</title>
<para>By design, creating and destroying pools is fast and easy. However,
be cautious when doing these operations. Although checks are performed to
prevent using devices known to be in use in a new pool, ZFS cannot always
know when a device is already in use. Destroying a pool is even easier. Use <command>zpool destroy</command> with caution. This is a simple command with significant
consequences. For information about destroy pools, see <olink targetdoc="" remap="internal" targetptr="gammr">Destroying ZFS Storage Pools</olink>.<indexterm xml:id="indexterm-105">
<primary>creating</primary>
<secondary>ZFS storage pool</secondary>
<tertiary>description</tertiary>
</indexterm>
<indexterm xml:id="indexterm-106">
<primary>destroying</primary>
<secondary>ZFS storage pool</secondary>
<tertiary>description</tertiary>
</indexterm>
</para>
<sect2 xml:id="gaynr">
<title>Creating a ZFS Storage Pool</title>
<para>To create a storage pool, use the <command>zpool create</command> command.
This command takes a pool name and any number of virtual devices as arguments.
The pool name must satisfy the naming conventions outlined in <olink targetdoc="" remap="internal" targetptr="gbcpt">ZFS Component Naming Requirements</olink>.<indexterm xml:id="indexterm-107">
<primary>creating</primary>
<secondary>ZFS storage pool (<command>zpool create</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-108">
<primary>
<command>zpool create</command>
</primary>
<secondary>basic pool</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-109">
<primary>ZFS storage pools</primary>
<secondary>creating (<command>zpool create</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<sect3 xml:id="gazgt">
<title>Creating a Basic Storage Pool</title>
<para>The following command creates a new pool named <filename>tank</filename> that
consists of the disks <filename>c1t0d0</filename> and <filename>c1t1d0</filename>:</para>
<screen># <userinput>zpool create tank c1t0d0 c1t1d0</userinput>
</screen>
<para>These whole disks are found in the <filename>/dev/dsk</filename> directory
and are labelled appropriately by ZFS to contain a single, large slice. Data
is dynamically striped across both disks.</para>
</sect3>
<sect3 xml:id="gazhv">
<title>Creating a Mirrored Storage Pool</title>
<para>To create a mirrored pool, use the <literal>mirror</literal> keyword,
followed by any number of storage devices that will comprise the mirror. Multiple
mirrors can be specified by repeating the <literal>mirror</literal> keyword
on the command line. The following command creates a pool with two, two-way
mirrors:</para>
<screen># <userinput>zpool create tank mirror c1d0 c2d0 mirror c3d0 c4d0</userinput>
</screen>
<para>The second <literal>mirror</literal> keyword indicates that a new top-level
virtual device is being specified. Data is dynamically striped across both
mirrors, with data being replicated between each disk appropriately.<indexterm xml:id="indexterm-110">
<primary>creating</primary>
<secondary>mirrored ZFS storage pool (<command>zpool create</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-111">
<primary>
<command>zpool create</command>
</primary>
<secondary>mirrored storage pool</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-112">
<primary>mirrored storage pool (<command>zpool create</command>)</primary>
<secondary>(example of)</secondary>
</indexterm>
<indexterm xml:id="indexterm-113">
<primary>ZFS storage pools</primary>
<secondary>creating mirrored configuration (<command>zpool create</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
</sect3>
<sect3 xml:id="gazhe">
<title>Creating a RAID-Z Storage Pool</title>
<para>Creating a RAID-Z pool is identical to creating a mirrored pool, except
that the <literal>raidz</literal> keyword is used instead of <literal>mirror</literal>.
The following example shows how to create a pool with a single RAID-Z device
that consists of five disks:<indexterm xml:id="indexterm-114">
<primary>creating</primary>
<secondary>RAID-Z storage pool (<command>zpool create</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-115">
<primary>RAID-Z configuration</primary>
<secondary>(example of)</secondary>
</indexterm>
<indexterm xml:id="indexterm-116">
<primary>
<command>zpool create</command>
</primary>
<secondary>RAID-Z storage pool</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-117">
<primary>ZFS storage pools</primary>
<secondary>creating a RAID-Z configuration (<command>zpool create</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool create tank raidz c1t0d0 c2t0d0 c3t0d0 c4t0d0 /dev/dsk/c5t0d0</userinput>
</screen>
<para>This example demonstrates that disks can be specified by using their
full paths. The <filename>/dev/dsk/c5t0d0</filename> device is identical to
the <filename>c5t0d0</filename> device.</para>
<para>A similar configuration could be created with disk slices. For example:</para>
<screen># <userinput>zpool create tank raidz c1t0d0s0 c2t0d0s0 c3t0d0s0 c4t0d0s0 c5t0d0s0</userinput>
</screen>
<para>However, the disks must be preformatted to have an appropriately sized
slice zero.</para>
<para>For more information about a RAID-Z configuration, see <olink targetdoc="" remap="internal" targetptr="gamtu">RAID-Z Storage Pool Configuration</olink>.</para>
</sect3>
</sect2>
<sect2 xml:id="gazhs">
<title>Handling ZFS Storage Pool Creation Errors</title>
<para>Pool creation errors can occur for many reasons. Some of these reasons
are obvious, such as when a specified device doesn't exist, while other reasons
are more subtle.</para>
<sect3 xml:id="gazht">
<title>Detecting in Use Devices</title>
<para>Before formatting a device, ZFS first determines if the disk is in use
by ZFS or some other part of the operating system. If the disk is in use,
you might see errors such as the following:</para>
<screen># <userinput>zpool create tank c1t0d0 c1t1d0</userinput>
invalid vdev specification
use '-f' to override the following errors:
/dev/dsk/c1t0d0s0 is currently mounted on /
/dev/dsk/c1t0d0s1 is currently mounted on swap
/dev/dsk/c1t1d0s0 is part of active ZFS pool 'zeepool'
Please see zpool(1M)</screen>
<para>Some of these errors can be overridden by using the <option>
f</option> option,
but most errors cannot. The following uses cannot be overridden by using the <option>
f</option> option, and you must manually correct them:<indexterm xml:id="indexterm-118">
<primary>detecting</primary>
<secondary>in-use devices</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-119">
<primary>in-use devices</primary>
<secondary>detecting</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<variablelist>
<varlistentry>
<term>
<emphasis role="strong">Mounted file system</emphasis>
</term>
<listitem>
<para>The disk or one of its slices contains a file system that
is currently mounted. To correct this error, use the <command>umount</command> command.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong">File system in /etc/vfstab</emphasis>
</term>
<listitem>
<para>The disk contains a file system that is listed in the <filename>/etc/vfstab</filename> file, but the file system is not currently mounted. To correct
this error, remove or comment out the line in the <filename>/etc/vfstab</filename> file.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong">Dedicated dump device</emphasis>
</term>
<listitem>
<para>The disk is in use as the dedicated dump device for the system.
To correct this error, use the <command>dumpadm</command> command.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong">Part of a ZFS pool</emphasis>
</term>
<listitem>
<para>The disk or file is part of an active ZFS storage pool. To
correct this error, use the <command>zpool</command> command to destroy the
pool.</para>
</listitem>
</varlistentry>
</variablelist>
<para>The following in-use checks serve as helpful warnings and can be overridden
by using the <option>
f</option> option to create the pool:</para>
<variablelist>
<varlistentry>
<term>
<emphasis role="strong">Contains a file system</emphasis>
</term>
<listitem>
<para>The disk contains a known file system, though it is not mounted
and doesn't appear to be in use.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong">Part of volume</emphasis>
</term>
<listitem>
<para>The disk is part of an SVM volume.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong">Live upgrade</emphasis>
</term>
<listitem>
<para>The disk is in use as an alternate boot environment for Solaris
Live Upgrade.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<emphasis role="strong">Part of exported ZFS pool</emphasis>
</term>
<listitem>
<para>The disk is part of a storage pool that has been exported
or manually removed from a system. In the latter case, the pool is reported
as <literal>potentially active</literal>, as the disk might or might not be
a network-attached drive in use by another system. Be cautious when overriding
a potentially active pool.</para>
</listitem>
</varlistentry>
</variablelist>
<para>The following example demonstrates how the <option>
f</option> option
is used:</para>
<screen># <userinput>zpool create tank c1t0d0</userinput>
invalid vdev specification
use '-f' to override the following errors:
/dev/dsk/c1t0d0s0 contains a ufs filesystem
# <userinput>zpool create -f tank c1t0d0</userinput>
</screen>
<para>Ideally, correct the errors rather than use the <option>
f</option> option.</para>
</sect3>
<sect3 xml:id="gazgc">
<title>Mismatched Replication Levels</title>
<para>Creating pools with virtual devices of different replication levels
is not recommended. The <command>zpool</command> command tries to prevent
you from accidentally creating a pool with mismatched replication levels.
If you try to create a pool with such a configuration, you see errors similar
to the following:<indexterm xml:id="indexterm-120">
<primary>detecting</primary>
<secondary>mismatched replication levels</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-121">
<primary>mismatched replication levels</primary>
<secondary>detecting</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool create tank c1t0d0 mirror c2t0d0 c3t0d0</userinput>
invalid vdev specification
use '-f' to override the following errors:
mismatched replication level: both disk and mirror vdevs are present
# <userinput>zpool create tank mirror c1t0d0 c2t0d0 mirror c3t0d0 c4t0d0 c5t0d0</userinput>
invalid vdev specification
use '-f' to override the following errors:
mismatched replication level: 2-way mirror and 3-way mirror vdevs are present</screen>
<para>You can override these errors with the <option>
f</option> option, though
this practice is not recommended. The command also warns you about creating
a mirrored or RAID-Z pool using devices of different sizes. While this configuration
is allowed, mismatched replication levels result in unused space on the larger
device, and requires the <option>
f</option> option to override the warning.</para>
</sect3>
<sect3 xml:id="gazhd">
<title>Doing a Dry Run of Storage Pool Creation</title>
<para>Because creating a pool can fail unexpectedly in different ways, and
because formatting disks is such a potentially harmful action, the <command>zfs
create</command> command has an additional option, <option>
n</option>, which
simulates creating the pool without actually writing data to disk. This option
performs the device in-use checking and replication level validation, and
reports any errors in the process. If no errors are found, you see output
similar to the following:<indexterm xml:id="indexterm-122">
<primary>dry run</primary>
<secondary>ZFS storage pool creation (<command>zpool create</command> <option>
n</option>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-123">
<primary>
<command>zpool create</command> <option>
n</option>
</primary>
<secondary>dry run</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-124">
<primary>ZFS storage pools</primary>
<secondary>doing a dry run (<command>zpool create</command> <option>
n</option>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool create -n tank mirror c1t0d0 c1t1d0</userinput>
would create 'tank' with the following layout:

        tank
          mirror
            c1t0d0
            c1t1d0</screen>
<para>Some errors cannot be detected without actually creating the pool. The
most common example is specifying the same device twice in the same configuration.
This error cannot be reliably detected without writing the data itself, so
the <command>create -n</command> command can report success and yet fail to
create the pool when run for real.</para>
</sect3>
<sect3 xml:id="gbeef">
<title>Default Mount Point for Storage Pools</title>
<para>When a pool is created, the default mount point for the root dataset
is <replaceable>/pool-name</replaceable> by default. This directory must either
not exist or be empty. If the directory does not exist, it is automatically
created. If the directory is empty, the root dataset is mounted on top of
the existing directory. To create a pool with a different default mount point,
use the <option>
m</option> option of the <command>zpool create</command> command:<indexterm xml:id="indexterm-125">
<primary>mountpoint</primary>
<secondary>default for ZFS storage pools</secondary>
</indexterm>
<indexterm xml:id="indexterm-126">
<primary>ZFS storage pools</primary>
<secondary>default mountpoint</secondary>
</indexterm>
</para>
<screen># <userinput>zpool create home c1t0d0</userinput>
default mountpoint '/home' exists and is not empty
use '-m' option to specify a different default
# <userinput>zpool create -m /export/zfs home c1t0d0</userinput>
</screen>
<para>This command creates a new pool <literal>home</literal> and the <literal>home</literal> dataset with a mount point of <filename>/export/zfs</filename>.</para>
<para>For more information about mount points, see <olink targetdoc="" remap="internal" targetptr="gaztn">Managing ZFS Mount Points</olink>.</para>
</sect3>
</sect2>
<sect2 xml:id="gammr">
<title>Destroying ZFS Storage Pools</title>
<para>Pools are destroyed by using the <command>zpool destroy</command> command.
This command destroys the pool even if it contains mounted datasets.<indexterm xml:id="indexterm-127">
<primary>destroying</primary>
<secondary>ZFS storage pool (<command>zpool destroy</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-128">
<primary>
<command>zpool destroy</command>
</primary>
<secondary>(example of)</secondary>
</indexterm>
<indexterm xml:id="indexterm-129">
<primary>ZFS storage pools</primary>
<secondary>destroying (<command>zpool destroy</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool destroy tank</userinput>
</screen>
<caution>

<para>Be very careful when you destroy a pool. Make sure you are
destroying the right pool and you always have copies of your data. If you
accidentally destroy the wrong pool, you can attempt to recover the pool.
For more information, see <olink targetdoc="" remap="internal" targetptr="gcfhw">Recovering Destroyed ZFS Storage Pools</olink>.</para>
</caution>
<sect3 xml:id="gazhm">
<title>Destroying a Pool With Faulted Devices</title>
<para>The act of destroying a pool requires that data be written to disk to
indicate that the pool is no longer valid. This state information prevents
the devices from showing up as a potential pool when you perform an import.
If one or more devices are unavailable, the pool can still be destroyed. However,
the necessary state information won't be written to these damaged devices.</para>
<para>These devices, when suitably repaired, are reported as <emphasis>potentially
active</emphasis> when you create a new pool, and appear as valid devices
when you search for pools to import. If a pool has enough faulted devices
such that the pool itself is faulted (meaning that a top-level virtual device
is faulted), then the command prints a warning and cannot complete without
the <option>
f</option> option. This option is necessary because the pool cannot
be opened, so whether data is stored there or not is unknown. For example:</para>
<screen># <userinput>zpool destroy tank</userinput>
cannot destroy 'tank': pool is faulted
use '-f' to force destruction anyway
# <userinput>zpool destroy -f tank</userinput>
</screen>
<para>For more information about pool and device health, see <olink targetdoc="" remap="internal" targetptr="gamno">Health Status of ZFS Storage Pools</olink>.</para>
<para>For more information about importing pools, see <olink targetdoc="" remap="internal" targetptr="gazuf">Importing ZFS Storage Pools</olink>.</para>
</sect3>
</sect2>
</sect1>
<sect1 xml:id="gayrd">
<title>Managing Devices in ZFS Storage Pools</title>
<para>Most of the basic information regarding devices is covered in <olink targetdoc="" remap="internal" targetptr="gcfog">Components of a ZFS Storage Pool</olink>. Once a pool has
been created, you can perform several tasks to manage the physical devices
within the pool.</para>
<sect2 xml:id="gazgw">
<title>Adding Devices to a Storage Pool</title>
<para>You can dynamically add space to a pool by adding a new top-level virtual
device. This space is immediately available to all datasets within the pool.
To add a new virtual device to a pool, use the <command>zpool add</command> command.
For example:<indexterm xml:id="indexterm-130">
<primary>adding</primary>
<secondary>devices to ZFS storage pool (<command>zpool add</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-131">
<primary>
<command>zpool add</command>
</primary>
<secondary>(example of)</secondary>
</indexterm>
<indexterm xml:id="indexterm-132">
<primary>ZFS storage pools</primary>
<secondary>adding devices to (<command>zpool add</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool add zeepool mirror c2t1d0 c2t2d0</userinput>
</screen>
<para>The format of the virtual devices is the same as for the <command>zpool
create</command> command, and the same rules apply. Devices are checked to
determine if they are in use, and the command cannot change the replication
level without the <option>
f</option> option. The command also supports the <option>
n</option> option so that you can perform a dry run. For example:</para>
<screen># <userinput>zpool add -n zeepool mirror c3t1d0 c3t2d0</userinput>
would update 'zeepool' to the following configuration:
      zeepool
        mirror
            c1t0d0
            c1t1d0
        mirror
            c2t1d0
            c2t2d0
        mirror
            c3t1d0
            c3t2d0</screen>
<para>This command syntax would add mirrored devices <literal>c3t1d0</literal> and <literal>c3t2d0</literal> to <filename>zeepool</filename>'s existing configuration.</para>
<para>For more information about how virtual device validation is done, see <olink targetdoc="" remap="internal" targetptr="gazht">Detecting in Use Devices</olink>.</para>
</sect2>
<sect2 xml:id="gcfhe">
<title>Attaching and Detaching Devices in a Storage Pool</title>
<para>In addition to the <command>zpool add</command> command, you can use
the <command>zpool attach</command> command to add a new device to an existing
mirrored or non-mirrored device. For example:<indexterm xml:id="indexterm-133">
<primary>attaching</primary>
<secondary>devices to ZFS storage pool (<command>zpool attach</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-134">
<primary>
<command>zpool attach</command>
</primary>
<secondary>(example of)</secondary>
</indexterm>
<indexterm xml:id="indexterm-135">
<primary>ZFS storage pools</primary>
<secondary>attaching devices to (<command>zpool attach</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool attach zeepool c1t1d0 c2t1d0</userinput>
</screen>
<para>If the existing device is part of a two-way mirror, attaching the new
device, creates a three-way mirror, and so on. In either case, the new device
begins to resilver  immediately.</para>
<para>In is example, <literal>zeepool</literal> is an existing two-way mirror
that is transformed to a three-way mirror by attaching <literal>c2t1d0</literal>,
the new device, to the existing device, <literal>c1t1d0</literal>.</para>
<para>You can use the <command>zpool detach</command> command to detach a
device from a pool. For example:</para>
<screen># <userinput>zpool detach zeepool c2t1d0</userinput>
</screen>
<para>However, this operation is refused if  there are no other valid replicas
of the data. For example:<indexterm xml:id="indexterm-136">
<primary>detaching</primary>
<secondary>devices to ZFS storage pool (<command>zpool detach</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-137">
<primary>
<command>zpool detach</command>
</primary>
<secondary>(example of)</secondary>
</indexterm>
<indexterm xml:id="indexterm-138">
<primary>ZFS storage pools</primary>
<secondary>detaching devices from (<command>zpool detach</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool detach newpool c1t2d0</userinput>
cannot detach c1t2d0: only applicable to mirror and replacing vdevs</screen>
</sect2>
<sect2 xml:id="gazgm">
<title>Onlining and Offlining Devices in a Storage Pool</title>
<para>ZFS allows individual devices to be taken offline or brought online.
When hardware is unreliable or not functioning properly, ZFS continues to
read or write data to the device, assuming the condition is only temporary.
If the condition is not temporary, it is possible to instruct ZFS to ignore
the device by bringing it offline. ZFS does not send any requests to an offlined
device.<indexterm xml:id="indexterm-139">
<primary>onlining and offlining devices</primary>
<secondary>ZFS storage pool</secondary>
<tertiary>description</tertiary>
</indexterm>
<indexterm xml:id="indexterm-140">
<primary>ZFS storage pools</primary>
<secondary>onlining and offlining devices</secondary>
<tertiary>description</tertiary>
</indexterm>
</para>
<note>

<para>Devices do not need to be taken offline in order to replace them.</para>
</note>
<para>You can use the <command>offline</command> command when you need to
temporarily disconnect storage. For example, if you need to physically disconnect
an array from one set of Fibre Channel switches and connect the array to a
different set, you could take the LUNs offline from the array that was used
in ZFS storage pools. After the array was reconnected and operational on the
new set of switches, you could then bring the same LUNs online. Data that
had been added to the storage pools while the LUNs were offline would resilver
to the LUNs after they were brought back online.</para>
<para>This scenario is possible assuming that the systems in question see
the storage once it is attached to the new switches, possibly through different
controllers than before, and your pools are set up as RAID-Z or mirrored configurations.</para>
<sect3 xml:id="gazfy">
<title>Taking a Device Offline</title>
<para>You can take a device offline by using the <command>zpool offline</command> command.
The device can be specified by path or by short name, if the device is a disk.
For example:</para>
<screen># <userinput>zpool offline tank c1t0d0</userinput>
bringing device c1t0d0 offline</screen>
<para>You cannot take a pool offline to the point where it becomes faulted.
For example, you cannot take offline two devices out of a RAID-Z configuration,
nor can you take offline a top-level virtual device.<indexterm xml:id="indexterm-141">
<primary>offlining a device (<command>zpool offline</command>)</primary>
<secondary>ZFS storage pool</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-142">
<primary>
<command>zpool offline</command>
</primary>
<secondary>(example of)</secondary>
</indexterm>
<indexterm xml:id="indexterm-143">
<primary>ZFS storage pools</primary>
<secondary>offlining a device (<command>zpool offline</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool offline tank c1t0d0</userinput>
cannot offline c1t0d0: no valid replicas</screen>
<note>

<para>Currently, you cannot replace a device that has been taken offline.</para>
</note>
<para>Offlined devices show up in the <literal>OFFLINE</literal> state
when you query pool status. For information about querying pool status, see <olink targetdoc="" remap="internal" targetptr="gaynp">Querying ZFS Storage Pool Status</olink>.</para>
<para>By default, the offline state is persistent. The device remains offline
when the system is rebooted. </para>
<para>To temporarily take a device offline, use the <command>zpool offline</command> <option>
t</option> option. For example:</para>
<screen># <userinput>zpool offline -t tank c1t0d0</userinput>
 bringing device 'c1t0d0' offline</screen>
<para>When the system is rebooted, this device is automatically returned to
the <literal>ONLINE</literal> state.</para>
<para>For more information on device health, see <olink targetdoc="" remap="internal" targetptr="gamno">Health Status of ZFS Storage Pools</olink>.</para>
</sect3>
<sect3 xml:id="gazgk">
<title>Bringing a Device Online</title>
<para>Once a device is taken offline, it can be restored by using the <command>zpool
online</command> command:</para>
<screen># <userinput>zpool online tank c1t0d0</userinput>
bringing device c1t0d0 online</screen>
<para>When a device is brought online, any data that has been written to the
pool is resynchronized to the newly available device. Note that you cannot
use device onlining to replace a disk. If you offline a device, replace the
drive, and try to bring it online, it remains in the faulted state.<indexterm xml:id="indexterm-144">
<primary>onlining a device</primary>
<secondary>ZFS storage pool (<command>zpool online</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-145">
<primary>
<command>zpool online</command>
</primary>
<secondary>(example of)</secondary>
</indexterm>
<indexterm xml:id="indexterm-146">
<primary>ZFS storage pools (<command>zpool online</command>)</primary>
<secondary>onlining a device</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<para>If you attempt to online a faulted device, a message similar to the
following is displayed from <command>fmd</command>:</para>
<screen># <userinput>zpool online tank c1t0d0</userinput>
Bringing device c1t0d0 online
# 
SUNW-MSG-ID: ZFS-8000-D3, TYPE: Fault, VER: 1, SEVERITY: Major
EVENT-TIME: Fri Mar 17 14:38:47 MST 2006
PLATFORM: SUNW,Ultra-60, CSN: -, HOSTNAME: neo
SOURCE: zfs-diagnosis, REV: 1.0
EVENT-ID: 043bb0dd-f0a5-4b8f-a52d-8809e2ce2e0a
DESC: A ZFS device failed.  Refer to http://sun.com/msg/ZFS-8000-D3 for more information.
AUTO-RESPONSE: No automated response will occur.
IMPACT: Fault tolerance of the pool may be compromised.
REC-ACTION: Run 'zpool status -x' and replace the bad device.</screen>
<para>For more information on replacing a faulted device, see <olink targetdoc="" remap="internal" targetptr="gbbvb">Repairing a Missing Device</olink>.</para>
</sect3>
</sect2>
<sect2 xml:id="gazge">
<title>Clearing Storage Pool Devices</title>
<para>If a device is taken offline due to a failure that causes errors to
be listed in the <command>zpool status</command> output, you can clear the
error counts with the <command>zpool clear</command> command.<indexterm xml:id="indexterm-147">
<primary>clearing</primary>
<secondary>a device in a ZFS storage pool (<command>zpool clear</command>)</secondary>
<tertiary>description</tertiary>
</indexterm>
<indexterm xml:id="indexterm-148">
<primary>
<command>zpool clear</command>
</primary>
<secondary>description</secondary>
</indexterm>
</para>
<para>If specified with no arguments, this command clears all device errors
within the pool. For example:</para>
<screen># <userinput>zpool clear tank</userinput>
</screen>
<para>If one or more devices are specified, this command only clear errors
associated with the specified devices. For example:<indexterm xml:id="indexterm-149">
<primary>clearing a device</primary>
<secondary>ZFS storage pool</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-150">
<primary>
<command>zpool clear</command>
</primary>
<secondary>(example of)</secondary>
</indexterm>
<indexterm xml:id="indexterm-151">
<primary>ZFS storage pools</primary>
<secondary>clearing a device</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool clear tank c1t0d0</userinput>
</screen>
<para>For more information on clearing <command>zpool</command> errors, see <olink targetdoc="" remap="internal" targetptr="gbbzv">Clearing Transient Errors</olink>.</para>
</sect2>
<sect2 xml:id="gazgd">
<title>Replacing Devices in a Storage Pool</title>
<para>You can replace a device in a storage pool by using the <command>zpool
replace</command> command.<indexterm xml:id="indexterm-152">
<primary>replacing</primary>
<secondary>a device (<command>zpool replace</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-153">
<primary>
<command>zpool replace</command>
</primary>
<secondary>(example of)</secondary>
</indexterm>
<indexterm xml:id="indexterm-154">
<primary>ZFS storage pools</primary>
<secondary>replacing a device (<command>zpool replace</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool replace tank c1t1d0 c1t2d0</userinput>
</screen>
<para>In this example, the previous device, <literal>c1t1d0</literal>, is
replaced by <literal>c1t2d0</literal>.</para>
<para>The replacement device must be greater than or equal to the minimum
size of all the devices in a mirror or RAID-Z        configuration. If the
replacement device is larger, the pool size in an unmirrored or non RAID-Z
configuration is increased when the replacement is complete.</para>
<para>For more information about replacing devices, see <olink targetdoc="" remap="internal" targetptr="gbbvb">Repairing a Missing Device</olink> and <olink targetdoc="" remap="internal" targetptr="gbbvf">Repairing a Damaged Device</olink>.</para>
</sect2>
</sect1>
<sect1 xml:id="gaynp">
<title>Querying ZFS Storage Pool Status</title>
<para>The <command>zpool list</command> command provides a number of ways
to request information regarding pool status. The information available generally
falls into three categories: basic usage information, I/O statistics, and
health status. All three types of storage pool information are covered in
this section.<indexterm xml:id="indexterm-155">
<primary>listing</primary>
<secondary>ZFS storage pools</secondary>
<tertiary>description</tertiary>
</indexterm>
<indexterm xml:id="indexterm-156">
<primary>
<command>zpool list</command>
</primary>
<secondary>description</secondary>
</indexterm>
</para>
<sect2 xml:id="gamml">
<title>Basic ZFS Storage Pool Information</title>
<para>You can use the <command>zpool list</command> command to display basic
information about pools.</para>
<sect3 xml:id="gazij">
<title>Listing Information About All Storage Pools</title>
<para>With no arguments, the command displays all the fields for all pools
on the system. For example:</para>
<screen># <userinput>zpool list</userinput>
NAME                    SIZE    USED   AVAIL    CAP  HEALTH     ALTROOT
tank                   80.0G   22.3G   47.7G    28%  ONLINE     -
dozer                   1.2T    384G    816G    32%  ONLINE     -</screen>
<para>This output displays the following information:</para>
<variablelist>
<varlistentry>
<term>
<literal>NAME</literal>
</term>
<listitem>
<para>The name of the pool.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>SIZE</literal>
</term>
<listitem>
<para>The total size of the pool, equal to the sum of the size of
all top-level virtual devices.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>USED</literal>
</term>
<listitem>
<para>The amount of space allocated by all datasets and internal
metadata. Note that this amount is different from the amount of space as reported
at the file system level.</para>
<para>For more information about determining available file system space,
see <olink targetdoc="" remap="internal" targetptr="gbchp">ZFS Space Accounting</olink>.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>AVAILABLE</literal>
</term>
<listitem>
<para>The amount of unallocated space in the pool.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>CAPACITY</literal> (<literal>CAP</literal>)</term>
<listitem>
<para>The amount of space used, expressed as a percentage of total
space.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>HEALTH</literal>
</term>
<listitem>
<para>The current health status of the pool.</para>
<para>For more information about pool health, see <olink targetdoc="" remap="internal" targetptr="gamno">Health Status of ZFS Storage Pools</olink>.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>ALTROOT</literal>
</term>
<listitem>
<para>The alternate root of the pool, if any.</para>
<para>For more information about alternate root pools, see <olink targetdoc="" remap="internal" targetptr="gbcgl">ZFS Alternate Root Pools</olink>.</para>
</listitem>
</varlistentry>
</variablelist>
<para>You can also gather statistics for a specific pool by specifying the
pool name. For example:<indexterm xml:id="indexterm-157">
<primary>listing</primary>
<secondary>ZFS storage pools</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-158">
<primary>
<command>zpool list</command>
</primary>
<secondary>(example of)</secondary>
</indexterm>
<indexterm xml:id="indexterm-159">
<primary>ZFS storage pools</primary>
<secondary>listing</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool list tank</userinput>
NAME                    SIZE    USED   AVAIL    CAP  HEALTH     ALTROOT
tank                   80.0G   22.3G   47.7G    28%  ONLINE     -</screen>
</sect3>
<sect3 xml:id="gazil">
<title>Listing Specific Storage Pool Statistics</title>
<para>Specific statistics can be requested by using the <option>
o</option> option.
This option allows for custom reports or a quick way to list pertinent information.
For example, to list only the name and size of each pool, you use the following
syntax:</para>
<screen># <userinput>zpool list -o name,size</userinput>
NAME                    SIZE
tank                   80.0G
dozer                   1.2T</screen>
<para>The column names correspond to the properties that are listed in <olink targetdoc="" remap="internal" targetptr="gazij">Listing Information About All Storage Pools</olink>.</para>
</sect3>
<sect3 xml:id="gazje">
<title>Scripting ZFS Storage Pool Output</title>
<para>The default output for the <command>zpool list</command> command is
designed for readability, and is not easy to use as part of a shell script.
To aid programmatic uses of the command, the <option>
H</option> option can
be used to suppress the column headings and separate fields by tabs, rather
than by spaces. For example, to request a simple list of all pool names on
the system:<indexterm xml:id="indexterm-160">
<primary>scripting</primary>
<secondary>ZFS storage pool output</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-161">
<primary>
<command>zpool list -Ho name</command>
</primary>
<secondary>(example of)</secondary>
</indexterm>
<indexterm xml:id="indexterm-162">
<primary>ZFS storage pools</primary>
<secondary>scripting storage pool output</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool list -Ho name</userinput>
tank
dozer</screen>
<para>Here is another example:</para>
<screen># <userinput>zpool list -H -o name,size</userinput>
tank   80.0G
dozer  1.2T</screen>
</sect3>
</sect2>
<sect2 xml:id="gammt">
<title>ZFS Storage Pool I/O Statistics</title>
<para>To request I/O statistics for a pool or specific virtual devices, use
the <command>zpool iostat</command> command. Similar to the <command>iostat</command> command,
this command can display a static snapshot of all I/O activity so far, as
well as updated statistics for every specified interval. The following statistics
are reported:<indexterm xml:id="indexterm-163">
<primary>displaying</primary>
<secondary>ZFS storage pool I/O statistics</secondary>
<tertiary>description</tertiary>
</indexterm>
</para>
<variablelist>
<varlistentry>
<term>
<literal>USED CAPACITY</literal>
</term>
<listitem>
<para>The amount of data currently stored in the pool or device.
This figure differs from the amount of space available to actual file systems
by a small amount due to internal implementation details.</para>
<para>For more information about the difference between pool space and dataset
space, see <olink targetdoc="" remap="internal" targetptr="gbchp">ZFS Space Accounting</olink>.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>AVAILABLE CAPACITY</literal>
</term>
<listitem>
<para>The amount of space available in the pool or device. As with
the <literal>used</literal> statistic, this is differs from the amount of
space available to datasets by a small margin.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>READ OPERATIONS</literal>
</term>
<listitem>
<para>The number of read I/O operations sent to the pool or device,
including metadata requests.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>WRITE OPERATIONS</literal>
</term>
<listitem>
<para>The number of write I/O operations sent to the pool or device.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>READ BANDWIDTH</literal>
</term>
<listitem>
<para>The bandwidth of all read operations (including metadata),
expressed as units per second.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>WRITE BANDWIDTH</literal>
</term>
<listitem>
<para>The bandwidth of all write operations, expressed as units
per second.</para>
</listitem>
</varlistentry>
</variablelist>
<sect3 xml:id="gazng">
<title>Listing Pool-Wide Statistics</title>
<para>With no options, the <command>zpool iostat</command> command displays
the accumulated statistics since boot for all pools on the system. For example:<indexterm xml:id="indexterm-164">
<primary>displaying</primary>
<secondary>ZFS storage pool-wide I/O statistics</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-165">
<primary>
<command>zpool iostat</command>
</primary>
<secondary>pool-wide (example of)</secondary>
</indexterm>
<indexterm xml:id="indexterm-166">
<primary>ZFS storage pools</primary>
<secondary>pool-wide I/O statistics</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool iostat</userinput>
               capacity     operations    bandwidth
pool         used  avail   read  write   read  write
----------  -----  -----  -----  -----  -----  -----
tank         100G  20.0G   1.2M   102K   1.2M  3.45K
dozer       12.3G  67.7G   132K  15.2K  32.1K  1.20K</screen>
<para>Because these statistics are cumulative since boot, bandwidth might
appear low if the pool is relatively idle. You can request a more accurate
view of current bandwidth usage by specifying an interval. For example:</para>
<screen># <userinput>zpool iostat tank 2</userinput>
               capacity     operations    bandwidth
pool         used  avail   read  write   read  write
----------  -----  -----  -----  -----  -----  -----
tank         100G  20.0G   1.2M   102K   1.2M  3.45K
tank         100G  20.0G    134      0  1.34K      0
tank         100G  20.0G     94    342  1.06K   4.1M</screen>
<para>In this example, the command displays usage statistics only for the
pool <literal>tank</literal> every two seconds until the you type Ctrl-C.
Alternately, you can specify an additional <literal>count</literal> parameter,
which causes the command to terminate after the specified number of iterations.
For example, <command>zpool iostat 2 3</command> would print a summary every
two seconds for three iterations, for a total of six seconds. If there is
a single pool, then the statistics is displayed on consecutive lines. If more
than one pool exists, then an additional dashed line delineates each iteration
to provide visual separation.</para>
</sect3>
<sect3 xml:id="gazne">
<title>Listing Virtual Device Statistics</title>
<para>In addition to pool-wide I/O statistics, the <command>zpool iostat</command> command
can display statistics for specific virtual devices. This command can be used
to identify abnormally slow devices, or simply to observe the distribution
of I/O generated by ZFS. To request the complete virtual device layout as
well as all I/O statistics, use the <command>zpool iostat -v</command> command.
For example:<indexterm xml:id="indexterm-167">
<primary>displaying</primary>
<secondary>ZFS storage pool vdev I/O statistics</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-168">
<primary>
<command>zpool iostat -v</command>
</primary>
<secondary>vdev (example of)</secondary>
</indexterm>
<indexterm xml:id="indexterm-169">
<primary>ZFS storage pools</primary>
<secondary>vdev I/O statistics</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool iostat -v</userinput>
               capacity     operations    bandwidth
tank         used  avail   read  write   read  write
----------  -----  -----  -----  -----  -----  -----
mirror      20.4G  59.6G      0     22      0  6.00K
  c1t0d0        -      -      1    295  11.2K   148K
  c1t1d0        -      -      1    299  11.2K   148K
----------  -----  -----  -----  -----  -----  -----
total       24.5K   149M      0     22      0  6.00K</screen>
<para>Note two important things when viewing I/O statistics on a virtual device
basis.</para>
<itemizedlist>
<listitem>
<para>First, space usage is only available for top-level virtual
devices. The way in which space is allocated among mirror and RAID-Z virtual
devices is particular to the implementation and not easily expressed as a
single number. </para>
</listitem>
<listitem>
<para>Second, the numbers might not add up exactly as you would
expect them to. In particular, operations across RAID-Z and mirrored devices
will not be exactly equal. This difference is particularly noticeable immediately
after a pool is created, as a significant amount of I/O is done directly to
the disks as part of pool creation that is not accounted for at the mirror
level. Over time, these numbers should gradually equalize, although broken,
unresponsive, or offlined devices can affect this symmetry as well.</para>
</listitem>
</itemizedlist>
<para>You can use the same set of options (interval and count) when examining
virtual device statistics.</para>
</sect3>
</sect2>
<sect2 xml:id="gamno">
<title>Health Status of ZFS Storage Pools</title>
<para>ZFS provides an integrated method of examining pool and device health.
The health of a pool is determined from the state of all its devices. This
state information is displaying by using the <command>zpool status</command> command.
In addition, potential pool and device failures are reported by <command>fmd</command> and
are displayed on the system console and the <command>/var/adm/messages</command> file.
This section describes how to determine pool and device health. This chapter
does not document how to repair or recover from unhealthy pools. For more
information on troubleshooting and data recovery, see <olink targetdoc="" remap="internal" targetptr="gavwg">Chapter 9, ZFS Troubleshooting and Data Recovery</olink>.<indexterm xml:id="indexterm-170">
<primary>displaying</primary>
<secondary>health status of storage pools</secondary>
<tertiary>description of</tertiary>
</indexterm>
<indexterm xml:id="indexterm-171">
<primary>ZFS storage pools</primary>
<secondary>displaying health status</secondary>
</indexterm>
</para>
<para>Each device can fall into one of the following states:</para>
<variablelist>
<varlistentry>
<term>
<literal>ONLINE</literal>
</term>
<listitem>
<para>The device is in normal working order. While some transient
errors might still occur, the device is otherwise in working order.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>DEGRADED</literal>
</term>
<listitem>
<para>The virtual device has experienced failure but is still able
to function. This state is most common when a mirror or RAID-Z device has
lost one or more constituent devices. The fault tolerance of the pool might
be compromised, as a subsequent fault in another device might be unrecoverable.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>FAULTED</literal>
</term>
<listitem>
<para>The virtual device is completely inaccessible. This status
typically indicates total failure of the device, such that ZFS is incapable
of sending or receiving data from it. If a top-level virtual device is in
this state, then the pool is completely inaccessible.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>OFFLINE</literal>
</term>
<listitem>
<para>The virtual device has been explicitly taken offline by the
administrator.</para>
</listitem>
</varlistentry>
<varlistentry>
<term>
<literal>UNAVAILABLE</literal>
</term>
<listitem>
<para>The device or virtual device cannot be opened. In some cases,
pools with <literal>UNAVAILABLE</literal> devices appear in <literal>DEGRADED</literal> mode.
If a top-level virtual device is unavailable, then nothing in the pool can
be accessed.</para>
</listitem>
</varlistentry>
</variablelist>
<para>The health of a pool is determined from the health of all its top-level
virtual devices. If all virtual devices are <literal>ONLINE</literal>, then
the pool is also <literal>ONLINE</literal>. If any one of the virtual devices
is <literal>DEGRADED</literal> or <literal>UNAVAILABLE</literal>, then the
pool is also <literal>DEGRADED</literal>. If a top-level virtual device is <literal>FAULTED</literal> or <literal>OFFLINE</literal>, then the pool is also <literal>FAULTED</literal>. A pool in the faulted state is completely inaccessible. No data
can be recovered until the necessary devices are attached or repaired. A pool
in the degraded state continues to run, but you might not achieve the same
level of data replication or data throughput if the pool were online.</para>
<sect3 xml:id="gazqw">
<title>Basic Storage Pool Health Status</title>
<para>The simplest way to request a quick overview of pool health status is
to use the <command>zpool status</command> command:<indexterm xml:id="indexterm-172">
<primary>displaying</primary>
<secondary>ZFS storage pool health status</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-173">
<primary>
<command>zpool status -x</command>
</primary>
<secondary>(example of)</secondary>
</indexterm>
<indexterm xml:id="indexterm-174">
<primary>ZFS storage pools</primary>
<secondary>displaying health status</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool status -x</userinput>
all pools are healthy</screen>
<para>Specific pools can be examined by specifying a pool name to the command.
Any pool that is not in the <literal>ONLINE</literal> state should be investigated
for potential problems, as described in the next section.</para>
</sect3>
<sect3 xml:id="gazqf">
<title>Detailed Health Status</title>
<para>You can request a more detailed health summary by using the <option>
v</option> option.
For example:</para>
<screen># <userinput>zpool status -v tank</userinput>
  pool: tank
 state: DEGRADED
status: One or more devices could not be opened.  Sufficient replicas exist 
        for the pool to continue functioning in a degraded state.
action: Attach the missing device and online it using 'zpool online'.
   see: http://www.sun.com/msg/ZFS-8000-2Q
 scrub: none requested
config:

        NAME                STATE     READ WRITE CKSUM
        tank                DEGRADED     0     0     0
          mirror            DEGRADED     0     0     0
            c1t0d0          FAULTED      0     0     0  cannot open
            c1t1d0          ONLINE       0     0     0
errors: No known data errors</screen>
<para>This output displays a complete description of why the pool is in its
current state, including a readable description of the problem and a link
to a knowledge article for more information. Each knowledge article provides
up-to-date information on the best way to recover from your current problem.
Using the detailed configuration information, you should be able to determine
which device is damaged and how to repair the pool.<indexterm xml:id="indexterm-175">
<primary>displaying</primary>
<secondary>detailed ZFS storage pool health status</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-176">
<primary>
<command>zpool status -v</command>
</primary>
<secondary>(example of)</secondary>
</indexterm>
<indexterm xml:id="indexterm-177">
<primary>ZFS storage pools</primary>
<secondary>displaying detailed health status</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<para>In the above example, the faulted device should be replaced. After the
device is replaced, use the <command>zpool online</command> command to bring
the device back online. For example:</para>
<screen># <userinput>zpool online tank c1t0d0</userinput>
Bringing device c1t0d0 online
# <userinput>zpool status -x</userinput>
all pools are healthy</screen>
<para>If a pool has an offlined device, the command output identifies the
problem pool. For example:</para>
<screen># <userinput>zpool status -x</userinput>
  pool: tank
 state: DEGRADED
status: One or more devices could not be opened.  Sufficient replicas exist for
        the pool to continue functioning in a degraded state.
action: Attach the missing device and online it using 'zpool online'.
   see: http://www.sun.com/msg/ZFS-8000-D3
 scrub: resilver completed with 0 errors on Fri Mar 17 14:38:47 2006
config:

        NAME         STATE     READ WRITE CKSUM
        tank         DEGRADED     0     0     0
          mirror     DEGRADED     0     0     0
            c1t0d0   UNAVAIL      0     0     0  cannot open
            c1t1d0   ONLINE       0     0     0</screen>
<para>The <literal>READ</literal> and <literal>WRITE</literal> columns provides
a count of I/O errors seen on the device, while the <literal>CKSUM</literal> column
provides a count of uncorrectable checksum errors that occurred on the device.
Both of these error counts likely indicate potential device failure, and some
corrective action is needed. If non-zero errors are reported for a top-level
virtual device, portions of your data might have become inaccessible. The
errors count identifies any known data errors.</para>
<para>In the example output above, the offlined device is not causing data
errors.</para>
<para>For more information about diagnosing and repairing faulted pools and
data, see <olink targetdoc="" remap="internal" targetptr="gavwg">Chapter 9, ZFS Troubleshooting and Data Recovery</olink>.</para>
</sect3>
</sect2>
</sect1>
<sect1 xml:id="gbchy">
<title>Migrating ZFS Storage Pools</title>
<para>Occasionally, you might need to move a storage pool between machines.
To do so, the storage devices must be disconnected from the original machine
and reconnected to the destination machine. This task can be accomplished
by physically recabling the devices, or by using multiported devices such
as the devices on a SAN. ZFS enables you to export the pool from one machine
and import it on the destination machine, even if the machines are of different
endianness. For information about replicating or migrating file systems between
different storage pools, which might reside on different machines, see <olink targetdoc="" remap="internal" targetptr="gbchx">Saving and Restoring ZFS Data</olink>.<indexterm xml:id="indexterm-178">
<primary>migrating ZFS storage pools</primary>
<secondary>description</secondary>
</indexterm>
<indexterm xml:id="indexterm-179">
<primary>ZFS storage pools</primary>
<secondary>migrating</secondary>
<tertiary>description</tertiary>
</indexterm>
</para>
<sect2 xml:id="gazre">
<title>Preparing for ZFS Storage Pool Migration</title>
<para>Storage pools should be explicitly exported to indicate that they are
ready to be migrated. This operation flushes any unwritten data to disk, writes
data to the disk indicating that the export was done, and removes all knowledge
of the pool from the system.</para>
<para>If you do not explicitly export the pool, but instead remove the disks
manually, you can still import the resulting pool on another system. However,
you might lose the last few seconds of data transactions, and the pool will
appear faulted on the original machine because the devices are no longer present.
By default, the destination machine refuses to import a pool that has not
been explicitly exported. This condition is necessary to prevent accidentally
importing an active pool that consists of network attached storage that is
still in use on another system.</para>
</sect2>
<sect2 xml:id="gazqr">
<title>Exporting a ZFS Storage Pool</title>
<para>To export a pool, use the <command>zpool export</command> command. For
example:</para>
<screen># <userinput>zpool export tank</userinput>
</screen>
<para>Once this command is executed, the pool <literal>tank</literal> is no
longer visible on the system. The command attempts to unmount any mounted
file systems within the pool before continuing. If any of the file systems
fail to unmount, you can forcefully unmount them by using the <option>
f</option> option.
For example:</para>
<screen># <userinput>zpool export tank</userinput>
cannot unmount '/export/home/eschrock': Device busy
# <userinput>zpool export -f tank</userinput>
</screen>
<para>If devices are unavailable at the time of export, the disks cannot be
specified as cleanly exported. If one of these devices is later attached to
a system without any of the working devices, it appears as “potentially
active.” If emulated volumes are in use in the pool, the pool cannot
be exported, even with the <option>
f</option> option. To export a pool with
an emulated volume, first make sure that all consumers of the volume are no
longer active.<indexterm xml:id="indexterm-180">
<primary>exporting</primary>
<secondary>ZFS storage pool</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-181">
<primary>
<command>zpool export</command>
</primary>
<secondary>(example of)</secondary>
</indexterm>
<indexterm xml:id="indexterm-182">
<primary>ZFS storage pools</primary>
<secondary>exporting</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<para>For more information about emulated volumes, see <olink targetdoc="" remap="internal" targetptr="gaypf">Emulated Volumes</olink>.</para>
</sect2>
<sect2 xml:id="gazru">
<title>Determining Available Storage Pools to Import</title>
<para>Once the pool has been removed from the system (either through export
or by forcefully removing the devices), attach the devices to the target system.
Although ZFS can handle some situations in which only a portion of the devices
is available, all devices within the pool must be moved between the systems.
The devices do not necessarily have to be attached under the same device name.
ZFS detects any moved or renamed devices, and adjusts the configuration appropriately.
To discover available pools, run the <command>zpool import</command> command
with no options. For example:</para>
<screen># <userinput>zpool import</userinput>
  pool: tank
    id: 3778921145927357706
 state: ONLINE
action: The pool can be imported using its name or numeric identifier.
config:

        tank        ONLINE
          mirror    ONLINE
            c1t0d0  ONLINE
            c1t1d0  ONLINE</screen>
<para>In this example, the pool <literal>tank</literal> is available to be
imported on the target system. Each pool is identified by a name as well as
a unique numeric identifier. If multiple pools available to import have the
same name, you can use the numeric identifier to distinguish between them.<indexterm xml:id="indexterm-183">
<primary>identifying</primary>
<secondary>ZFS storage pool for import (<command>zpool import -a</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-184">
<primary>
<command>zpool import -a</command>
</primary>
<secondary>(example of)</secondary>
</indexterm>
<indexterm xml:id="indexterm-185">
<primary>ZFS storage pools</primary>
<secondary>identifying for import (<command>zpool import -a</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<para>Similar to the <command>zpool status</command> command, the <command>zpool
import</command> command refers to a knowledge article available on the web
with the most up-to-date information regarding repair procedures for this
problem. In this case, the user can force the pool to be imported. However,
importing a pool that is currently in use by another system over a storage
network can result in data corruption and panics as both systems attempt to
write to the same storage. If some devices in the pool are not available but
enough redundancy is available to have a usable pool, the pool appears in
the <literal>DEGRADED</literal> state. For example:</para>
<screen># <userinput>zpool import</userinput>
  pool: tank
    id: 3778921145927357706
 state: DEGRADED
status: One or more devices are missing from the system.
action: The pool can be imported despite missing or damaged devices.  The
        fault tolerance of the pool may be compromised if imported.
   see: http://www.sun.com/msg/ZFS-8000-2Q
config:

        tank         DEGRADED
          mirror     DEGRADED
            c1t0d0   UNAVAIL   cannot open
            c1t1d0   ONLINE</screen>
<para>In this example, the first disk is damaged or missing, though you can
still import the pool because the mirrored data is still accessible. If too
many faulted or missing devices are present, the pool cannot be imported.
For example:</para>
<screen># <userinput>zpool import</userinput>
  pool: dozer
    id: 12090808386336829175
 state: FAULTED
action: The pool cannot be imported. Attach the missing
        devices and try again.
   see: http://www.sun.com/msg/ZFS-8000-6X
config:
        raidz               FAULTED
          c1t0d0    ONLINE
          c1t1d0    FAULTED
          c1t2d0    ONLINE
          c1t3d0    FAULTED</screen>
<para>In this example, two disks are missing from a RAID-Z virtual device,
which means that sufficient replicated data is not available to reconstruct
the pool. In some cases, not enough devices are present to determine the complete
configuration. In this case, ZFS doesn't know what other devices were part
of the pool, though ZFS does report as much information as possible about
the situation. For example:</para>
<screen># <userinput>zpool import</userinput>
pool: dozer
    id: 12090808386336829175
 state: FAULTED
status: One or more devices are missing from the system.
action: The pool cannot be imported. Attach the missing
        devices and try again.
   see: http://www.sun.com/msg/ZFS-8000-6X
config:
        dozer          FAULTED   missing device
          raidz       ONLINE
            c1t0d0    ONLINE
            c1t1d0    ONLINE
            c1t2d0    ONLINE
            c1t3d0    ONLINE
        Additional devices are known to be part of this pool, though their
        exact configuration cannot be determined.</screen>
</sect2>
<sect2 xml:id="gaztk">
<title>Finding ZFS Storage Pools From Alternate Directories</title>
<para>By default, the <command>zpool import</command> command only searches
devices within the <filename>/dev/dsk</filename> directory. If devices exist
in another directory, or you are using pools backed by files, you must use
the <option>
d</option> option to search different directories. For example:</para>
<screen># <userinput>zpool create dozer /file/a /file/b</userinput>
# <userinput>zpool export dozer</userinput>
# <userinput>zpool import</userinput>
no pools available
# <userinput>zpool import -d /file</userinput>
  pool: dozer
    id: 672153753596386982
 state: ONLINE
action: The pool can be imported using its name or numeric identifier.
config:

        dozer       ONLINE
          /file/a   ONLINE
          /file/b   ONLINE
# <userinput>zpool import -d /file dozer</userinput>
</screen>
<para>If devices exist in multiple directories, you can specify multiple <option>
d</option> options.<indexterm xml:id="indexterm-186">
<primary>importing</primary>
<secondary>ZFS storage pool from alternate directories (<command>zpool import -d</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-187">
<primary>
<command>zpool import -d</command>
</primary>
<secondary>(example of)</secondary>
</indexterm>
<indexterm xml:id="indexterm-188">
<primary>ZFS storage pools</primary>
<secondary>importing from alternate directories (<command>zpool import -d</command>)</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
</sect2>
<sect2 xml:id="gazuf">
<title>Importing ZFS Storage Pools</title>
<para>Once a pool has been identified for import, you can import it by specifying
the name of the pool or its numeric identifier as an argument to the <command>zpool
import</command> command. For example:</para>
<screen># <userinput>zpool import tank</userinput>
</screen>
<para>If multiple available pools have the same name, you can specify which
pool to import using the numeric identifier. For example:</para>
<screen># <userinput>zpool import</userinput>
  pool: dozer
    id: 2704475622193776801
 state: ONLINE
action: The pool can be imported using its name or numeric identifier.
config:

        dozer       ONLINE
          c1t9d0    ONLINE

  pool: dozer
    id: 6223921996155991199
 state: ONLINE
action: The pool can be imported using its name or numeric identifier.
config:

        dozer       ONLINE
          c1t8d0    ONLINE
# <userinput>zpool import dozer</userinput>
cannot import 'dozer': more than one matching pool
import by numeric ID instead
# <userinput>zpool import 6223921996155991199</userinput>
</screen>
<para>If the pool name conflicts with an existing pool name, you can import
the pool under a different name. For example:</para>
<screen># <userinput>zpool import dozer zeepool</userinput>
</screen>
<para>This command imports the exported pool <literal>dozer</literal> using
the new name <literal>zeepool</literal>. If the pool was not cleanly exported,
ZFS requires the <option>
f</option> flag to prevent users from accidentally
importing a pool that is still in use on another system. For example:</para>
<screen># <userinput>zpool import dozer</userinput>
cannot import 'dozer': pool may be in use on another system
use '-f' to import anyway
# <userinput>zpool import -f dozer</userinput>
</screen>
<para>Pools can also be imported under an alternate root by using the <option>
R</option> option.
For more information on alternate root pools, see <olink targetdoc="" remap="internal" targetptr="gbcgl">ZFS Alternate Root Pools</olink>.<indexterm xml:id="indexterm-189">
<primary>importing</primary>
<secondary>ZFS storage pool</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-190">
<primary>
<command>zpool import</command> <replaceable>name</replaceable>
</primary>
<secondary>(example of)</secondary>
</indexterm>
<indexterm xml:id="indexterm-191">
<primary>ZFS storage pools</primary>
<secondary>importing</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
</sect2>
<sect2 xml:id="gcfhw">
<title>Recovering Destroyed ZFS Storage Pools</title>
<para>You can use the <command>zpool import</command> <option>
D</option> command
to recover a storage pool that has been destroyed. For example:</para>
<screen># <userinput>zpool destroy tank</userinput>
# <userinput>zpool import -D</userinput>
pool: tank
    id: 3778921145927357706
 state: ONLINE (DESTROYED)
action: The pool can be imported using its name or numeric identifier.  The
        pool was destroyed, but can be imported using the '-Df' flags.
config:

        tank        ONLINE
          mirror    ONLINE
            c1t0d0  ONLINE
            c1t1d0  ONLINE</screen>
<para>In the above <command>zpool import</command> output, you can identify
this pool as the destroyed pool because of the following state information:</para>
<screen>state: ONLINE (DESTROYED)</screen>
<para>To recover the destroyed pool, issue the <command>zpool import</command> <option>
D</option> command again with the pool to be recovered and the <option>
f</option> option.
For example:</para>
<screen># <userinput>zpool import -Df tank</userinput>
# <userinput>zpool status tank</userinput>
  pool: tank
 state: ONLINE
 scrub: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        tank        ONLINE       0     0     0
          mirror    ONLINE       0     0     0
            c1t0d0  ONLINE       0     0     0
            c1t1d0  ONLINE       0     0     0

errors: No known data errors</screen>
<para>If one of the devices in the destroyed pool is faulted or unavailable,
you might be able to recover the destroyed pool anyway. In this scenario,
import the degraded pool and then attempt to fix the device failure. For example:<indexterm xml:id="indexterm-192">
<primary>recovering</primary>
<secondary>destroyed ZFS storage pool</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
<indexterm xml:id="indexterm-193">
<primary>
<command>zpool import -D</command>
</primary>
<secondary>(example of)</secondary>
</indexterm>
<indexterm xml:id="indexterm-194">
<primary>ZFS storage pools</primary>
<secondary>recovering a destroyed pool</secondary>
<tertiary>(example of)</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool destroy dozer</userinput>
# <userinput>zpool import -D</userinput>
pool: dozer
    id: 
 state: DEGRADED (DESTROYED)
status: One or more devices are missing from the system.
action: The pool can be imported despite missing or damaged devices.  The
        fault tolerance of the pool may be compromised if imported.  The
        pool was destroyed, but can be imported using the '-Df' flags.
   see: http://www.sun.com/msg/ZFS-8000-2Q
config:

        dozer        DEGRADED
           raidz      ONLINE
            c1t0d0    ONLINE
            c1t1d0    ONLINE
            c1t2d0    UNAVAIL  cannot open
            c1t3d0    ONLINE
# <userinput>zpool import -Df dozer</userinput>
# <userinput>zpool status -x</userinput>
  pool: dozer
 state: DEGRADED
status: One or more devices could not be opened.  Sufficient replicas exist for
        the pool to continue functioning in a degraded state.
action: Attach the missing device and online it using 'zpool online'.
   see: http://www.sun.com/msg/ZFS-8000-D3
 scrub: resilver completed with 0 errors on Fri Mar 17 16:11:35 2006
config:

        NAME                     STATE     READ WRITE CKSUM
        dozer                    DEGRADED     0     0     0
          raidz                  ONLINE       0     0     0
            c1t0d0               ONLINE       0     0     0
            c1t1d0               ONLINE       0     0     0
            c1t2d0               UNAVAIL      0     0     0  cannot open
            c1t3d0               ONLINE       0     0     0

errors: No known data errors
# <userinput>zpool online dozer c1t2d0</userinput>
Bringing device c1t2d0 online
# <userinput>zpool status -x</userinput>
all pools are healthy</screen>
</sect2>
<sect2 xml:id="gcikw">
<title>Upgrading ZFS Storage Pools</title>
<para>In future ZFS releases, it might be necessary to upgrade your pools
to a newer version to take advantage of the features in the newer version.
The <command>zpool upgrade</command> command is available for this process.
In addition, the <command>zpool status</command> command has been modified
to notify you when your pools are running older versions. For example:<indexterm xml:id="indexterm-195">
<primary>upgrading</primary>
<secondary>ZFS storage pool</secondary>
<tertiary>description</tertiary>
</indexterm>
<indexterm xml:id="indexterm-196">
<primary>
<command>zpool upgrade</command>
</primary>
</indexterm>
<indexterm xml:id="indexterm-197">
<primary>ZFS storage pools</primary>
<secondary>upgrading</secondary>
<tertiary>description</tertiary>
</indexterm>
</para>
<screen># <userinput>zpool status</userinput>
  pool: test
 state: ONLINE
status: The pool is formatted using an older on-disk format.  The pool can
        still be used, but some features are unavailable.
action: Upgrade the pool using 'zpool upgrade'.  Once this is done, the
        pool will no longer be accessible on older software versions.
 scrub: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        test        ONLINE       0     0     0
          c1t27d0   ONLINE       0     0     0

errors: No known data errors</screen>
<para>In this ZFS release, running the <command>zpool upgrade</command> command
to upgrade pools should be unnecessary. Currently, this command displays the
initial ZFS version information.</para>
<screen># <userinput>zpool upgrade</userinput>
This system is currently running ZFS version 1.

All pools are formatted using this version.</screen>
<para>In future ZFS releases, you can use the following syntax to identify
additional information about a particular version and supported releases.</para>
<screen># <userinput>zpool upgrade -v</userinput>
This system is currently running ZFS version 1.

The following versions are supported:

VER  DESCRIPTION
---  --------------------------------------------------------
 1   Initial ZFS version.

For more information on a particular version, including supported releases, see:

http://www.opensolaris.org/os/community/zfs/version/N

Where 'N' is the version number.</screen>
<para>More information about the pool upgrade process will be provided in
future versions of this guide.</para>
</sect2>
</sect1>
</chapter>
