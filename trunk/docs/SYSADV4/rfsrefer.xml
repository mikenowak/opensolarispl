<chapter id="rfsrefer-1"><title>Accessing Network File Systems (Reference)</title><highlights><para>This chapter describes the NFS commands, as well as the different parts
of the NFS environment and how these parts work together.</para><itemizedlist><listitem><para><olink targetptr="rfsrefer-6" remap="internal">NFS Files</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-8" remap="internal">NFS Daemons</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-13" remap="internal">NFS Commands</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-37" remap="internal">Commands for Troubleshooting
NFS Problems</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-154" remap="internal">NFS Over RDMA</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-45" remap="internal">How the NFS Service Works</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-66" remap="internal">Autofs Maps</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-75" remap="internal">How Autofs Works</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-98" remap="internal">Autofs Reference</olink></para>
</listitem>
</itemizedlist><note><para>If your system has zones enabled and you want to use this feature
in a non-global zone, see <olink targetdoc="sysadrm" remap="external"><citetitle remap="book">System Administration Guide: Solaris Containers-Resource Management and Solaris Zones</citetitle></olink> for
more information.</para>
</note>
</highlights><sect1 id="rfsrefer-6"><title>NFS Files</title><para>You need several files to support NFS activities on any computer.
Many of these files are ASCII, but some of the files are data files. <olink targetptr="rfsrefer-tbl-7" remap="internal">Table 6&#8211;1</olink> lists these files and their
functions. </para><table frame="topbot" id="rfsrefer-tbl-7"><title>NFS Files</title><tgroup cols="2" colsep="0" rowsep="0"><colspec colnum="1" colname="column1" colwidth="3*"/><colspec colnum="2" colname="column2" colwidth="6*"/><thead><row rowsep="1"><entry><para>File Name</para>
</entry><entry><para>Function</para>
</entry>
</row>
</thead><tbody><row><entry><para><filename>/etc/default/autofs</filename></para>
</entry><entry><para>Lists configuration information for the autofs environment.</para>
</entry>
</row><row><entry><para><filename>/etc/default/fs</filename>     </para>
</entry><entry><para>Lists the default file-system type for local file systems.</para>
</entry>
</row><row><entry><para><filename>/etc/default/nfs</filename>  </para>
</entry><entry><para>Lists configuration information for <command>lockd</command> and <command>nfsd</command>. For more information, refer to <olink targetptr="rfsrefer-133" remap="internal">Keywords
for the /etc/default/nfs File</olink> and the <olink targetdoc="refman4" targetptr="nfs-4" remap="external"><citerefentry><refentrytitle>nfs</refentrytitle><manvolnum>4</manvolnum></citerefentry></olink> man page.</para>
</entry>
</row><row><entry><para><filename>/etc/default/nfslogd</filename>   </para>
</entry><entry><para>Lists configuration information for the NFS server logging daemon, <command>nfslogd</command>.</para>
</entry>
</row><row><entry><para><filename>/etc/dfs/dfstab</filename></para>
</entry><entry><para>Lists the local resources to be shared.</para>
</entry>
</row><row><entry><para><filename>/etc/dfs/fstypes</filename>    </para>
</entry><entry><para>Lists the default file-system types for remote file systems.</para>
</entry>
</row><row><entry><para><filename>/etc/dfs/sharetab</filename>    </para>
</entry><entry><para>Lists the local and remote resources that are shared. See the <olink targetdoc="refman4" targetptr="sharetab-4" remap="external"><citerefentry><refentrytitle>sharetab</refentrytitle><manvolnum>4</manvolnum></citerefentry></olink> man page. Do not edit this
file.</para>
</entry>
</row><row><entry><para><filename>/etc/mnttab</filename>   </para>
</entry><entry><para>Lists file systems that are currently mounted, including automounted
directories. See the <olink targetdoc="refman4" targetptr="mnttab-4" remap="external"><citerefentry><refentrytitle>mnttab</refentrytitle><manvolnum>4</manvolnum></citerefentry></olink> man
page. Do not edit this file.</para>
</entry>
</row><row><entry><para><filename>/etc/netconfig</filename></para>
</entry><entry><para>Lists the transport protocols. Do not edit this file. </para>
</entry>
</row><row><entry><para><filename>/etc/nfs/nfslog.conf</filename></para>
</entry><entry><para>Lists general configuration information for NFS server logging.</para>
</entry>
</row><row><entry><para><filename>/etc/nfs/nfslogtab</filename>   </para>
</entry><entry><para>Lists information for log postprocessing by <command>nfslogd</command>.
Do not edit this file.</para>
</entry>
</row><row><entry><para><filename>/etc/nfssec.conf</filename>   </para>
</entry><entry><para>Lists NFS security services.</para>
</entry>
</row><row><entry><para><filename>/etc/rmtab</filename></para>
</entry><entry><para>Lists file systems that are remotely mounted by NFS clients. See the <olink targetdoc="refman4" targetptr="rmtab-4" remap="external"><citerefentry><refentrytitle>rmtab</refentrytitle><manvolnum>4</manvolnum></citerefentry></olink> man page. Do not edit this
file.</para>
</entry>
</row><row><entry><para><filename>/etc/vfstab</filename>  </para>
</entry><entry><para>Defines file systems to be mounted locally. See the <olink targetdoc="refman4" targetptr="vfstab-4" remap="external"><citerefentry><refentrytitle>vfstab</refentrytitle><manvolnum>4</manvolnum></citerefentry></olink> man page.</para>
</entry>
</row>
</tbody>
</tgroup>
</table><para>The first entry in <filename>/etc/dfs/fstypes</filename> is often used
as the default file-system type for remote file systems. This entry defines
the NFS file-system type as the default.</para><para>Only one entry is in <filename>/etc/default/fs</filename>: the
default file-system type for local disks. You can determine the file-system
types that are supported on a client or server by checking the files in <filename>/kernel/fs</filename>.  </para><sect2 id="rfsrefer-146"><title><filename>/etc/default/autofs</filename> File</title><para>Starting in the Solaris 10 release, you can use the <filename>/etc/default/autofs</filename> file to configure your autofs environment. Specifically, this
file provides an additional way to configure your autofs commands and autofs
daemons. The same specifications you would make on the command line can be
made in this configuration file. However, unlike the specifications you would
make on the  command line, this file preserves your specifications, even during
upgrades to your operating system. Additionally,  you are no longer required
to update critical startup files to ensure that the existing behavior of your
autofs environment is preserved. You can make your specifications by providing
values for the following keywords:</para><variablelist termlength="wholeline"><varlistentry><term>AUTOMOUNT_TIMEOUT</term><listitem><para>Sets the duration for a file system to remain idle before
the file system is unmounted. This keyword is the equivalent of the <option>t</option> argument
for the <command>automount</command> command. The default value is 600.</para>
</listitem>
</varlistentry><varlistentry><term>AUTOMOUNT_VERBOSE</term><listitem><para>Provides notification of autofs mounts, unmounts, and other
nonessential events. This keyword is the equivalent of the <option>v</option> argument
for <command>automount</command>. The default value is FALSE.</para>
</listitem>
</varlistentry><varlistentry><term>AUTOMOUNTD_VERBOSE</term><listitem><para>Logs status messages to the console and is the equivalent
of the <option>v</option> argument for the <command>automountd</command> daemon.
The default value is FALSE.</para>
</listitem>
</varlistentry><varlistentry><term>AUTOMOUNTD_NOBROWSE</term><listitem><para>Turns browsing on or off for all autofs mount points and is
the equivalent of the <option>n</option> argument for <command>automountd</command>.
The default value is FALSE.</para>
</listitem>
</varlistentry><varlistentry><term>AUTOMOUNTD_TRACE</term><listitem><para>Expands each remote procedure call (RPC) and displays the
expanded RPC on standard output. This keyword is the equivalent of the <option>T</option> argument
for   <command>automountd</command>. The default value is 0. Values can range
from 0 to 5.</para>
</listitem>
</varlistentry><varlistentry><term>AUTOMOUNTD_ENV</term><listitem><para>Permits you to assign different values to different environments.
This keyword is the equivalent of the <option>D</option> argument for <command>automountd</command>. The AUTOMOUNTD_ENV keyword can be used multiple times. However,
you must use separate lines for each environment assignment.</para>
</listitem>
</varlistentry>
</variablelist><para>For more information, refer to the man pages for  <olink targetdoc="refman1m" targetptr="automount-1m" remap="external"><citerefentry><refentrytitle>automount</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> and <olink targetdoc="refman1m" targetptr="automountd-1m" remap="external"><citerefentry><refentrytitle>automountd</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink>. For procedural
information, refer to <olink targetptr="rfsadmin-250" remap="internal">How to Use the /etc/default/autofs
File</olink>.</para>
</sect2><sect2 id="rfsrefer-133"><title>Keywords for the <filename>/etc/default/nfs</filename> File</title><para>In NFS version 4, the following keywords can be set in the <filename>/etc/default/nfs</filename> file. These keywords control the NFS protocols that are used by
both the client and server.</para><variablelist termlength="wholeline"><varlistentry><term>NFS_SERVER_VERSMIN</term><listitem><para>Sets the minimum version of the NFS protocol to be registered
and offered by the server. Starting in the Solaris 10 release, the default
is 2. Other valid values include 3 or 4. Refer to <olink targetptr="rfsadmin-68" remap="internal">Setting Up NFS Services</olink>.</para>
</listitem>
</varlistentry><varlistentry><term>NFS_SERVER_VERSMAX</term><listitem><para>Sets the maximum version of the NFS protocol to be registered
and offered by the server. Starting in the Solaris 10 release, the default
is 4. Other valid values include 2 or 3. Refer to <olink targetptr="rfsadmin-68" remap="internal">Setting Up NFS Services</olink>.</para>
</listitem>
</varlistentry><varlistentry><term>NFS_CLIENT_VERSMIN</term><listitem><para>Sets the minimum version of the NFS protocol to be used by the
NFS client. Starting in the Solaris 10 release, the default is 2. Other valid
values include 3 or 4. Refer to <olink targetptr="rfsadmin-68" remap="internal">Setting Up
NFS Services</olink>.</para>
</listitem>
</varlistentry><varlistentry><term>NFS_CLIENT_VERSMAX</term><listitem><para>Sets the maximum version of the NFS protocol to be used by the
NFS client. Starting in the Solaris 10 release, the default is 4. Other valid
values include 2 or 3. Refer to <olink targetptr="rfsadmin-68" remap="internal">Setting Up
NFS Services</olink>.</para>
</listitem>
</varlistentry><varlistentry><term>NFS_SERVER_DELEGATION</term><listitem><para>Controls whether the NFS version 4 delegation feature is enabled
for the server. If this feature is enabled, the server attempts to provide
delegations to the NFS version 4 client. By default, server delegation is
enabled. To disable server delegation, see <olink targetptr="rfsadmin-965" remap="internal">How
to Select Different Versions of NFS on a Server</olink>. For more information,
refer to <olink targetptr="rfsrefer-140" remap="internal">Delegation in NFS Version 4</olink>.</para>
</listitem>
</varlistentry><varlistentry><term>NFSMAPID_DOMAIN</term><listitem><para>Sets a common domain for clients and servers. Overrides the default
behavior of using the local DNS domain name. For task information, refer to <olink targetptr="rfsadmin-68" remap="internal">Setting Up NFS Services</olink>. Also, see <olink targetptr="rfsrefer-118" remap="internal">nfsmapid Daemon</olink>.</para>
</listitem>
</varlistentry>
</variablelist>
</sect2><sect2 id="rfsrefer-101"><title><filename>/etc/default/nfslogd</filename> File</title><para>This file defines some of the parameters that are used when using NFS
server logging. The following parameters can be defined.</para><variablelist termlength="wholeline"><varlistentry><term>CYCLE_FREQUENCY</term><listitem><para>Determines the number of hours that must pass before the log
files are cycled.  The default value is 24 hours. This option is used to prevent
the log files from growing too large. </para>
</listitem>
</varlistentry><varlistentry><term>IDLE_TIME</term><listitem><para>Sets the number of seconds <command>nfslogd</command> should
sleep before checking for more information in the buffer file. This parameter
also determines how often the configuration file is checked. This parameter,
along with MIN_PROCESSING_SIZE, determines how often the buffer file is processed.
The default value is 300 seconds. Increasing this number can improve performance
by reducing the number of checks.</para>
</listitem>
</varlistentry><varlistentry><term>MAPPING_UPDATE_INTERVAL</term><listitem><para>Specifies the number of seconds between updates of the records
in the file-handle-to-path mapping tables. The default value is 86400 seconds
or one day. This parameter helps keep the file-handle-to-path mapping tables
up-to-date without having to continually update the tables.</para>
</listitem>
</varlistentry><varlistentry><term>MAX_LOGS_PRESERVE</term><listitem><para>Determines the number of log files to be saved. The default
value is 10. </para>
</listitem>
</varlistentry><varlistentry><term>MIN_PROCESSING_SIZE</term><listitem><para>Sets the minimum number of bytes that the buffer file must
reach before processing and writing to the log file. This parameter, along
with IDLE_TIME, determines how often the buffer file is processed. The default
value is 524288 bytes. Increasing this number can improve performance by reducing
the number of times the buffer file is processed.</para>
</listitem>
</varlistentry><varlistentry><term>PRUNE_TIMEOUT</term><listitem><para>Selects the number of hours that must pass before a file-handle-to-path
mapping record times out and can be reduced. The default value is 168 hours
or 7 days.</para>
</listitem>
</varlistentry><varlistentry><term>UMASK</term><listitem><para>Specifies the file mode creation mask for the log files that
are created by <command>nfslogd</command>. The default value is 0137.</para>
</listitem>
</varlistentry>
</variablelist>
</sect2><sect2 id="rfsrefer-102"><title><filename>/etc/nfs/nfslog.conf</filename> File</title><para>This file defines the path, file names, and type of logging to be used
by <command>nfslogd</command>. Each definition is associated with a <replaceable>tag</replaceable>. Starting NFS server logging requires that you identify
the <replaceable>tag</replaceable> for each file system. The global tag defines
the default values. You can use the following parameters with each tag as
needed.</para><variablelist termlength="wholeline"><varlistentry><term>defaultdir=<replaceable>path</replaceable></term><listitem><para>Specifies the default directory path for the logging files.
Unless you specify differently, the default directory is <filename>/var/nfs</filename>.</para>
</listitem>
</varlistentry><varlistentry><term>log=<replaceable>path/filename</replaceable></term><listitem><para>Sets the path and file name for the log files. The default
is <filename>/var/nfs/nfslog</filename>.</para>
</listitem>
</varlistentry><varlistentry><term>fhtable=<replaceable>path/filename</replaceable></term><listitem><para>Selects the path and file name for the file-handle-to-path
database files. The default is <filename>/var/nfs/fhtable</filename>.</para>
</listitem>
</varlistentry><varlistentry><term>buffer=<replaceable>path/filename</replaceable></term><listitem><para>Determines the path and file name for the buffer files. The
default is <filename>/var/nfs/nfslog_workbuffer</filename>.</para>
</listitem>
</varlistentry><varlistentry><term>logformat=<replaceable>basic</replaceable>|<replaceable>extended</replaceable></term><listitem><para>Selects the format to be used when creating user-readable
log files. The basic format produces a log file that is similar to some <command>ftpd</command> daemons. The extended format gives a more detailed view.</para>
</listitem>
</varlistentry>
</variablelist><para>If the path is not specified, the path that is defined by <literal>defaultdir</literal> is used. Also, you can override <literal>defaultdir</literal> by
using an absolute path.</para><para>To identify the files more easily, place the files in separate directories.
Here is an example of the changes that are needed.</para><screen>% <userinput>cat /etc/nfs/nfslog.conf</userinput>
#ident  "@(#)nfslog.conf        1.5     99/02/21 SMI"
#
  .
  .
# NFS server log configuration file.
#

global  defaultdir=/var/nfs \
        log=nfslog fhtable=fhtable buffer=nfslog_workbuffer

publicftp log=logs/nfslog fhtable=fh/fhtables buffer=buffers/workbuffer</screen><itemizedlist><para>In this example, any file system that is shared with <literal>log=publicftp</literal> uses the following values: </para><listitem><para>The default directory is <literal>/var/nfs</literal>.</para>
</listitem><listitem><para>Log files are stored in <literal>/var/nfs/logs/nfslog*</literal>.</para>
</listitem><listitem><para>File-handle-to-path database tables are stored in <literal>/var/nfs/fh/fhtables</literal>.</para>
</listitem><listitem><para>Buffer files are stored in <literal>/var/nfs/buffers/workbuffer</literal>.</para>
</listitem>
</itemizedlist><para>For procedural information, refer to <olink targetptr="rfsadmin-101" remap="internal">How
to Enable NFS Server Logging</olink>.</para>
</sect2>
</sect1><sect1 id="rfsrefer-8"><title>NFS Daemons</title><para>To support NFS activities, several daemons are started when a system
goes into run level <literal>3</literal> or multiuser mode. The <command>mountd</command> and <command>nfsd</command> daemons are run on systems that are servers. The automatic
startup of the server daemons depends on the existence of entries that are
labeled with the NFS file-system type in <filename>/etc/dfs/sharetab</filename>.
To support NFS file locking, the <command>lockd</command> and <command>statd</command> daemons
are run on NFS clients and servers. However, unlike previous versions of NFS,
in NFS version 4, the daemons <command>lockd</command>, <command>statd</command>, <command>mountd</command>, and <command>nfslogd</command> are not used.</para><para>This section describes the following daemons.</para><itemizedlist><listitem><para><olink targetptr="rfsrefer-65" remap="internal">automountd Daemon</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-9" remap="internal">lockd Daemon</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-10" remap="internal">mountd Daemon</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-144" remap="internal">nfs4cbd Daemon</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-11" remap="internal">nfsd Daemon</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-7" remap="internal">nfslogd Daemon</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-118" remap="internal">nfsmapid Daemon</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-12" remap="internal">statd Daemon</olink></para>
</listitem>
</itemizedlist><sect2 id="rfsrefer-65"><title><command>automountd</command> Daemon</title><para>This daemon handles the mounting and unmounting requests from the autofs
service. The syntax of the command is as follows:</para><para><command>automountd</command> <literal>[</literal> <option>Tnv</option> <command>]</command> <command>[</command> <command>-D</command> <replaceable>name</replaceable>=<replaceable>value</replaceable> <literal>]</literal></para><itemizedlist><para>The command behaves in the following ways:</para><listitem><para><option>T</option> enables tracing.</para>
</listitem><listitem><para><option>n</option> disables browsing on all autofs nodes.</para>
</listitem><listitem><para><option>v</option> selects to log all status messages to the
console.</para>
</listitem><listitem><para><option>D</option> <replaceable>name</replaceable>=<replaceable>value</replaceable> substitutes <replaceable>value</replaceable> for the automount
map variable that is indicated by <replaceable>name</replaceable>.</para>
</listitem>
</itemizedlist><para>The default value for the automount map is <filename>/etc/auto_master</filename>.
Use the <option>T</option> option for troubleshooting.</para>
</sect2><sect2 id="rfsrefer-9"><title><command>lockd</command> Daemon</title><para>This daemon supports record-locking operations on NFS files. The <command>lockd</command> daemon manages RPC connections between the client and the server
for the Network Lock Manager (NLM) protocol. The daemon is normally started
without any options. You can use three options with this command. See the <olink targetdoc="refman1m" targetptr="lockd-1m" remap="external"><citerefentry><refentrytitle>lockd</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> man page. These options can
either be used from the command line or by editing the appropriate string
in <filename>/etc/default/nfs</filename>. The following are descriptions of
keywords that can be set in the <filename>/etc/default/nfs</filename> file.</para><note><para>Starting in the Solaris 10 release, the <literal>LOCKD_GRACE_PERIOD</literal> keyword
and the <option>g</option> option have been deprecated. The deprecated keyword
is replaced with the new keyword <literal>GRACE_PERIOD</literal>.  If both
keywords are set, the value for <literal>GRACE_PERIOD</literal> overrides
the value for <literal>LOCKD_GRACE_PERIOD</literal>.  See the description
of <literal>GRACE_PERIOD</literal> that follows.</para>
</note><para>Like <literal>LOCKD_GRACE_PERIOD</literal>, <literal>GRACE_PERIOD</literal>=<replaceable>graceperiod</replaceable> in <filename>/etc/default/nfs</filename> sets the
number of seconds after a server reboot that the clients have to reclaim both
NFS version 3 locks, provided by NLM, and version 4 locks.  Thus, the value
for <literal>GRACE_PERIOD</literal> controls the length of the grace period
for lock recovery, for both NFS version 3 and NFS version 4.</para><para>The <literal>LOCKD_RETRANSMIT_TIMEOUT</literal>=<replaceable>timeout</replaceable> parameter
in <filename>/etc/default/nfs</filename> selects the number of seconds to
wait before retransmitting a lock request to the remote server. This option
affects the NFS client-side service. The default value for <replaceable>timeout</replaceable> is
15 seconds. Decreasing the <replaceable>timeout</replaceable> value can improve
response time for NFS clients on a &#8220;noisy&#8221; network. However, this
change can cause additional server load by increasing the frequency of lock
requests. The same parameter can be used from the command line by starting
the daemon with the <option>t</option> <replaceable>timeout</replaceable> option.</para><para>The <literal>LOCKD_SERVERS</literal>=<replaceable>nthreads</replaceable> parameter
in <filename>/etc/default/nfs</filename> specifies the maximum number of concurrent
threads that the server handles per connection. Base the value for <replaceable>nthreads</replaceable> on the load that is expected on the NFS server. The default
value is 20. Each NFS client that uses TCP uses a single connection with the
NFS server. Therefore, each client can use a maximum of 20 concurrent threads
on the server.</para><para>All NFS clients that use UDP share a single connection with the NFS
server. Under these conditions, you might have to increase the number of threads
that are available for the UDP connection. A minimum calculation would be
to allow two threads for each UDP client. However, this number is specific
to the workload on the client, so two threads per client might not be sufficient.
The disadvantage to using more threads is that when the threads are used,
more memory is used on the NFS server. If the threads are never used, however,
increasing <replaceable>nthreads</replaceable> has no effect. The same parameter
can be used from the command line by starting the daemon with the <option role="nodash">nthreads</option> option.</para>
</sect2><sect2 id="rfsrefer-10"><title><command>mountd</command> Daemon</title><para>This daemon handles file-system mount requests from remote systems and
provides access control. The <command>mountd</command> daemon checks <filename>/etc/dfs/sharetab</filename> to determine which file systems are available for remote mounting
and which systems are allowed to do the remote mounting. You can use the <option>v</option> option and the <option>r</option> option with this command. See
the <olink targetdoc="refman1m" targetptr="mountd-1m" remap="external"><citerefentry><refentrytitle>mountd</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> man
page.</para><para>The <option>v</option> option runs the command in verbose mode. Every
time an NFS server determines the access that a client should be granted,
a message is printed on the console. The information that is generated can
be useful when trying to determine why a client cannot access a file system.</para><para>The <option>r</option> option rejects all future mount requests from
clients. This option does not affect clients that already have a file system
mounted.</para><note><para>NFS version 4 does not use this daemon.</para>
</note>
</sect2><sect2 id="rfsrefer-144"><title><command>nfs4cbd</command> Daemon</title><para><command>nfs4cbd</command>, which is for the exclusive use of the NFS
version 4 client, manages the communication endpoints for the NFS version
4 callback program. The daemon has no user-accessible interface. For more
information, see the <olink targetdoc="refman1m" targetptr="nfs4cbd-1m" remap="external"><citerefentry><refentrytitle>nfs4cbd</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> man
page.</para>
</sect2><sect2 id="rfsrefer-11"><title><command>nfsd</command> Daemon</title><para>This daemon handles other client file-system requests. You can use several
options with this command. See the <olink targetdoc="refman1m" targetptr="nfsd-1m" remap="external"><citerefentry><refentrytitle>nfsd</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> man page for a complete listing. These
options can either be used from the command line or by editing the appropriate
string in <filename>/etc/default/nfs</filename>.</para><para>The <literal>NFSD_LISTEN_BACKLOG</literal>=<replaceable>length</replaceable> parameter
in <filename>/etc/default/nfs</filename> sets the length of the connection
queue over connection-oriented transports for NFS and TCP. The default value
is 32 entries. The same selection can be made from the command line by starting <command>nfsd</command> with the <option>l</option> option.</para><para>The <literal>NFSD_MAX_CONNECTIONS</literal>=<replaceable>#-conn</replaceable> parameter
in <filename>/etc/default/nfs</filename> selects the maximum number of connections
per connection-oriented transport. The default value for <emphasis>#-conn</emphasis> is unlimited.
The same parameter can be used from the command line by starting the daemon
with the <option>c</option> <replaceable>#-conn</replaceable> option.</para><para>The <literal>NFSD_SERVER</literal>=<replaceable>nservers</replaceable> parameter
in <filename>/etc/default/nfs</filename> selects the maximum number of concurrent
requests that a server can handle. The default value for <replaceable>nservers</replaceable> is
16. The same selection can be made from the command line by starting <command>nfsd</command> with the <option role="nodash">nservers</option> option.</para><para>Unlike older versions of this daemon, <command>nfsd</command> does not
spawn multiple copies to handle concurrent requests. Checking the process
table with <command>ps</command> only shows one copy of the daemon running.</para>
</sect2><sect2 id="rfsrefer-7"><title><command>nfslogd</command> Daemon</title><para>This daemon provides operational logging. NFS operations that are logged
against a server are based on the configuration options that are defined in <filename>/etc/default/nfslogd</filename>. When NFS server logging is enabled, records
of all RPC operations on a selected file system are written to a buffer file
by the kernel. Then <command>nfslogd</command> postprocesses these requests.
The name service switch is used to help map UIDs to logins and IP addresses
to host names. The number is recorded if no match can be found through the
identified name services. </para><para>Mapping of file handles to path names is also handled by <command>nfslogd</command>.
The daemon tracks these mappings in a file-handle-to-path mapping table. One
mapping table exists for each tag that is identified in <filename>/etc/nfs/nfslogd</filename>. After post-processing, the records are written to ASCII log files.</para><note><para>NFS version 4 does not use this daemon.</para>
</note>
</sect2><sect2 id="rfsrefer-118"><title><command>nfsmapid</command> Daemon</title><para>Version 4 of the NFS protocol (RFC3530) changed the way user or group
 identifiers (UID or GID) are exchanged between the client and server. The
 protocol requires that a file's owner and group attributes be exchanged between
 an NFS version 4 client and an NFS version 4 server as strings in the form
of  <literal>user@nfsv4_domain</literal> or <literal>group@nfsv4_domain</literal>,
respectively.</para><para>For example, user <literal>known_user</literal> has a UID 123456 on
an NFS version 4 client whose fully qualified hostname is <literal>system.example.com</literal>. For the client to make requests to the NFS version 4 server, the
client must map the UID 123456 to <literal>known_user@example.com</literal> and
then send this attribute to the NFS version 4 server. The NFS version 4 server
expects to receive user and group file attributes in the <literal>user_or_group@nfsv4_domain</literal> format. After the server receives <literal>known_user@example.com</literal> from
the client, the server maps the string to the local UID 123456, which is understood
by the underlying file system. This functionality assumes that every UID and
GID in the network is unique and that the NFS version 4 domains on the client
match the NFS version 4 domains on the server.</para><note><para>If the server does not recognize the given user or group name,
even if  the NFS version 4 domains match, the server is unable to map the
user or group name to its unique ID, an integer value. Under such circumstances,
the server maps the inbound user or group name to the <literal>nobody</literal> user.
To prevent such occurrences, administrators should avoid making special accounts
that only exist on the NFS version 4 client.</para>
</note><para>The NFS version 4 client and server are both capable of performing integer-to-string
 and string-to-integer conversions. For example, in response to a GETATTR
operation, the NFS version 4 server maps UIDs and GIDs obtained from the underlying
file system into their respective string representation and sends this information
to the client. Alternately, the client must also map UIDs and GIDs into string
representations. For example, in response to the <command>chown</command> command,
the client maps the new UID or GID to a string representation before sending
a  SETATTR operation to the server.</para><itemizedlist><para>Note, however, that the client and server respond differently to unrecognized
 strings:</para><listitem><para>If the user does not exist on the server, even within the
same NFS version 4 domain configuration, the server rejects the remote procedure
call (RPC) and returns an error message to the client. This situation limits
the operations that can be performed by the remote user.</para>
</listitem><listitem><para>If the user exists on both the client and server, but they
have mismatched domains, the server rejects the attribute modifying operations
(such as SETATTR) that require the server to map the inbound user string to
an integer   value that the underlying file system can understand. For NFS
version 4 clients and servers to function properly, their NFS version 4 domains,
the portion of the string after the <literal>@</literal> sign, should match.</para>
</listitem><listitem><para>If the NFS version 4 client does not recognize a user or group
name from the server, the client is unable to map the string to its unique
ID, an integer value. Under such circumstances, the client maps the inbound
user or group string to the <literal>nobody</literal> user. This mapping to <literal>nobody</literal> creates varied problems for different applications. As for
NFS version 4 functionality, operations that modify file attributes will fail.</para>
</listitem>
</itemizedlist><sect3 id="epubq"><title>Configuration Files and <command>nfsmapid</command></title><itemizedlist><para>The following describes how the <command>nfsmapid</command> daemon uses
the <filename>/etc/nsswitch.conf</filename> and <filename>/etc/resolv.conf</filename> files:</para><listitem><para><command>nfsmapid</command> uses standard C library functions
to request password and group information from back-end name services. These
name services are controlled by the settings in the <filename>/etc/nsswitch.conf</filename> file.
Any changes to the <filename>nsswitch.conf</filename> file affect <command>nfsmapid</command> operations. For more information about the <filename>nsswitch.conf</filename> file,
see the <olink targetdoc="refman4" targetptr="nsswitch.conf-4" remap="external"><citerefentry><refentrytitle>nsswitch.conf</refentrytitle><manvolnum>4</manvolnum></citerefentry></olink> man page.</para>
</listitem><listitem><para>To ensure that the NFS version 4 clients are capable of mounting
file systems from different domains, <command>nfsmapid</command> relies on
the configuration of the DNS TXT resource record (RR), <literal>_nfsv4idmapdomain</literal>. For more information about configuring the <literal>_nfsv4idmapdomain</literal> resource record, see <olink targetptr="epubp" remap="internal">nfsmapid and DNS
TXT Records</olink>. Also, note the following:</para><itemizedlist><listitem><para>The DNS TXT RR should be explicitly configured on the DNS
server with the desired domain information.</para>
</listitem><listitem><para>The <filename>/etc/resolv.conf</filename> file should be configured
with the desired parameters to enable the <command>resolver</command> to find
the DNS server and search the TXT records for client and server NFS version
4 domains.</para>
</listitem>
</itemizedlist><itemizedlist><para>For more information, see the following:</para><listitem><para><olink targetptr="epubt" remap="internal">Precedence Rules</olink></para>
</listitem><listitem><para><olink targetptr="gbvgf" remap="internal">Configuring the NFS Version 4 Default
Domain</olink></para>
</listitem><listitem><para><olink targetdoc="refman4" targetptr="resolv.conf-4" remap="external"><citerefentry><refentrytitle>resolv.conf</refentrytitle><manvolnum>4</manvolnum></citerefentry></olink> man page</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</sect3><sect3 id="epubt"><title>Precedence Rules</title><orderedlist><para>For <command>nfsmapid</command> to work properly, NFS version 4 clients
and servers must have the same domain. To ensure matching NFS version 4 domains, <command>nfsmapid</command> follows these strict precedence rules:</para><listitem><para>The daemon first checks the <filename>/etc/default/nfs</filename> file
for a value that has been assigned to the NFSMAPID_DOMAIN keyword. If a value
is found, the assigned value takes precedence over any other settings. The
assigned value is appended to the outbound attribute strings and is compared
against inbound attribute strings. For more information about keywords in
the <filename>/etc/default/nfs</filename> file, see <olink targetptr="rfsrefer-133" remap="internal">Keywords for the /etc/default/nfs File</olink>. For
procedural information, see <olink targetptr="rfsadmin-68" remap="internal">Setting Up NFS
Services</olink>.</para><note><para>The use of the NFSMAPID_DOMAIN setting is not scalable and is
not recommended for large deployments.</para>
</note>
</listitem><listitem><para>If no value has been assigned to NFSMAPID_DOMAIN, then the
daemon checks for a domain name from a DNS TXT RR. <command>nfsmapid</command> relies
on directives in the <filename>/etc/resolv.conf</filename> file that are used
by the set of routines in the <command>resolver</command>. The <command>resolver</command> searches
through the configured DNS servers for the <literal>_nfsv4idmapdomain</literal> TXT
RR. Note that the use of DNS TXT records is more scalable. For this reason,
continued use of TXT records is much preferred over setting the keyword in
the <filename>/etc/default/nfs</filename> file.</para>
</listitem><listitem><para>If no DNS TXT record provides a domain name, then by default
the <command>nfsmapid</command> daemon uses the configured DNS domain.</para>
</listitem><listitem><para>If the <filename>/etc/resolv.conf</filename> file does not
exist, <command>nfsmapid</command> obtains the NFS version 4 domain name by
following the behavior of the <command>domainname</command> command. Specifically,
if the <filename>/etc/defaultdomain</filename> file exists, <command>nfsmapid</command> uses
the contents of that file for the NFS version 4 domain. If the <filename>/etc/defaultdomain</filename> file does not exist, <command>nfsmapid</command> uses the domain
name that is provided by the network's configured naming service. For more
information, see the <olink targetdoc="refman1m" targetptr="domainname-1m" remap="external"><citerefentry><refentrytitle>domainname</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> man page.</para>
</listitem>
</orderedlist>
</sect3><sect3 id="epubp"><title><command>nfsmapid</command> and DNS TXT Records</title><para>The ubiquitous nature of DNS provides an efficient storage and distribution
mechanism for the NFS version 4 domain name. Additionally, because of the
inherent scalability of DNS, the use of DNS TXT resource records is the preferred
method for configuring the NFS version 4 domain name for large deployments.
You should configure the <literal>_nfsv4idmapdomain</literal> TXT record on
enterprise-level DNS servers. Such configurations ensure that any NFS version
4 client or server can find its NFS version 4 domain by traversing the DNS
tree.</para><para>The following is an example of a preferred entry for enabling the DNS
server to provide the NFS version 4 domain name:</para><screen>_nfsv4idmapdomain		IN		TXT			"foo.bar"</screen><para>In this example, the domain name to configure is the value that is enclosed
in  double-quotes. Note that no <literal>ttl</literal> field is specified
and that no domain is appended to <literal>_nfsv4idmapdomain</literal>, which
is the value in the <literal>owner</literal> field. This configuration enables
the TXT record to use the zone's <literal>${ORIGIN}</literal> entry from the
Start-Of-Authority (SOA) record. For example, at different levels of the domain
namespace, the record could read as follows:</para><screen>_nfsv4idmapdomain.subnet.yourcorp.com.    IN    TXT    "foo.bar"
_nfsv4idmapdomain.yourcorp.com.           IN    TXT    "foo.bar"</screen><para>This configuration provides DNS clients with the added flexibility of
using the <filename>resolv.conf</filename> file to search up the DNS tree
hierarchy. See the <olink targetdoc="refman4" targetptr="resolv.conf-4" remap="external"><citerefentry><refentrytitle>resolv.conf</refentrytitle><manvolnum>4</manvolnum></citerefentry></olink> man page. This capability provides a higher probability
of finding the TXT record. For even more flexibility, lower level DNS sub-domains
can define their own DNS TXT resource  records (RRs). This capability enables
lower level DNS sub-domains to override the TXT record that is defined by
the top level DNS domain.</para><note><para>The domain that is specified by the TXT record can be an arbitrary
string that does not necessarily match the DNS domain for clients and servers
that use NFS version 4. You have the option of not sharing NFS version 4 data
with other DNS domains.</para>
</note>
</sect3><sect3 id="gcpfe"><title>Checking for the NFS Version 4 Domain</title><para>Before assigning a value for your network's NFS version 4 domain, check
to see if an NFS version 4 domain has already been configured for your network.
The  following examples provide ways of identifying your network's NFS version
4 domain.</para><itemizedlist><listitem><para>To identify the NFS version 4 domain from a DNS TXT RR, use
either the <command>nslookup</command> or the <command>dig</command> command:</para><para>The following provides sample output for the <command>nslookup</command> command:</para><screen># <userinput>nslookup -q=txt _nfsv4idmapdomain</userinput>
Server:         10.255.255.255
Address:        10.255.255.255#53

_nfsv4idmapdomain.example.company.com text = "company.com"</screen><para>See this sample output for the <command>dig</command> command:</para><screen># <userinput>dig +domain=<replaceable>example.company.com</replaceable> -t TXT _nfsv4idmapdomain</userinput>
...
;; QUESTION SECTION:
;_nfsv4idmapdomain.example.company.com. IN    TXT

;; ANSWER SECTION:
_nfsv4idmapdomain.example.company.com. 21600 IN TXT   "company.com"

;; AUTHORITY SECTION:
...</screen><para>For information about setting up a DNS TXT RR, see <olink targetptr="epubp" remap="internal">nfsmapid and DNS TXT Records</olink>.</para>
</listitem><listitem><para>If your network is not setup with a NFS version 4 DNS TXT
RR, use the following command to identify your NFS version 4 domain from the
DNS domain name:</para><screen># <userinput>egrep domain /etc/resolv.conf</userinput>
domain example.company.com</screen>
</listitem><listitem><para>If the <filename>/etc/resolv.conf</filename> file is not configured
to provide a DNS domain name for the client, use the following command to
identify the domain from the network's NFS version 4 domain configuration:</para><screen># <userinput>cat /var/run/nfs4_domain</userinput>
company.com</screen>
</listitem><listitem><para>If you are using a different naming service, such as NIS,
use the following command to identify the domain for the naming service configured
for your network:</para><screen><userinput># domainname</userinput>
it.example.company.com</screen>
</listitem>
</itemizedlist><itemizedlist><para>For more information, see the following man pages:</para><listitem><para><olink targetdoc="refman1m" targetptr="nslookup-1m" remap="external"><citerefentry><refentrytitle>nslookup</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink></para>
</listitem><listitem><para><olink targetdoc="refman1m" targetptr="dig-1m" remap="external"><citerefentry><refentrytitle>dig</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink></para>
</listitem><listitem><para><olink targetdoc="refman4" targetptr="resolv.conf-4" remap="external"><citerefentry><refentrytitle>resolv.conf</refentrytitle><manvolnum>4</manvolnum></citerefentry></olink></para>
</listitem><listitem><para><olink targetdoc="refman1m" targetptr="domainname-1m" remap="external"><citerefentry><refentrytitle>domainname</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink></para>
</listitem>
</itemizedlist>
</sect3><sect3 id="gbvgf"><title>Configuring the NFS Version 4 Default Domain</title><itemizedlist><para>This section describes how the network obtains the desired default domain:</para><listitem><para>For the Solaris Express 5/06 release, see <olink targetptr="gbvgj" remap="internal">Configuring an NFS Version 4 Default Domain in the Solaris
Express 5/06 Release</olink>.</para>
</listitem><listitem><para>For the initial Solaris 10 release, see <olink targetptr="gbvgu" remap="internal">Configuring an NFS Version 4 Default Domain in the Solaris 10 Release</olink>.</para>
</listitem>
</itemizedlist><sect4 id="gbvgj"><title>Configuring an NFS Version 4 Default Domain in the
Solaris Express 5/06 Release</title><itemizedlist><para>In the initial Solaris 10 release, the domain was defined during the
first system reboot after installing the OS. In the Solaris Express 5/06 release,
the NFS version 4 domain is defined during the installation of the OS. To
provide this functionality, the following features have been added: </para><listitem><para>The <command>sysidtool</command> command includes the <command>sysidnfs4</command> program. This program runs during the installation process to determine
whether an NFS version 4 domain has been configured for the network. See the
man pages for <olink targetdoc="refman1m" targetptr="sysidtool-1m" remap="external"><citerefentry><refentrytitle>sysidtool</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> and <olink targetdoc="refman1m" targetptr="sysidnfs4-1m" remap="external"><citerefentry><refentrytitle>sysidnfs4</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink>.</para>
</listitem><listitem><para>The <filename>sysidcfg</filename> file has a new keyword, <literal>nfs4_domain</literal>. This keyword can be used to define the NFS version
4 domain. Note that other keywords can also be defined in the <filename>sysidcfg</filename> file.
See the <olink targetdoc="refman4" targetptr="sysidcfg-4" remap="external"><citerefentry><refentrytitle>sysidcfg</refentrytitle><manvolnum>4</manvolnum></citerefentry></olink> man
page.</para>
</listitem>
</itemizedlist><orderedlist><para>The following describes how the functionality operates:</para><listitem><para>The <command>sysidnfs4</command> program checks the <filename>/etc/.sysIDtool.state</filename> file to determine whether an NFS version 4 domain has been identified.</para><itemizedlist><listitem><para>If the <filename>.sysIDtool.state</filename> file shows that
an NFS version 4 domain has been configured for the network, the <command>sysidnfs4</command> program makes no further checks. See the following example of a <filename>.sysIDtool.state</filename> file:</para><screen>1       # System previously configured?
1       # Bootparams succeeded?
1       # System is on a network?
1       # Extended network information gathered?
1       # Autobinder succeeded?
1       # Network has subnets?
1       # root password prompted for?
1       # locale and term prompted for?
1       # security policy in place
1       # NFSv4 domain configured
xterms</screen><para>The <literal>1</literal> that appears before <literal># NFSv4 domain
configured</literal> confirms that the NFS version 4 domain has been configured.</para>
</listitem><listitem><para>If the <filename>.sysIDtool.state</filename> file shows that
no NFS version 4 domain has been configured for the network, the <command>sysidnfs4</command> program must make further checks. See the following example of
a <filename>.sysIDtool.state</filename> file:</para><screen>1       # System previously configured?
1       # Bootparams succeeded?
1       # System is on a network?
1       # Extended network information gathered?
1       # Autobinder succeeded?
1       # Network has subnets?
1       # root password prompted for?
1       # locale and term prompted for?
1       # security policy in place
0       # NFSv4 domain configured
xterms</screen><para>The <literal>0</literal> that appears before <literal># NFSv4 domain
configured</literal> confirms that no NFS version 4 domain has been configured.</para>
</listitem>
</itemizedlist>
</listitem><listitem><para>If no NFS version 4 domain has been identified, the <command>sysidnfs4</command> program checks the <literal>nfs4_domain</literal> keyword in the <filename>sysidcfg</filename> file.</para><itemizedlist><listitem><para>If a value for <literal>nfs4_domain</literal> exists, that
value is assigned to the NFSMAPID_DOMAIN keyword in the <filename>/etc/default/nfs</filename> file. Note that any value assigned to NFSMAPID_DOMAIN overrides
the dynamic domain selection capability of the <command>nfsmapid</command> daemon.
For more information about the dynamic domain selection capability of <command>nfsmapid</command>, see <olink targetptr="epubt" remap="internal">Precedence Rules</olink>.</para>
</listitem><listitem><para>If no value for <literal>nfs4_domain</literal> exists, the <command>sysidnfs4</command> program identifies the domain that <command>nfsmapid</command> derives
from the operating system's configured name services. This derived value is
presented as a default domain at an interactive prompt that gives you the
option of accepting the default value or assigning a different NFS version
4 domain.</para>
</listitem>
</itemizedlist>
</listitem>
</orderedlist><itemizedlist><para>This functionality makes the following obsolete:</para><listitem><para>The sample <trademark>JumpStart</trademark> script, <filename>set_nfs4_domain</filename>, which was provided in the initial Solaris 10 media distribution
is no longer required and is discouraged.</para>
</listitem><listitem><para>The <filename>/etc/.NFS4inst_state.domain</filename> file,
which was created by the previous implementation of the <command>sysidnfs4</command> program,
is no longer required.</para>
</listitem>
</itemizedlist><note><para>Because of the inherent ubiquitous and scalable nature of DNS,
the use of DNS TXT records for configuring the domain of large NFS version
4 deployments continues to be preferred and strongly encouraged. See <olink targetptr="epubp" remap="internal">nfsmapid and DNS TXT Records</olink>.</para>
</note><itemizedlist><para>For specific information about the Solaris installation process, see
the following:</para><listitem><para><olink targetdoc="solarisinstall" remap="external"><citetitle remap="book">Solaris Express Installation Guide: Basic Installations</citetitle></olink></para>
</listitem><listitem><para><olink targetdoc="solinstallnet" remap="external"><citetitle remap="book">Solaris Express Installation Guide: Network-Based Installations</citetitle></olink></para>
</listitem>
</itemizedlist>
</sect4><sect4 id="gbvgu"><title>Configuring an NFS Version 4 Default Domain in the
Solaris 10 Release</title><para>In the initial Solaris 10 release of NFS version 4, if your network
includes multiple DNS domains, but only has a single UID and GID namespace,
all clients must use one value for NFSMAPID_DOMAIN.  For sites that use DNS, <command>nfsmapid</command> resolves this issue by obtaining  the domain name from
the value that you assigned to <literal>_nfsv4idmapdomain</literal>.  For
more information, see <olink targetptr="epubp" remap="internal">nfsmapid and DNS TXT Records</olink>.
If your  network is not configured to use DNS, during the first system boot
the Solaris OS uses the <olink targetdoc="refman1m" targetptr="sysidconfig-1m" remap="external"><citerefentry><refentrytitle>sysidconfig</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> utility to provide the  following prompts for an NFS
version 4 domain name:</para><screen>This system is configured with NFS version 4, which uses a 
domain name that is automatically derived from the system's 
name services. The derived domain name is sufficient for most 
configurations. In a few cases, mounts that cross different 
domains might cause files to be owned by <literal>nobody</literal> due to the 
lack of a common domain name.

Do you need to override the system's default NFS verion 4 domain 
name (yes/no)? [<replaceable>no</replaceable>]</screen><para>The default response is <literal>[no]</literal>. If you choose <literal>[no]</literal>, you see the following:</para><screen>For more information about how the NFS version 4 default domain name is 
derived and its impact, refer to the man pages for <command>nfsmapid(1M)</command> and 
<command>nfs(4)</command>, and the System Administration Guide: Network Services.</screen><para>If you choose <literal>[yes]</literal>, you see this prompt:</para><screen>Enter the domain to be used as the NFS version 4 domain name.
NFS version 4 domain name []:</screen><note><para>If a value for NFSMAPID_DOMAIN exists in <filename>/etc/default/nfs</filename>,
the <literal>[domain_name]</literal> that you provide overrides that value.</para>
</note>
</sect4>
</sect3><sect3 id="feoos"><title>Additional Information About <command>nfsmapid</command></title><itemizedlist><para>For more information about <command>nfsmapid</command>, see the following:</para><listitem><para><olink targetdoc="refman1m" targetptr="nfsmapid-1m" remap="external"><citerefentry><refentrytitle>nfsmapid</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> man
page</para>
</listitem><listitem><para><olink targetdoc="refman4" targetptr="nfs-4" remap="external"><citerefentry><refentrytitle>nfs</refentrytitle><manvolnum>4</manvolnum></citerefentry></olink> man
page</para>
</listitem><listitem><para><ulink type="url" url="http://www.ietf.org/rfc/rfc1464.txt">http://www.ietf.org/rfc/rfc1464.txt</ulink></para>
</listitem><listitem><para><olink targetptr="gande" remap="internal">ACLs and nfsmapid in NFS Version
4</olink></para>
</listitem>
</itemizedlist>
</sect3>
</sect2><sect2 id="rfsrefer-12"><title><command>statd</command> Daemon</title><para>This daemon works with <command>lockd</command> to provide crash and
recovery functions for the lock manager. The <command>statd</command> daemon
tracks the clients that hold locks on an NFS server. If a server crashes,
on rebooting <command>statd</command> on the server contacts <command>statd</command> on
the client. The client <command>statd</command> can then attempt to reclaim
any locks on the server. The client <command>statd</command> also informs
the server <command>statd</command> when a client has crashed so that the
client's locks on the server can be cleared. You have no options to select
with this daemon. For more information, see the <olink targetdoc="refman1m" targetptr="statd-1m" remap="external"><citerefentry><refentrytitle>statd</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> man page. </para><para>In the Solaris 7 release, the way that <command>statd</command> tracks
the clients has been improved. In all earlier Solaris releases, <command>statd</command> created
files in <literal>/var/statmon/sm</literal> for each client by using the client's
unqualified host name. This file naming caused problems if you had two clients
in different domains that shared a host name, or if clients were not resident
in the same domain as the NFS server. Because the unqualified host name only
lists the host name, without any domain or IP-address information, the older
version of <command>statd</command> had no way to differentiate between these
types of clients. To fix this problem, the Solaris 7 <command>statd</command> creates
a symbolic link in <literal>/var/statmon/sm</literal> to the unqualified host
name by using the IP address of the client. The new link resembles the following:</para><screen># ls -l /var/statmon/sm
lrwxrwxrwx   1 daemon          11 Apr 29 16:32 ipv4.192.168.255.255 -> myhost
lrwxrwxrwx   1 daemon          11 Apr 29 16:32 ipv6.fec0::56:a00:20ff:feb9:2734 -> v6host
--w-------   1 daemon          11 Apr 29 16:32 myhost
--w-------   1 daemon          11 Apr 29 16:32 v6host</screen><para>In this example, the client host name is <literal>myhost</literal> and
the client's IP address is <literal>192.168.255.255</literal>. If another
host with the name <literal>myhost</literal> were mounting a file system,
two symbolic links would lead to the host name.</para><note><para>NFS version 4 does not use this daemon.</para>
</note>
</sect2>
</sect1><sect1 id="rfsrefer-13"><title>NFS Commands</title><itemizedlist><para>These commands must be run as <literal>root</literal> to be fully
effective, but requests for information can be made by all users: </para><listitem><para><olink targetptr="rfsrefer-64" remap="internal">automount Command</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-14" remap="internal">clear_locks Command</olink></para>
</listitem><listitem><para><olink targetptr="gcays" remap="internal">fsstat Command</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-15" remap="internal">mount Command</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-21" remap="internal">mountall Command</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-36" remap="internal">setmnt Command</olink></para>
</listitem><listitem><para><olink targetptr="gcrvu" remap="internal">sharemgr Command</olink></para>
</listitem><listitem><para><olink targetptr="gecpc" remap="internal">sharectl Command</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-25" remap="internal">share Command</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-30" remap="internal">shareall Command</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-34" remap="internal">showmount Command</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-19" remap="internal">umount Command</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-23" remap="internal">umountall Command</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-28" remap="internal">unshare Command</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-32" remap="internal">unshareall Command</olink></para>
</listitem>
</itemizedlist><sect2 id="rfsrefer-64"><title><command>automount</command> Command</title><para>This command installs autofs mount points and associates the information
in the automaster files with each mount point. The syntax of the command is
as follows:</para><para><command>automount</command> <literal>[</literal> <option>t</option> <replaceable>duration</replaceable> <literal>]</literal> <literal>[</literal> <option>v</option> <literal>]</literal></para><para><option>t</option> <replaceable>duration</replaceable> sets the time,
in seconds, that a file system is to remain mounted, and <option>v</option> selects
the verbose mode. Running this command in the verbose mode allows for easier
troubleshooting.</para><para>If not specifically set, the value for duration is set to 5 minutes.
In most circumstances, this value is good. However, on systems that have many
automounted file systems, you might need to increase the duration value. In
particular, if a server has many users active, checking the automounted file
systems every 5 minutes can be inefficient. Checking the autofs file systems
every 1800 seconds, which is 30 minutes, could be more optimal. By not unmounting
the file systems every 5 minutes, <filename>/etc/mnttab</filename> can become
large. To reduce the output when <command>df</command> checks each entry in <filename>/etc/mnttab</filename>, you can filter the output from <command>df</command> by
using the <option>F</option> option (see the <olink targetdoc="refman1m" targetptr="df-1m" remap="external"><citerefentry><refentrytitle>df</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> man page) or by using <command>egrep</command>.</para><para>You should consider that adjusting the duration also changes how quickly
changes to the automounter maps are reflected. Changes cannot be seen until
the file system is unmounted. Refer to <olink targetptr="rfsadmin-132" remap="internal">Modifying
the Maps</olink> for instructions on how to modify automounter maps.</para>
</sect2><sect2 id="rfsrefer-14"><title><command>clear_locks</command> Command</title><para>This command enables you to remove all file, record, and share locks
for an NFS client. You must be <literal>root</literal> to run this command.
From an NFS server, you can clear the locks for a specific client. From an
NFS client, you can clear locks for that client on a specific server. The
following example would clear the locks for the NFS client that is named <literal>tulip</literal> on the current system. </para><screen># <userinput>clear_locks tulip</userinput></screen><para>Using the <option>s</option> option enables you to specify which NFS
host to clear the locks from. You must run this option from the NFS client,
which created the locks. In this situation, the locks from the client would
be removed from the NFS server that is named <literal>bee</literal>.</para><screen># <userinput>clear_locks -s bee</userinput></screen><caution><para>This command should only be run when a client crashes and cannot
clear its locks. To avoid data corruption problems, do not clear locks for
an active client.</para>
</caution>
</sect2><sect2 id="gcays"><title><command>fsstat</command> Command</title><para>Starting in the Solaris 10 11/06 release, the <command>fsstat</command> utility
enables you to monitor file system operations by file system type and by mount
point. Various options allow you to customize the output. See the following
examples.</para><para>This example shows output for NFS version 3, version 4, and the <filename>root</filename> mount point.</para><screen>% fsstat nfs3 nfs4 /
  new     name   name    attr    attr   lookup   rddir   read   read   write   write
 file    remov   chng     get     set      ops     ops    ops  bytes     ops   bytes
3.81K       90  3.65K   5.89M   11.9K    35.5M   26.6K   109K   118M   35.0K   8.16G  nfs3
  759      503    457   93.6K   1.44K     454K   8.82K  65.4K   827M     292    223K  nfs4
25.2K    18.1K  1.12K   54.7M    1017     259M   1.76M  22.4M  20.1G   1.43M   3.77G  /</screen><para>This example uses the <option>i</option> option to provide statistics
about the I/O operations for NFS version 3,  version 4, and the <filename>root</filename> mount
point.</para><screen>% fsstat -i nfs3 nfs4 /
 read    read    write   write   rddir   rddir   rwlock   rwulock
  ops   bytes      ops   bytes     ops   bytes      ops       ops
 109K    118M    35.0K   8.16G   26.6K   4.45M     170K      170K  nfs3
65.4K    827M      292    223K   8.82K   2.62M    74.1K     74.1K  nfs4
22.4M   20.1G    1.43M   3.77G   1.76M   3.29G    25.5M     25.5M  /</screen><para>This example uses the <option>n</option> option to provide statistics
about the naming operations for NFS version 3, version 4, and the <filename>root</filename> mount
point.</para><screen>% fsstat -n nfs3 nfs4 /
lookup   creat   remov  link   renam  mkdir  rmdir   rddir  symlnk  rdlnk
 35.5M   3.79K      90     2   3.64K      5      0   26.6K      11   136K  nfs3
  454K     403     503     0     101      0      0   8.82K     356  1.20K  nfs4
  259M   25.2K   18.1K   114    1017     10      2   1.76M      12  8.23M  /</screen><para>For more information, see the <olink targetdoc="refman" targetptr="fsstat-1m" remap="external"><citerefentry><refentrytitle>fsstat</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> man page.</para>
</sect2><sect2 id="rfsrefer-15"><title><command>mount</command> Command</title><para>With this command, you can attach a named file system, either local
or remote, to a specified mount point. For more information, see the <olink targetdoc="refman1m" targetptr="mount-1m" remap="external"><citerefentry><refentrytitle>mount</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> man page. Used without arguments, <command>mount</command> displays a list of file systems that are currently mounted
on your computer. </para><para>Many types of file systems are included in the standard Solaris installation.
Each file-system type has a specific man page that lists the options to <command>mount</command> that are appropriate for that file-system type. The man page
for NFS file systems is <olink targetdoc="refman1m" targetptr="mount-nfs-1m" remap="external"><citerefentry><refentrytitle>mount_nfs</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink>.
For UFS file systems, see <olink targetdoc="refman1m" targetptr="mount-ufs-1m" remap="external"><citerefentry><refentrytitle>mount_ufs</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink>. </para><para>The Solaris 7 release includes the ability to select a path name to
mount from an NFS server by using an NFS URL instead of the standard <literal>server:/pathname</literal> syntax. See <olink targetptr="rfsadmin-92" remap="internal">How to Mount an NFS
File System Using an NFS URL</olink> for further information.</para><caution><para>The version of the <command>mount</command> command that is
included in any Solaris release from 2.6 to the current release does not warn
about invalid options. The command silently ignores any options that cannot
be interpreted. Ensure that you verify all of the options that were used so
that you can prevent unexpected behavior.</para>
</caution><sect3 id="rfsrefer-16"><title><filename>mount</filename> Options for NFS
File Systems</title><para>The subsequent text lists some of the options that can follow the <option>o</option> flag when you are mounting an NFS file system. For a complete list
of options, refer to the <olink targetdoc="refman1m" targetptr="mount-nfs-1m" remap="external"><citerefentry><refentrytitle>mount_nfs</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> man
page. </para><variablelist termlength="wholeline"><varlistentry><term>bg|fg</term><listitem><para>These options can be used to select the retry behavior if a mount
fails. The <option role="nodash">bg</option> option causes the mount attempts
to be run in the background. The <option role="nodash">fg</option> option
causes the mount attempt to be run in the foreground. The default is <option role="nodash">fg</option>, which is the best selection for file systems that
must be available. This option prevents further processing until the mount
is complete. <option role="nodash">bg</option> is a good selection for noncritical
file systems because the client can do other processing while waiting for
the mount request to be completed.      </para>
</listitem>
</varlistentry><varlistentry><term>forcedirectio</term><listitem><para>This option improves performance of large sequential data transfers.
Data is copied directly to a user buffer. No caching is performed in the kernel
on the client. This option is off by default. </para><para>Previously, all write requests were serialized by both the NFS client
and the NFS server. The NFS client has been modified to permit an application
to issue concurrent writes, as well as concurrent reads and writes, to a single
file. You can enable this functionality on the client by using the <option role="nodash">forcedirectio</option> mount option.  When you use this option,
you are enabling this functionality for all files within the mounted file
system. You could also enable this functionality on a single file on the client
by using the <function>directio</function> interface. Unless this functionality
has been enabled, writes to files are serialized. Also, if concurrent writes
or concurrent reads and writes are occurring, then POSIX semantics are no
longer being supported for that file.</para><para>For an example of how to use this option, refer to <olink targetptr="rfsrefer-18" remap="internal">Using the mount Command</olink>.</para>
</listitem>
</varlistentry><varlistentry><term>largefiles</term><listitem><para>With this option, you can access files that are larger than 2
Gbytes on a server that is running the Solaris 2.6 release. Whether a large
file can be accessed can only be controlled on the server, so this option
is silently ignored on NFS version 3 mounts. Starting with release 2.6, by
default, all UFS file systems are mounted with <option role="nodash">largefiles</option>.
For mounts that use the NFS version 2 protocol, the <option role="nodash">largefiles</option> option causes the mount to fail with an error.</para>
</listitem>
</varlistentry><varlistentry><term>nolargefiles</term><listitem><para>This option for UFS mounts guarantees that no large files can
exist on the file system. See the <olink targetdoc="refman1m" targetptr="mount-ufs-1m" remap="external"><citerefentry><refentrytitle>mount_ufs</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> man page. Because the existence
of large files can only be controlled on the NFS server, no option for <option role="nodash">nolargefiles</option> exists when using NFS mounts. Attempts
to NFS-mount a file system by using this option are rejected with an error.</para>
</listitem>
</varlistentry><varlistentry><term>nosuid|suid</term><listitem><para>Starting in the Solaris 10 release, the <option role="nodash">nosuid</option> option is the equivalent of specifying the <option role="nodash">nodevices</option> option with the <option role="nodash">nosetuid</option> option.
When the <option role="nodash">nodevices</option> option is specified, the
opening of device-special files on the mounted file system is disallowed.
When the <option role="nodash">nosetuid</option> option is specified, the <literal>setuid</literal> bit and <literal>setgid</literal> bit in binary files that
are located  in the file system are ignored. The processes run with the  privileges
of the user who executes the binary file.</para><para>The <option role="nodash">suid</option> option is the equivalent of
specifying the <option role="nodash">devices</option> option with the <option role="nodash">setuid</option> option. When the <option role="nodash">devices</option> option
is specified, the opening of device-special files on the mounted file system
is allowed. When the <option role="nodash">setuid</option> option is specified,
the <literal>setuid</literal> bit and the <literal>setgid</literal> bit in
binary files that are located in the file  system are honored by the kernel.</para><para>If neither option is specified, the default option is <option role="nodash">suid</option>, which provides the default behavior of  specifying the <option role="nodash">devices</option> option with the <option role="nodash">setuid</option> option.</para><para>The following table describes the effect of combining <option role="nodash">nosuid</option> or <option role="nodash">suid</option> with <option role="nodash">devices</option> or <option role="nodash">nodevices</option>, and <option role="nodash">setuid</option> or <option role="nodash">nosetuid</option>.
Note that in each combination of options, the most restrictive option determines
the behavior.</para><informaltable frame="topbot"><tgroup cols="4" colsep="0" rowsep="0"><colspec colname="colspec0" colwidth="25*"/><colspec colname="colspec1" colwidth="25*"/><colspec colname="colspec2" colwidth="25*"/><colspec colname="colspec3" colwidth="25*"/><thead><row rowsep="1"><entry><para>Behavior From the Combined Options</para>
</entry><entry><para>Option</para>
</entry><entry><para>Option</para>
</entry><entry><para>Option</para>
</entry>
</row>
</thead><tbody><row><entry><para>The equivalent of <option role="nodash">nosetuid</option> with <option role="nodash">nodevices</option></para>
</entry><entry><para><option role="nodash">nosuid</option></para>
</entry><entry><para><option role="nodash">nosetuid</option></para>
</entry><entry><para><option role="nodash">nodevices</option></para>
</entry>
</row><row><entry><para>The equivalent of <option role="nodash">nosetuid</option> with <option role="nodash">nodevices</option></para>
</entry><entry><para><option role="nodash">nosuid</option></para>
</entry><entry><para><option role="nodash">nosetuid</option></para>
</entry><entry><para><option role="nodash">devices</option></para>
</entry>
</row><row><entry><para>The equivalent of <option role="nodash">nosetuid</option> with <option role="nodash">nodevices</option></para>
</entry><entry><para><option role="nodash">nosuid</option></para>
</entry><entry><para><option role="nodash">setuid</option></para>
</entry><entry><para><option role="nodash">nodevices</option></para>
</entry>
</row><row><entry><para>The equivalent of <option role="nodash">nosetuid</option> with <option role="nodash">nodevices</option></para>
</entry><entry><para><option role="nodash">nosuid</option></para>
</entry><entry><para><option role="nodash">setuid</option></para>
</entry><entry><para><option role="nodash">devices</option></para>
</entry>
</row><row><entry><para>The equivalent of <option role="nodash">nosetuid</option> with <option role="nodash">nodevices</option></para>
</entry><entry><para><option role="nodash">suid</option></para>
</entry><entry><para><option role="nodash">nosetuid</option></para>
</entry><entry><para><option role="nodash">nodevices</option></para>
</entry>
</row><row><entry><para>The equivalent of <option role="nodash">nosetuid</option> with <option role="nodash">devices</option></para>
</entry><entry><para><option role="nodash">suid</option></para>
</entry><entry><para><option role="nodash">nosetuid</option></para>
</entry><entry><para><option role="nodash">devices</option></para>
</entry>
</row><row><entry><para>The equivalent of <option role="nodash">setuid</option> with <option role="nodash">nodevices</option></para>
</entry><entry><para><option role="nodash">suid</option></para>
</entry><entry><para><option role="nodash">setuid</option></para>
</entry><entry><para><option role="nodash">nodevices</option></para>
</entry>
</row><row><entry><para>The equivalent of <option role="nodash">setuid</option> with <option role="nodash">devices</option></para>
</entry><entry><para><option role="nodash">suid</option></para>
</entry><entry><para><option role="nodash">setuid</option></para>
</entry><entry><para><option role="nodash">devices</option></para>
</entry>
</row>
</tbody>
</tgroup>
</informaltable><para>The <option role="nodash">nosuid</option> option provides
additional security for NFS  clients that access potentially untrusted servers.
The  mounting of remote file systems with this option reduces  the chance
of privilege escalation through importing  untrusted devices or importing
untrusted <literal>setuid</literal> binary  files. All these options are available
in all Solaris file systems.</para>
</listitem>
</varlistentry><varlistentry><term>public</term><listitem><para>This option forces the use of the public file handle when contacting
the NFS server. If the public file handle is supported by the server, the
mounting operation is faster because the MOUNT protocol is not used. Also,
because the MOUNT protocol is not used, the public option allows mounting
to occur through a firewall.</para>
</listitem>
</varlistentry><varlistentry><term>rw|ro</term><listitem><para>The <option>rw</option> and <option>ro</option> options indicate
whether a file system is to be mounted read-write or read-only. The default
is read-write, which is the appropriate option for remote home directories,
mail-spooling directories, or other file systems that need to be changed by
users. The read-only option is appropriate for directories that should not
be changed by users. For example, shared copies of the man pages should not
be writable by users.      </para>
</listitem>
</varlistentry><varlistentry><term>sec=<replaceable>mode</replaceable></term><listitem><para>You can use this option to specify the authentication mechanism
to be used during the mount transaction. The value for <replaceable>mode</replaceable> can
be one of the following.</para><itemizedlist><listitem><para>Use <literal>krb5</literal> for Kerberos version 5 authentication
service.</para>
</listitem><listitem><para>Use <literal>krb5i</literal> for Kerberos version 5 with integrity.</para>
</listitem><listitem><para>Use <literal>krb5p</literal> for Kerberos version 5 with privacy.</para>
</listitem><listitem><para>Use <literal>none</literal> for no authentication.</para>
</listitem><listitem><para>Use <literal>dh</literal> for Diffie-Hellman (DH) authentication.</para>
</listitem><listitem><para>Use <literal>sys</literal> for standard UNIX authentication.</para>
</listitem>
</itemizedlist><para>The modes are also defined in <filename>/etc/nfssec.conf</filename>.</para>
</listitem>
</varlistentry><varlistentry><term>soft|hard</term><listitem><para>An NFS file system that is mounted with the <option role="nodash">soft</option> option returns an error if the server does not respond. The <option role="nodash">hard</option> option causes the mount to continue to retry until
the server responds. The default is <option role="nodash">hard</option>, which
should be used for most file systems. Applications frequently do not check
return values from <option role="nodash">soft</option>-mounted file systems,
which can make the application fail or can lead to corrupted files. If the
application does check the return values, routing problems and other conditions
can still confuse the application or lead to file corruption if the <option role="nodash">soft</option> option is used. In most situations, the <option role="nodash">soft</option> option should not be used. If a file system is
mounted by using the <option role="nodash">hard</option> option and becomes
unavailable, an application that uses this file system hangs until the file
system becomes available.   </para>
</listitem>
</varlistentry>
</variablelist>
</sect3><sect3 id="rfsrefer-18"><title>Using the <command>mount</command> Command</title><para>Refer to the following examples.</para><itemizedlist><listitem><para>In NFS version 2 or version 3, both of these commands mount an
NFS file system from the server <filename>bee</filename> read-only.</para><screen># <userinput>mount -F nfs -r bee:/export/share/man /usr/man</userinput></screen><screen># <userinput>mount -F nfs -o ro bee:/export/share/man /usr/man</userinput></screen><para>In NFS version 4, the following command line would accomplish the same
mount.</para><screen># <userinput>mount -F nfs -o vers=4 -r bee:/export/share/man /usr/man</userinput></screen>
</listitem><listitem><para>In NFS version 2 or version 3, this command uses the <option>O</option> option
to force the man pages from the server <literal>bee</literal> to be mounted
on the local system even if <filename>/usr/man</filename> has already been
mounted. See the following.</para><screen># <userinput>mount -F nfs -O bee:/export/share/man /usr/man</userinput></screen><para>In NFS version 4, the following command line would accomplish the same
mount.</para><screen># <userinput>mount -F nfs -o vers=4 -O bee:/export/share/man /usr/man</userinput></screen>
</listitem><listitem><para>In NFS version 2 or version 3, this command uses client failover.</para><screen># <userinput>mount -F nfs -r bee,wasp:/export/share/man /usr/man</userinput></screen><para>In NFS version 4, the following command line uses client failover.</para><screen># <userinput>mount -F nfs -o vers=4 -r bee,wasp:/export/share/man /usr/man</userinput></screen><note><para>When used from the command line, the listed servers must support
the same version of the NFS protocol. Do not use both version 2 and version
3 servers when running <command>mount</command> from the command line. You
can use both servers with autofs. Autofs automatically selects the best subset
of version 2 or version 3 servers.</para>
</note>
</listitem><listitem><para>Here is an example of using an NFS URL with the <command>mount</command> command
in NFS version 2 or version 3.</para><screen># <userinput>mount -F nfs nfs://bee//export/share/man /usr/man</userinput></screen><para>Here is an example of using an NFS URL with the <command>mount</command> command
in NFS version 4.</para><screen># <userinput>mount -F nfs -o vers=4 nfs://bee//export/share/man /usr/man</userinput></screen>
</listitem><listitem><para>Use the <option role="nodash">forcedirectio</option> mount
option to enable the client to permit concurrent writes, as well as concurrent
reads and  writes, to a file. Here is an example. </para><screen># <userinput>mount -F nfs -o forcedirectio bee:/home/somebody /mnt</userinput></screen><para>In this example, the command mounts an NFS file system from the server <literal>bee</literal> and enables concurrent reads and writes for each file in the
directory <filename>/mnt</filename>.  When support for concurrent reads and
writes is enabled, the following occurs.</para><itemizedlist><listitem><para>The client permits applications to write to a file in parallel.</para>
</listitem><listitem><para>Caching is disabled on the client. Consequently, data from
reads and writes is kept on the server. More explicitly, because the client
does not cache the data that is read or written, any data that the application
does not already have cached for itself is read from the server. The client's
operating system does not have a copy of this data. Normally, the NFS client
caches data in the kernel for applications to use.</para><para>Because caching
is disabled on the client, the read-ahead and write-behind processes are disabled.
A read-ahead process occurs when the kernel anticipates the data that an application
might request next. The kernel then starts the process of gathering that data
in advance. The kernel's goal is to have the data ready before the application
makes a request for the data.</para><para>The client uses the write-behind
process to increase write throughput. Instead of immediately starting an I/O
operation every time an application writes data to a file, the data is cached
in memory. Later, the data is written to the disk.</para><para>Potentially,
the write-behind process permits the data to be written in larger chunks or
to be written asynchronously from the application. Typically, the result of
using larger chunks is increased throughput. Asynchronous writes permit overlap
between application processing and I/O processing. Also, asynchronous writes
permit the storage subsystem to optimize    the I/O by providing a better
sequencing of the I/O. Synchronous writes force a sequence of I/O on the storage
subsystem that might not be optimal.</para>
</listitem><listitem><para>Significant performance degradation can occur if the application
is not prepared to handle the semantics of data that is not being cached.
Multithreaded applications avoid this problem.</para>
</listitem>
</itemizedlist><note><para>If support for concurrent writes is not enabled, all write requests
are serialized. When requests are serialized, the following occurs. When a
write request is in progress, a second write request has to wait for the first
write request to be completed before proceeding.</para>
</note>
</listitem><listitem><para>Use the <command>mount</command> command with no arguments to
display file systems that are mounted on a client. See the following.</para><screen>% <userinput>mount</userinput>
/ on /dev/dsk/c0t3d0s0 read/write/setuid on Wed Apr 7 13:20:47 2004
/usr on /dev/dsk/c0t3d0s6 read/write/setuid on Wed Apr 7 13:20:47 20041995
/proc on /proc read/write/setuid on Wed Apr 7 13:20:47 2004
/dev/fd on fd read/write/setuid on Wed Apr 7 13:20:47 2004
/tmp on swap read/write on Wed Apr 7 13:20:51 2004
/opt on /dev/dsk/c0t3d0s5 setuid/read/write on Wed Apr 7 13:20:51 20041995
/home/kathys on bee:/export/home/bee7/kathys              
  intr/noquota/nosuid/remote on Wed Apr 24 13:22:13 2004</screen>
</listitem>
</itemizedlist>
</sect3>
</sect2><sect2 id="rfsrefer-19"><title><command>umount</command> Command</title><para>This command enables you to remove a remote file system that is
currently mounted. The <command>umount</command> command supports the <option>V</option> option
to allow for testing. You might also use the <option>a</option> option to
unmount several file systems at one time. If <replaceable>mount-points</replaceable> are
included with the <option>a</option> option, those file systems are unmounted.
If no mount points are included, an attempt is made to unmount all file systems
that are listed in <filename>/etc/mnttab</filename> except for the &#8220;required&#8221;
file systems, such as <filename>/</filename>, <filename>/usr</filename>, <filename>/var</filename>, <filename>/proc</filename>, <filename>/dev/fd</filename>,
and <filename>/tmp</filename>. Because the file system is already mounted
and should have an entry in <filename>/etc/mnttab</filename>, you do not need
to include a flag for the file-system type.  </para><para>The <option>f</option> option forces a busy file system to be unmounted.
You can use this option to unhang a client that is hung while trying to mount
an unmountable file system.</para><caution><para>By forcing an unmount of a file system, you can cause data
loss if files are being written to.</para>
</caution><para>See the following examples.</para><example id="rfsrefer-ex-152"><title>Unmounting a File System</title><para>This example unmounts a file system that is mounted on <filename>/usr/man</filename>: </para><screen># <userinput>umount /usr/man</userinput></screen>
</example><example id="rfsrefer-ex-153"><title>Using Options with <command>umount</command></title><para>This example displays the results of running <command>umount</command> <option>a -V</option>: </para><screen># <userinput>umount -a -V</userinput>
umount /home/kathys
umount /opt
umount /home
umount /net</screen><para>Notice that this command does not actually unmount the file systems.</para>
</example>
</sect2><sect2 id="rfsrefer-21"><title><command>mountall</command> Command</title><itemizedlist><para>Use this command to mount all file systems or a specific group of file
systems that are listed in a file-system table. The command provides a way
of doing the following:</para><listitem><para>Selecting the file-system type to be accessed with the <option>F</option> <replaceable>FSType</replaceable> option</para>
</listitem><listitem><para>Selecting all the remote file systems that are listed in a
file-system table with the <option>r</option> option</para>
</listitem><listitem><para>Selecting all the local file systems with the <option>l</option> option</para>
</listitem>
</itemizedlist><para>Because all file systems that are labeled as NFS file-system type are
remote file systems, some of these options are redundant. For more information,
see the <olink targetdoc="refman1m" targetptr="mountall-1m" remap="external"><citerefentry><refentrytitle>mountall</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> man
page.  </para><para>Note that the following two examples of user input are equivalent: </para><screen># <userinput>mountall -F nfs</userinput></screen><screen># <userinput>mountall -F nfs -r</userinput></screen>
</sect2><sect2 id="rfsrefer-23"><title><command>umountall</command> Command</title><para>Use this command to unmount a group of file systems. The <option>k</option> option
runs the <command>fuser</command> <option>k</option> <replaceable>mount-point</replaceable> command
to kill any processes that are associated with the <replaceable>mount-point</replaceable>. The <option>s</option> option indicates that unmount is not to be performed in parallel. <option>l</option> specifies that only local file systems are to be used, and <option>r</option> specifies
that only remote file systems are to be used. The <option>h</option> <replaceable>host</replaceable> option indicates that all file systems from the named host
should be unmounted. You cannot combine the <option>h</option> option with <option>l</option> or <option>r</option>.             </para><para>The following is an example of unmounting all file systems that are
mounted from remote hosts: </para><screen># <userinput>umountall -r</userinput></screen><para>The following is an example of unmounting all file systems that are
currently mounted from the server <literal>bee</literal>: </para><screen># <userinput>umountall -h bee</userinput></screen>
</sect2><sect2 id="gcrvu"><title><command>sharemgr</command> Command</title><itemizedlist><para>The
Solaris Express, Developer Edition 2/07 release includes the <command>sharemgr</command> utility,
which is an administrative tool that provides an enhanced method of sharing
files and performing related tasks. Previously, such tasks were accomplished
by adding entries to the <filename>/etc/dfs/dfstab</filename> file and using
the <command>share</command> command to create a temporary share. You made
the share permanent by rebooting the system or using the <command>shareall</command> command.
Related administrative tasks necessitated that you manually edit configuration
files. The <command>sharemgr</command> utility simplifies this process by
introducing two concepts:</para><listitem><para><emphasis role="strong">Share</emphasis> &#8211; One or more
files or directories in a share group.</para>
</listitem><listitem><para><emphasis role="strong">Share group</emphasis> &#8211; A container
of one or more shared files or directories. Note the following:</para><itemizedlist><listitem><para>Options for <command>sharemgr</command> are set to a share
group, not to a specific file or directory. All options apply to each file
and directory in the group.</para>
</listitem><listitem><para>A file or directory can only be assigned to one share group.
However, you can move a file or directory from one group to another.</para>
</listitem><listitem><para>A share group can be used by multiple file-system types. For
example, the share group <literal>my_group</literal> could be used by NFS
and ZFS and be assigned one set of options for NFS and another set of options
for ZFS.</para><note><para>When a share is managed by ZFS, <command>sharemgr</command> identifies
the share and lists it in a <literal>zfs</literal> share group.</para>
</note>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist><para>The <command>sharemgr</command> utility accomplishes different tasks
by using subcommands. Options and their related properties can be used with
each subcommand. The utility uses the following syntax:</para><screen># sharemgr [subcommand] [option] [share_group]</screen><note><para>The <command>sharemgr</command> utility provides a unique way
of checking the validity of a desired configuration. The <option>n</option> option
allows you to test the validity of the options and properties you want to
use with a specific subcommand. The test does not change your configuration.
For example, if you use the <option>n</option> option with the subcommand <command>create</command>, no share group is created.</para>
</note><para>The following tables describes the subcommands supported by the <command>sharemgr</command> utility.</para><table frame="topbot" id="gcsrw"><title>Subcommands Supported by <command>sharemgr</command></title><tgroup cols="2" colsep="0" rowsep="0"><colspec colwidth="34.38*"/><colspec colwidth="65.62*"/><thead><row rowsep="1"><entry><para>Subcommand</para>
</entry><entry><para>Description</para>
</entry>
</row>
</thead><tbody><row><entry><para><olink type="custom-text" targetptr="gcrwl" remap="internal">create</olink></para>
</entry><entry><para>Makes (or creates) a new share group</para>
</entry>
</row><row><entry><para><olink type="custom-text" targetptr="gcrwp" remap="internal">delete</olink></para>
</entry><entry><para>Removes a share group</para>
</entry>
</row><row><entry><para><olink type="custom-text" targetptr="gcrvm" remap="internal">list</olink></para>
</entry><entry><para>Lists the current share groups</para>
</entry>
</row><row><entry><para><olink type="custom-text" targetptr="gcrvv" remap="internal">show</olink></para>
</entry><entry><para>Lists the shares by share group</para>
</entry>
</row><row><entry><para><olink type="custom-text" targetptr="gcrxg" remap="internal">set</olink></para>
</entry><entry><para>Sets a share group's properties, including the group's security property</para>
</entry>
</row><row><entry><para><olink type="custom-text" targetptr="gcswk" remap="internal">unset</olink></para>
</entry><entry><para>Removes (or unsets) properties from a share group</para>
</entry>
</row><row><entry><para><olink type="custom-text" targetptr="gcryf" remap="internal">add-share</olink></para>
</entry><entry><para>Adds a new share to a share group</para>
</entry>
</row><row><entry><para><olink type="custom-text" targetptr="gcrxz" remap="internal">move-share</olink></para>
</entry><entry><para>Moves a share from one share group to another</para>
</entry>
</row><row><entry><para><olink type="custom-text" targetptr="gcryv" remap="internal">remove-share</olink></para>
</entry><entry><para>Removes a share from a share group</para>
</entry>
</row><row><entry><para><olink type="custom-text" targetptr="gcrxq" remap="internal">set-share</olink></para>
</entry><entry><para>Updates the properties associated with a share</para>
</entry>
</row><row><entry><para><olink type="custom-text" targetptr="gcrys" remap="internal">disable</olink></para>
</entry><entry><para>Unshares one or more share groups</para>
</entry>
</row><row><entry><para><olink type="custom-text" targetptr="gcrxj" remap="internal">enable</olink></para>
</entry><entry><para>Shares one or more share groups</para>
</entry>
</row><row><entry><para><olink type="custom-text" targetptr="gcryq" remap="internal">start</olink></para>
</entry><entry><para>Used by the <command>smf</command> utility to share one or more share
groups</para>
</entry>
</row><row><entry><para><olink type="custom-text" targetptr="gcryx" remap="internal">stop</olink></para>
</entry><entry><para>Used by the <command>smf</command> utility to unshare one or more share
groups</para>
</entry>
</row><row><entry><para><olink type="custom-text" targetptr="gcrxi" remap="internal">-h</olink></para>
</entry><entry><para>Provides online-help descriptions of <command>sharemgr</command> and
its subcommands and options, and shows the syntax to use</para>
</entry>
</row>
</tbody>
</tgroup>
</table><para>The following table describes the properties supported by the <command>sharemgr</command> utility.</para><table frame="topbot" id="gcrwf"><title>Properties Supported by <command>sharemgr</command> Utility</title><tgroup cols="3" colsep="0" rowsep="0"><colspec colwidth="22.01*"/><colspec colwidth="24.98*"/><colspec colwidth="52.00*"/><thead><row rowsep="1"><entry><para>Property</para>
</entry><entry><para>Value</para>
</entry><entry><para>Description</para>
</entry>
</row>
</thead><tbody><row><entry><para><literal>aclok</literal></para>
</entry><entry><para>boolean</para>
</entry><entry><para>Enable access control lists (ACL) for NFS version 2.</para>
</entry>
</row><row><entry><para><literal>anon</literal></para>
</entry><entry><para>UID</para>
</entry><entry><para>Specify the User ID for unknown users.</para>
</entry>
</row><row><entry><para><literal>index</literal></para>
</entry><entry><para>file path</para>
</entry><entry><para>Include the specified file in a content list for a directory.</para>
</entry>
</row><row><entry><para><literal>log</literal></para>
</entry><entry><para>tag</para>
</entry><entry><para>Specify a tag for NFS server logging. Note that these tags are defined
in the <filename>/etc/nfs/nfslog.conf</filename> file.</para>
</entry>
</row><row><entry><para><literal>nosub</literal></para>
</entry><entry><para>boolean</para>
</entry><entry><para>Disallow clients from mounting subdirectories of shares.</para>
</entry>
</row><row><entry><para><literal>nosuid</literal></para>
</entry><entry><para>boolean</para>
</entry><entry><para>Disallow the use of the <function>setuid</function> and <function>setgid</function> functions.</para>
</entry>
</row><row><entry><para><literal>public</literal></para>
</entry><entry><para>boolean</para>
</entry><entry><para>Move the location of a public file handle from <filename>root</filename> to
an exported directory for WebNFS-enabled 	browsers and clients. Only one
file system (or share) on each server can use this property. This property
is not accepted by a share group. For more information, see <olink targetdoc="group-refman" targetptr="share-nfs-1m" remap="external"><citerefentry><refentrytitle>share_nfs</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> man page.</para>
</entry>
</row><row><entry><para><literal>ro</literal></para>
</entry><entry><para>access-list, boolean, or an asterisk (<literal>*</literal>)</para>
</entry><entry><para>If the property is set to an access-list, permissions for the list are
set to read-only. For information about access lists, see the <olink targetdoc="refman1m" targetptr="share-nfs-1m" remap="external"><citerefentry><refentrytitle>share_nfs</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> man page.</para><para>If you set the property to no value or <literal>true</literal>, permissions
for the share group are set to read-only.</para><para>If you set the property to an asterisk (<literal>*</literal>), permissions
for all hosts are set to read-only.</para><note><para>To use this property, you must use the <option>S</option> option
to set the security mode. For more information about setting a security mode,
see <olink targetptr="gcrxg" remap="internal">set Subcommand</olink>.</para>
</note>
</entry>
</row><row><entry><para><literal>root</literal></para>
</entry><entry><para>access-list or an asterisk (<literal>*</literal>)</para>
</entry><entry><para>If the property is set to an access-list, permissions for the list are
set to <filename>root</filename> access. For information about access-lists,
see the <olink targetdoc="refman1m" targetptr="share-nfs-1m" remap="external"><citerefentry><refentrytitle>share_nfs</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> man
page.</para><para>If you set the property to an asterisk (<literal>*</literal>), all hosts
have <filename>root</filename> access.</para><note><para>To use this property, you must use the <option>S</option> option
to set the security mode. For more information about setting a security mode,
see <olink targetptr="gcrxg" remap="internal">set Subcommand</olink>.</para>
</note>
</entry>
</row><row><entry><para><literal>rw</literal></para>
</entry><entry><para>access-list, boolean, or an asterisk (<literal>*</literal>)</para>
</entry><entry><para>If the property is set to an access-list, permissions for the list are
set to read-write. For information about access-lists, see the <olink targetdoc="refman1m" targetptr="share-nfs-1m" remap="external"><citerefentry><refentrytitle>share_nfs</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> man page.</para><para>If you set the property to no value or <literal>true</literal>, permissions
for the share group are set to read-write.</para><para>If you set the property to an asterisk (<literal>*</literal>), permissions
for all hosts are set to read-write.</para><note><para>To use this property, you must use the <option>S</option> option
to set the security mode. For more information about setting a security mode,
see <olink targetptr="gcrxg" remap="internal">set Subcommand</olink>.</para>
</note>
</entry>
</row><row><entry><para><literal>window</literal></para>
</entry><entry><para>integer</para>
</entry><entry><para>For the <literal>dh</literal> security mode, set the number of seconds
a credential is available.</para><note><para>To use this property, you must use the <option>S</option> option
to set the security mode. For more information about setting a security mode,
see <olink targetptr="gcrxg" remap="internal">set Subcommand</olink>.</para>
</note>
</entry>
</row>
</tbody>
</tgroup>
</table><note><para><command>sharemgr</command> and <command>sharectl</command> are
the preferred utilities for managing your file systems and file-sharing protocols.</para>
</note><itemizedlist><para>For
procedures that use the <command>sharemgr</command> utility, see the following:</para><listitem><para><olink targetptr="rfsadmin-56" remap="internal">Automatic File-System Sharing</olink></para>
</listitem><listitem><para><olink targetptr="rfsadmin-61" remap="internal">Mounting File Systems</olink></para>
</listitem><listitem><para><olink targetptr="rfsadmin-96" remap="internal">Administering the Secure NFS
System</olink></para>
</listitem>
</itemizedlist><para>Also, see the <citerefentry><refentrytitle>sharemgr</refentrytitle><manvolnum>1M</manvolnum></citerefentry> man page.</para><itemizedlist><para>The <command>sharectl</command> utility is an administrative tool that enables you to configure
and manage file-sharing protocols, such as NFS. For more information, see
the following:</para><listitem><para><citerefentry><refentrytitle>sharectl</refentrytitle><manvolnum>1M</manvolnum></citerefentry> man page</para>
</listitem><listitem><para><olink targetptr="gecpc" remap="internal">sharectl Command</olink>.</para>
</listitem>
</itemizedlist><para>The sections that follow describe each subcommand for <command>sharemgr</command> and
provide examples.</para><itemizedlist><listitem><para><olink targetptr="gcrwl" remap="internal">create Subcommand</olink></para>
</listitem><listitem><para><olink targetptr="gcrwp" remap="internal">delete Subcommand</olink></para>
</listitem><listitem><para><olink targetptr="gcrvm" remap="internal">list Subcommand</olink></para>
</listitem><listitem><para><olink targetptr="gcrvv" remap="internal">show Subcommand</olink></para>
</listitem><listitem><para><olink targetptr="gcrxg" remap="internal">set Subcommand</olink></para>
</listitem><listitem><para><olink targetptr="gcswk" remap="internal">unset Subcommand</olink></para>
</listitem><listitem><para><olink targetptr="gcryf" remap="internal">add-share Subcommand</olink></para>
</listitem><listitem><para><olink targetptr="gcrxz" remap="internal">move-share Subcommand</olink></para>
</listitem><listitem><para><olink targetptr="gcryv" remap="internal">remove-share Subcommand</olink></para>
</listitem><listitem><para><olink targetptr="gcrxq" remap="internal">set-share Subcommand</olink></para>
</listitem><listitem><para><olink targetptr="gcrxj" remap="internal">enable Subcommand</olink></para>
</listitem><listitem><para><olink targetptr="gcrys" remap="internal">disable Subcommand</olink></para>
</listitem><listitem><para><olink targetptr="gcryq" remap="internal">start Subcommand</olink></para>
</listitem><listitem><para><olink targetptr="gcryx" remap="internal">stop Subcommand</olink></para>
</listitem><listitem><para><olink targetptr="gcrxi" remap="internal">-h Feature</olink></para>
</listitem>
</itemizedlist><sect3 id="gcrwl"><title><command>create</command> Subcommand</title><itemizedlist><para>The <command>create</command> subcommand makes (or creates) a share group. After you create
a share group,  use the add-share subcommand to add shares to the group. Note
the following:</para><listitem><para>The group name is restricted to alphanumeric characters, the
hyphen (-), and the underscore (_). The first character must be alphabetic.</para>
</listitem><listitem><para>By default, a newly created share group is enabled.</para>
</listitem><listitem><para>If a file-system type is not specified, all supported protocols
are added to the share group.</para>
</listitem><listitem><para>When adding options to the share group, you must explicitly
define the protocol associated with the options.</para>
</listitem>
</itemizedlist><para>This subcommand supports the following options:</para><variablelist termlength="xtranarrow"><varlistentry><term><option>n</option></term><listitem><para>Checks the validity of a desired configuration.</para>
</listitem>
</varlistentry><varlistentry><term><option>P</option></term><listitem><para>Specifies a file-system type. The default is NFS.</para>
</listitem>
</varlistentry><varlistentry><term><option>p</option></term><listitem><para>Specifies a property for the new share group.</para>
</listitem>
</varlistentry><varlistentry><term><option>h</option></term><listitem><para>Provides an online-help description.</para>
</listitem>
</varlistentry>
</variablelist><para>The create subcommand uses the following syntax:</para><screen># sharemgr create [-h] [-n] [-P protocol] [-p property=value] share_group</screen><itemizedlist><para>The
following example creates <literal>my_group</literal> with the following parameters:</para><listitem><para>The share group uses NFS.</para>
</listitem><listitem><para>The permissions for the group are set to read-write.</para>
</listitem><listitem><para>The functions, <function>setuid</function> and <function>setgid</function>,
cannot be used on <literal>my_group</literal>.</para>
</listitem>
</itemizedlist><screen>example# <userinput>sharemgr create -P nfs -p rw=true -p nosuid=true my_group</userinput></screen>
</sect3><sect3 id="gcrwp"><title><command>delete</command> Subcommand</title><para>This subcommand removes a share group. Before using this option, use
the  <command>remove-share</command> subcommand to delete all shares from
the group. Alternately, use  the <option>f</option> option with this subcommand
to force the removal of a group that might still contain shares. The <option>f</option> option
unshares and removes all shares from the share group, so the share group can
be removed.</para><para>To remove a protocol from a share group, use the <option>P</option> option.
Note that when using the <option>P</option> option, the share group is not
removed. Only the protocol is removed from the group.</para><para>This subcommand supports the following options:</para><variablelist termlength="xtranarrow"><varlistentry><term><option>n</option></term><listitem><para>Checks the validity of command-line string</para>
</listitem>
</varlistentry><varlistentry><term><option>f</option></term><listitem><para>Forces a share group to be removed</para>
</listitem>
</varlistentry><varlistentry><term><option>P</option></term><listitem><para>Specifies a file-system type to be removed from the group</para>
</listitem>
</varlistentry><varlistentry><term><option>h</option></term><listitem><para>Provides an online-help description</para>
</listitem>
</varlistentry>
</variablelist><para>This subcommand uses the following syntax:</para><screen># sharemgr delete [-h] [-n] [-f] [-P protocol] share_group</screen>
</sect3><sect3 id="gcrvm"><title><command>list</command> Subcommand</title><para>This subcommand provides a list of current share groups. You can customize
the output by using various options with this subcommand.</para><para>This subcommand supports the following options:</para><variablelist termlength="xtranarrow"><varlistentry><term><option>P</option></term><listitem><para>Enables you to see a list of groups that use a specific file-system
type.</para>
</listitem>
</varlistentry><varlistentry><term><option>v</option></term><listitem><para>Is the verbose option and provides the following:</para><itemizedlist><listitem><para>Group name</para>
</listitem><listitem><para>Status of the group, specifically whether the group is enabled
or disabled</para>
</listitem><listitem><para>File-system type
used by the group</para>
</listitem>
</itemizedlist>
</listitem>
</varlistentry><varlistentry><term><option>h</option></term><listitem><para>Provides an online-help description.</para>
</listitem>
</varlistentry>
</variablelist><para>This subcommand uses the following syntax:</para><screen># sharemgr list [-h] [-P protocol] [-v]</screen><para>The following example shows the output for the <option>v</option> option:</para><screen>example# <userinput>sharemgr list -v</userinput>
group01   enabled    nfs
group02   disabled   nfs</screen>
</sect3><sect3 id="gcrvv"><title><command>show</command> Subcommand</title><para>This subcommand provides a list of shares by group. By specifying one
or more share groups, you can limit the output to a list of shares in the
specified groups. If no groups are specified, the list shows the shares in
each group.</para><para>This subcommand supports the following options:</para><variablelist termlength="xtranarrow"><varlistentry><term><option>p</option></term><listitem><para>Shows you the properties assigned to each group.</para>
</listitem>
</varlistentry><varlistentry><term><option>v</option></term><listitem><para>Is the verbose option. If included, the verbose option provides
the resource name and descriptions of each share.</para>
</listitem>
</varlistentry><varlistentry><term><option>x</option></term><listitem><para>Creates an XML file for the output. Because this option automatically
includes the information you would get from the <option>p</option> and <option>v</option> options,
no other options are needed when you use this option.</para>
</listitem>
</varlistentry><varlistentry><term><option>h</option></term><listitem><para>Provides an online-help description.</para>
</listitem>
</varlistentry>
</variablelist><para>This subcommand uses the following syntax:</para><screen># sharemgr show [-h] [-v] [-p] [-x] [share_group...]</screen><para>The following example uses the <option>p</option> option to show the
shares and group properties for <literal>my_group</literal>:</para><screen>example01# <userinput>sharemgr show -p my_group</userinput>
my_group	nfs=(rw=true nosuid=true)
	/export/home/home0
	/export/home/home1</screen><para>The next example uses the <option>v</option> option to show the shares
in <literal>my_group</literal> and their  descriptions:</para><screen>example02# <userinput>sharemgr show -v my_group</userinput>
my_group
	HOME0=/export/home/home0	"Home directory set 0"
	HOME1=/export/home/home1	"Home directory set 1"</screen>
</sect3><sect3 id="gcrxg"><title><command>set</command> Subcommand</title><itemizedlist><para>This
subcommand sets properties to a share group. Note the following conditions:</para><listitem><para>If you do not change the protocol currently associated with
the group, the previous property value is replaced with the new value.</para>
</listitem><listitem><para>If you specify a different protocol for the group, the new
properties for the different protocol are added to the existing shares.</para>
</listitem>
</itemizedlist><para>A group can be associated with more than one protocol and can have different
properties for each protocol.</para><para>This subcommand supports the following options:</para><variablelist termlength="xtranarrow"><varlistentry><term><option>n</option></term><listitem><para>Checks the validity of the command-line string.</para>
</listitem>
</varlistentry><varlistentry><term><option>P</option></term><listitem><para>Specifies a file-system type.</para>
</listitem>
</varlistentry><varlistentry><term><option>p</option></term><listitem><para>Specifies a property for the share group.</para>
</listitem>
</varlistentry><varlistentry><term><option>S</option></term><listitem><para>Specifies the security mode, such as <literal>sys</literal>, <literal>dh</literal>, or <literal>krb5</literal>. For more information about security
modes, see the <olink targetdoc="refman5" targetptr="nfssec-5" remap="external"><citerefentry><refentrytitle>nfssec</refentrytitle><manvolnum>5</manvolnum></citerefentry></olink> man
page.</para>
</listitem>
</varlistentry><varlistentry><term><option>s</option></term><listitem><para>Specifies the path to the share, which is a file or a directory.</para>
</listitem>
</varlistentry><varlistentry><term><option>h</option></term><listitem><para>Provides an online-help description.</para>
</listitem>
</varlistentry>
</variablelist><para>This subcommand uses the following syntax:</para><screen remap="wide"># sharemgr set [-h] [-n] [-P protocol] [-s share-path] [-S security-mode] [-p property=value] share_group</screen><para>The following example sets the user ID for unknown users in <literal>my_group</literal> to <literal>1234546</literal>:</para><screen>example01# <userinput>sharemgr set -p anon=123456 my_group</userinput></screen><itemizedlist><para>In
the next example, the following occurs:</para><listitem><para><literal>my_group</literal> continues to use NFS.</para>
</listitem><listitem><para>Previously, <literal>nosuid</literal> was set to <literal>true</literal>.
Because <command>set</command> replaces the previous 	  value for <literal>nosuid</literal> with the new value, the functions, <function>setuid</function> and <function>setgid</function>, can now be used on the shares in <literal>my_group</literal>.</para>
</listitem><listitem><para>The permissions for the group continue to be set to read-write.</para>
</listitem>
</itemizedlist><screen>example02# <userinput>sharemgr create -P nfs -p rw=true -p nosuid=true my_group</userinput>	
example02# <userinput>sharemgr set -P nfs -p nosuid=false my_group</userinput></screen>
</sect3><sect3 id="gcswk"><title><command>unset</command> Subcommand</title><para>This subcommand removes (or unsets) properties from a share group.</para><para>This subcommand supports the following options:</para><variablelist termlength="xtranarrow"><varlistentry><term><option>n</option></term><listitem><para>Checks the validity of the command-line string</para>
</listitem>
</varlistentry><varlistentry><term><option>P</option></term><listitem><para>Specifies the protocol associated with the properties being
removed from the group</para>
</listitem>
</varlistentry><varlistentry><term><option>p</option></term><listitem><para>Specifies the property to be removed from the share group</para>
</listitem>
</varlistentry><varlistentry><term><option>s</option></term><listitem><para>Specifies the path to the share, which is a file or a directory.</para>
</listitem>
</varlistentry><varlistentry><term><option>h</option></term><listitem><para>Provides an online-help description</para>
</listitem>
</varlistentry>
</variablelist><para>This subcommand uses the following syntax:</para><screen># sharemgr unset [-h] [-n] -P protocol [-s share-path] [-p property] share_group</screen>
</sect3><sect3 id="gcryf"><title><command>add-share</command> Subcommand</title><para>After creating a share group, use this subcommand to add shares to the
group. A share is a path to a file or a directory. Note that a share can exist
in one group only. If you try to add a share to another group, you will get
an error message.</para><para>This subcommand supports the following options:</para><variablelist termlength="xtranarrow"><varlistentry><term><option>n</option></term><listitem><para>Checks the validity of the command-line string.</para>
</listitem>
</varlistentry><varlistentry><term><option>s</option></term><listitem><para>Specifies the path to the share, which is a file or a directory.</para>
</listitem>
</varlistentry><varlistentry><term><option>t</option></term><listitem><para>Specifies that the share is transient. Transient shares are
automatically removed from the group when you reboot the system or use the <command>disable</command> or <command>stop</command> subcommands.</para>
</listitem>
</varlistentry><varlistentry><term><option>d</option></term><listitem><para>Adds descriptive text about the share.</para>
</listitem>
</varlistentry><varlistentry><term><option>r</option></term><listitem><para>Assigns the share a resource name that identifies the share.
Note that the resource name only uses alphanumeric characters, the hyphen
(-), and the underscore (_). The first character in the name must be alphabetic.</para>
</listitem>
</varlistentry><varlistentry><term><option>h</option></term><listitem><para>Provides an online-help description.</para>
</listitem>
</varlistentry>
</variablelist><para>This subcommand uses the following syntax:</para><screen># sharemgr add-share [-h] [-n] -s share-path [-t] [-d description] [-r resource-name] share_group</screen><para>The following example adds the shares <filename>/export/home/home0</filename> and <filename>/export/home/home1</filename> to <literal>my_group</literal>.</para><screen>example# <userinput>sharemgr add-share -s /export/home/home0 my_group</userinput>
example# <userinput>sharemgr add-share -s /export/home/home1 my_group</userinput></screen>
</sect3><sect3 id="gcrxz"><title><command>move-share</command> Subcommand</title><para>Use this subcommand to move a share from one group to another.</para><para>This subcommand supports the following options:</para><variablelist termlength="xtranarrow"><varlistentry><term><option>n</option></term><listitem><para>Checks the validity of the command-line string</para>
</listitem>
</varlistentry><varlistentry><term><option>s</option></term><listitem><para>Specifies the path to the share, which is a file or a directory</para>
</listitem>
</varlistentry><varlistentry><term><option>h</option></term><listitem><para>Provides an online-help description</para>
</listitem>
</varlistentry>
</variablelist><para>This subcommand uses the following syntax:</para><screen># sharemgr move-share [-h] [-n] -s share-path share_group</screen><para>The following example shows a share that was added to <literal>my_group</literal> and
then moved to <literal>your_group</literal>.</para><screen>example# <userinput>sharemgr add-share  -s /export/home/home0 my_group</userinput>
example# <userinput>sharemgr move-share -s /export/home/home0 your_group</userinput></screen>
</sect3><sect3 id="gcryv"><title><command>remove-share</command> Subcommand</title><para>Use this subcommand to remove a share from a share group.</para><para>This subcommand supports the following options:</para><variablelist termlength="xtranarrow"><varlistentry><term><option>n</option></term><listitem><para>Checks the validity of the command-line string</para>
</listitem>
</varlistentry><varlistentry><term><option>s</option></term><listitem><para>Specifies the path to the share, which is a file or a directory</para>
</listitem>
</varlistentry><varlistentry><term><option>h</option></term><listitem><para>Provides an online-help description</para>
</listitem>
</varlistentry>
</variablelist><para>This subcommand uses the following syntax:</para><screen># sharemgr remove-share [-h] [-n] -s share-path share_group</screen><para>The following example removes the share <filename>/export/home/home0</filename> from <literal>my_group</literal>.</para><screen>example# <userinput>sharemgr remove-share -s /export/home/home0 my_group</userinput></screen>
</sect3><sect3 id="gcrxq"><title><command>set-share</command> Subcommand</title><para>Use this subcommand to change the properties associated with a share.
Currently, you can use this subcommand to change the descriptive text associated
with a specific share.</para><para>This subcommand supports the following options:</para><variablelist termlength="xtranarrow"><varlistentry><term><option>n</option></term><listitem><para>Checks the validity of the command-line string.</para>
</listitem>
</varlistentry><varlistentry><term><option>s</option></term><listitem><para>Specifies the path to the share, which is a file or a directory.</para>
</listitem>
</varlistentry><varlistentry><term><option>d</option></term><listitem><para>Adds descriptive text about the share.</para>
</listitem>
</varlistentry><varlistentry><term><option>r</option></term><listitem><para>Assigns the share a resource name that identifies the share.
Note that the resource name only uses alphanumeric characters, the hyphen
(-), and the underscore (_). The first character in the name must be alphabetic.</para>
</listitem>
</varlistentry><varlistentry><term><option>h</option></term><listitem><para>Provides an online-help description.</para>
</listitem>
</varlistentry>
</variablelist><para>This subcommand uses the following syntax:</para><screen># sharemgr set-share [-h] [-n] -s share-path [-d description] [-r resource-name] share_group</screen><para>The following example shows how a description is changed.</para><screen>example# <userinput>sharemgr add-share  -s /export/home/home0 -d "original text" my_group</userinput>
example# <userinput>sharemgr set-share  -s /export/home/home0 -d "new text" my_group</userinput></screen>
</sect3><sect3 id="gcrxj"><title><command>enable</command> Subcommand</title><para>Use this subcommand to share (or enable) the shares in the groups that
you specify. Note that the groups you create are enabled by default. You must
use this subcommand to enable a group that has previously been disabled with
the <command>disable</command> subcommand.</para><note><para>If you specify a protocol, only the groups that are associated
with that protocol are enabled.</para>
</note><para>This subcommand supports the following options:</para><variablelist termlength="xtranarrow"><varlistentry><term><option>a</option></term><listitem><para>Specifies all groups</para>
</listitem>
</varlistentry><varlistentry><term><option>n</option></term><listitem><para>Checks the validity of the command-line string</para>
</listitem>
</varlistentry><varlistentry><term><option>P</option></term><listitem><para>Specifies a file-system type</para>
</listitem>
</varlistentry><varlistentry><term><option>h</option></term><listitem><para>Provides an online-help description</para>
</listitem>
</varlistentry>
</variablelist><para>This subcommand uses the following syntax:</para><screen># sharemgr enable [-h] [-n] [-P protocol] [share_group | -a]</screen><para>The following example shares (or enables) the shares in all groups that
use NFS.</para><screen>example01# <userinput>sharemgr enable -P NFS -a</userinput></screen><para>In this next example, all shares in <literal>my_group</literal> are
shared (or enabled).</para><screen>example02# <userinput>sharemgr enable my_group</userinput></screen>
</sect3><sect3 id="gcrys"><title><command>disable</command> Subcommand</title><para>Use this subcommand to unshare (or disable) the shares in the groups
that you specify. This subcommand can be reversed by using the <command>enable</command> subcommand.</para><note><para>If you specify a protocol, only the groups that are associated
with that protocol are disabled.</para>
</note><para>This subcommand supports the following options:</para><variablelist termlength="xtranarrow"><varlistentry><term><option>a</option></term><listitem><para>Specifies all groups</para>
</listitem>
</varlistentry><varlistentry><term><option>n</option></term><listitem><para>Checks the validity of the command-line string</para>
</listitem>
</varlistentry><varlistentry><term><option>P</option></term><listitem><para>Specifies a file-system type</para>
</listitem>
</varlistentry><varlistentry><term><option>h</option></term><listitem><para>Provides an online-help description</para>
</listitem>
</varlistentry>
</variablelist><para>This subcommand uses the following syntax:</para><screen># sharemgr disable [-h] [-P protocol] [share_group | -a]</screen><para>The following example unshares (or disables) the shares in all groups
that use NFS.</para><screen>example01# <userinput>sharemgr disable -P NFS -a</userinput></screen><para>In this next example, all shares in <literal>my_group</literal> are
unshared (or disabled).</para><screen>example02# <userinput>sharemgr disable my_group</userinput></screen>
</sect3><sect3 id="gcryq"><title><command>start</command> Subcommand</title><itemizedlist><para>This
subcommand is similar to the <command>enable</command> subcommand with these
distinctions:</para><listitem><para><command>start</command> enables the shares to start sharing
in specified groups that have previously been stopped by the <command>stop</command> subcommand.</para>
</listitem><listitem><para><command>start</command> can be used by SMF (Service Management
Facility) to enable shares to start sharing in specified groups during a system
boot. See the man pages for <olink targetdoc="refman5" targetptr="smf-5" remap="external"><citerefentry><refentrytitle>smf</refentrytitle><manvolnum>5</manvolnum></citerefentry></olink> and <olink targetdoc="refman1m" targetptr="svcadm-1m" remap="external"><citerefentry><refentrytitle>svcadm</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink>.</para>
</listitem><listitem><para><command>start</command> works only on groups that are enabled.</para>
</listitem>
</itemizedlist><para>This subcommand supports the following options:</para><variablelist termlength="xtranarrow"><varlistentry><term><option>a</option></term><listitem><para>Specifies all groups</para>
</listitem>
</varlistentry><varlistentry><term><option>h</option></term><listitem><para>Provides an online-help description</para>
</listitem>
</varlistentry>
</variablelist><table frame="all" pgwide="100" id="gctdc"><title>Two Ways to Start a Share
Group</title><tgroup cols="2" colsep="1" rowsep="1"><colspec colwidth="50*"/><colspec colwidth="50*"/><thead><row><entry><para>Using the <command>start</command> Subcommand</para>
</entry><entry><para>Using the <command>svcadm</command> Command</para>
</entry>
</row>
</thead><tbody><row><entry><para>The <command>start</command> subcommand uses the following syntax:</para><screen># sharemgr start [-h] [share-group | -a]</screen><para>The following example enables the shares in all groups to start sharing.</para><screen># <userinput>sharemgr start -a</userinput></screen>
</entry><entry><para>The <command>svcadm</command> command uses the following syntax:</para><screen># svcadm start network/shares/group:<replaceable>share-group</replaceable></screen><para>The following example enables the shares in <literal>my-group</literal> to start
sharing.</para><screen># <userinput>svcadm start network/shares/group:my-group</userinput></screen>
</entry>
</row>
</tbody>
</tgroup>
</table>
</sect3><sect3 id="gcryx"><title><command>stop</command> Subcommand</title><itemizedlist><para>This
subcommand is similar to the <command>disable</command> subcommand with these
distinctions:</para><listitem><para><command>stop</command> unshares the shares in specified groups,
and can only be reversed by using the <command>start</command> subcommand.</para>
</listitem><listitem><para><command>stop</command> can be used by SMF (Service Management
Facility) to unshare the shares in specified groups during a system shutdown.
See the man pages for <olink targetdoc="refman5" targetptr="smf-5" remap="external"><citerefentry><refentrytitle>smf</refentrytitle><manvolnum>5</manvolnum></citerefentry></olink> and <olink targetdoc="refman1m" targetptr="svcadm-1m" remap="external"><citerefentry><refentrytitle>svcadm</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink>.</para>
</listitem><listitem><para><command>stop</command> works only on groups that are enabled.</para>
</listitem><listitem><para><command>stop</command> permanently removes all transient
shares.</para>
</listitem>
</itemizedlist><para>This subcommand supports the following options:</para><variablelist termlength="xtranarrow"><varlistentry><term><option>a</option></term><listitem><para>Specifies all groups</para>
</listitem>
</varlistentry><varlistentry><term><option>h</option></term><listitem><para>Provides an online-help description</para>
</listitem>
</varlistentry>
</variablelist><table frame="all" pgwide="100" id="gctdh"><title>Two Ways to Stop a Share
Group</title><tgroup cols="2" colsep="1" rowsep="1"><colspec colwidth="50*"/><colspec colwidth="50*"/><thead><row><entry><para>Using the <command>stop</command> Subcommand</para>
</entry><entry><para>Using the <command>svcadm</command> Command</para>
</entry>
</row>
</thead><tbody><row><entry><para>The <command>stop</command> subcommand uses the following syntax:</para><screen># sharemgr stop [-h] [share-group | -a]</screen><para>The following example unshares the shares in all groups.</para><screen># <userinput>sharemgr stop -a</userinput></screen>
</entry><entry><para>The <command>svcadm</command> command uses the following syntax:</para><screen># svcadm stop network/shares/group:<replaceable>share-group</replaceable></screen><para>The following example unshares the shares in <literal>my-group</literal>.</para><screen># <userinput>svcadm stop network/shares/group:my-group</userinput></screen>
</entry>
</row>
</tbody>
</tgroup>
</table>
</sect3><sect3 id="gcrxi"><title><command>-h</command> Feature</title><para>The <command>sharemgr</command> utility has an online help feature that
describes <command>sharemgr</command> and its subcommands and options, and
shows proper syntax. For online help, use <option>h</option>.</para><para>This feature uses the following syntax:</para><screen># sharemgr [subcommand] -h</screen><para>The following example uses <option>h</option> to provide a complete
description of the <command>sharemgr</command> utility.</para><screen>example01# <userinput>sharemgr -h</userinput>
USAGE: # sharemgr [subcommand] [option] [share_group]

DESCRIPTION: Configures and manages file sharing

SUBCOMMANDS:
create        Makes (or creates) a new share group
delete        Removes a share group
list          Lists the current share groups
show          Lists the shares by share group
set           Sets a share group's properties
unset         Removes (or unsets) properties from a share group
add-share     Adds a new share to a share group
move-share    Moves a share from one share group to another
remove-share  Removes a share from a share group
set-share     Updates the properties associated with a share
disable       Unshares one or more share groups
enable        Shares one or more share groups
start         Used by the smf utility to share one or more share groups
stop          Used by the smf utility to unshare one or more share groups
-h            Online-help feature

SEE ALSO:
sharemgr(1M) man page
System Administration Guide: Network Services</screen><para>This next example uses <option>h</option> to provide information about
the <command>set</command> subcommand.</para><screen>example02# <userinput>sharemgr set -h</userinput>
USAGE: # sharemgr set [-h] [-n] [-P protocol] [-S security-mode] [-p property=value] share_group

DESCRIPTION: Sets a share group's properties

OPTIONS:
-h  Online-help feature
-n  Checks the validity of the command-line string
-P  Specifies a file-system type.
-p  Specifies a property for the share group.
-S  Specifies the security mode, such as sys, dh, or krb5

PROPERTIES:
aclok={true|false}        Enable access control lists (ACL) for NFS version 2.
anon=UID                  Specify the User ID for unknown users.
index=file path           Include the specified file in a content list for a directory.
log=tag                   Specify a tag to use for log messages.
nosub={true|false}        Disallow clients from mounting subdirectories of shares.
nosuid={true|false}       Disallow the use of the setuid() and setgid() functions.
ro={access-list|true|*}   If ro is set to an access-list, permissions for the list
                          are read-only. If ro is set to no value or to true, 
                          permissions for the share group are read-only. If ro
                          is set to an asterisk (*), permissions for all hosts
                          are set to read-only.
root={access-list|*}      If root  is set to an access-list, permissions for the
                          list are set to allow root access. If root is set to
                          an asterisk (*), all hosts have root access.
rw={access-list|true|*}   If rw is set to an access list, permissions for the list
                          are set to read-write. If rw is set to no value or to 
                          true, permissions for the share group are set to 
                          read-write. If rw is set to an asterisk (*), permissions
                          for all hosts are set to read-write.
window=integer            For the dh security mode, set the number of seconds a
                          credential is available.

SEE ALSO:
sharemgr(1M) man page
System Administration Guide: Network Services
nfssec(5) man page for more information about security modes</screen><para>For help, you can also refer to the <citerefentry><refentrytitle>sharemgr</refentrytitle><manvolnum>1M</manvolnum></citerefentry> man page.</para>
</sect3>
</sect2><sect2 id="gecpc"><title><command>sharectl</command> Command</title><itemizedlist><para>The
Solaris Express, Developer Edition 2/07 release includes the <command>sharectl</command> utility,
which is an administrative tool that enables you to configure and manage file-sharing
protocols, such as NFS. You can use this command to do the following:</para><listitem><para>Set client and server operational properties</para>
</listitem><listitem><para>Display property values for a specific protocol</para>
</listitem><listitem><para>Obtain the status of a protocol</para>
</listitem>
</itemizedlist><para>The <command>sharectl</command> utility uses the following syntax:</para><screen># sharectl subcommand [option] [protocol]</screen><para>The <command>sharectl</command> utility supports the following subcommands:</para><table frame="topbot" id="gecom"><title>Subcommands for <command>sharectl</command> Utility</title><tgroup cols="2" colsep="0" rowsep="0"><colspec colwidth="50*"/><colspec colwidth="50*"/><thead><row rowsep="1"><entry><para>Subcommand</para>
</entry><entry><para>Description</para>
</entry>
</row>
</thead><tbody><row><entry><para><literal>set</literal></para>
</entry><entry><para>Defines the properties for a file-sharing protocol. For a list of properties
and property values, see the parameters described in 	the <olink targetdoc="group-refman" targetptr="nfs-4" remap="external"><citerefentry><refentrytitle>nfs</refentrytitle><manvolnum>4</manvolnum></citerefentry></olink> man page.</para>
</entry>
</row><row><entry><para><literal>get</literal></para>
</entry><entry><para>Displays the properties and property values for the specified protocol.</para>
</entry>
</row><row><entry><para><literal>status</literal></para>
</entry><entry><para>Displays whether the specified protocol is enabled or disabled. If no
protocol is specified, the status of all file-sharing protocols is displayed.</para>
</entry>
</row>
</tbody>
</tgroup>
</table><note><para><command>sharemgr</command> and <command>sharectl</command> are
the preferred utilities for managing your file systems and file-sharing protocols.</para>
</note><itemizedlist><para>For
more information about the <command>sharectl</command> utility, see the following:</para><listitem><para><citerefentry><refentrytitle>sharectl</refentrytitle><manvolnum>1M</manvolnum></citerefentry> man page</para>
</listitem><listitem><para><olink targetptr="gecpf" remap="internal">set Subcommand</olink></para>
</listitem><listitem><para><olink targetptr="gecph" remap="internal">get Subcommand</olink></para>
</listitem><listitem><para><olink targetptr="gecoi" remap="internal">status Subcommand</olink></para>
</listitem>
</itemizedlist><itemizedlist><para>For
information about the <command>sharemgr</command> utility, see the following:</para><listitem><para><citerefentry><refentrytitle>sharemgr</refentrytitle><manvolnum>1M</manvolnum></citerefentry> man page</para>
</listitem><listitem><para><olink targetptr="gcrvu" remap="internal">sharemgr Command</olink></para>
</listitem>
</itemizedlist><sect3 id="gecpf"><title><command>set</command> Subcommand</title><para>The <command>set</command> subcommand, which defines the properties
for a file-sharing protocol, supports the following options:</para><variablelist><varlistentry><term><option>h</option></term><listitem><para>Provides an online-help description</para>
</listitem>
</varlistentry><varlistentry><term><option>p</option></term><listitem><para>Defines a property for the protocol</para>
</listitem>
</varlistentry>
</variablelist><para>The <command>set</command> subcommand uses the following syntax:</para><screen># sharectl set [-h] [-p property=value] protocol</screen><note><para>The following:</para><itemizedlist><listitem><para>You must have <literal>root</literal> privileges to use the <command>set</command> subcommand.</para>
</listitem><listitem><para>You do not need to repeat this command-line syntax for each
additional property value. You can use the <option>p</option> option multiple
times to define multiple properties on the same command line.</para>
</listitem>
</itemizedlist>
</note><para>The following example sets the minimum version of the NFS protocol for
the client to <literal>3</literal>:</para><screen># <userinput>sharectl set -p nfs_client_versmin=3 nfs</userinput></screen>
</sect3><sect3 id="gecph"><title><command>get</command> Subcommand</title><para>The <command>get</command> subcommand, which displays the properties
and property values for the specified protocol, supports the following options:</para><variablelist><varlistentry><term><option>h</option></term><listitem><para>Provides an online-help description.</para>
</listitem>
</varlistentry><varlistentry><term><option>p</option></term><listitem><para>Identifies the property value for the specified property.
If the 	<option>p</option> option is not used, all property values are
displayed.</para>
</listitem>
</varlistentry>
</variablelist><para>The <command>get</command> subcommand uses the following syntax:</para><screen># sharectl get [-h] [-p property] protocol</screen><note><para>You must have <literal>root</literal> privileges to use the <command>get</command> subcommand.</para>
</note><para>The following example uses <literal>nfsd_servers</literal>, which is
the property that enables you to specify the maximum number of concurrent
NFS requests:</para><screen># <userinput>sharectl get -p nfsd_servers nfs</userinput>
nfsd_servers=<userinput>16</userinput></screen><para>In the following example, because the <option>p</option> option is not
used, all property values are displayed:</para><screen># <userinput>sharectl get nfs</userinput>
listen_backlog=32
protocol=ALL
servers=32
lockd_listen_backlog=32
lockd_servers=20
lockd_retransmit_timeout=5
grace_period=90
nfsmapid_domain=company.com
server_versmin=2
server_versmax=4
client_versmin=2
client_versmax=4
max_connections=-1</screen>
</sect3><sect3 id="gecoi"><title><command>status</command> Subcommand</title><para>The <command>status</command> subcommand, which displays whether the
specified protocol is enabled or disabled, supports the following option:</para><variablelist><varlistentry><term><option>h</option></term><listitem><para>Provides an online-help description</para>
</listitem>
</varlistentry>
</variablelist><para>The <command>status</command> subcommand uses the following syntax:</para><screen># sharectl status [-h] [protocol]</screen><para>The following example shows the status of the NFS protocol:</para><screen># <userinput>sharectl status nfs</userinput>
nfs	   enabled</screen>
</sect3>
</sect2><sect2 id="rfsrefer-25"><title><command>share</command> Command</title><note><para>Starting
with the Solaris Express, Developer Edition 2/07 release, <command>sharemgr</command> and <command>sharectl</command> are the preferred utilities for managing your file systems
and file-sharing protocols. See <olink targetptr="gcrvu" remap="internal">sharemgr Command</olink> and <olink targetptr="gecpc" remap="internal">sharectl Command</olink></para>
</note><para>With this command, you can make a local file system on an NFS
server available for mounting. You can also use the <command>share</command> command
to display a list of the file systems on your system that are currently shared.
The NFS server must be running for the <command>share</command> command to
work. The NFS server software is started automatically during boot if an entry
is in <filename>/etc/dfs/dfstab</filename>. The command does not report an
error if the NFS server software is not running, so you must verify that the
software is running.  </para><para>The objects that can be shared include any directory tree. However,
each file system hierarchy is limited by the disk slice or partition that
the file system is located on. For instance, sharing the root (<filename>/</filename>)
file system would not also share <filename>/usr</filename>, unless these directories
are on the same disk partition or slice. Normal installation places root on
slice 0 and <filename>/usr</filename> on slice 6. Also, sharing <filename>/usr</filename> would
not share any other local disk partitions that are mounted on subdirectories
of <filename>/usr</filename>.</para><para>A file system cannot be shared if that file system is part of a larger
file system that is already being shared. For example, if <filename>/usr</filename> and <filename>/usr/local</filename> are on one disk slice, <filename>/usr</filename> can
be shared or <filename>/usr/local</filename> can be shared. However, if both
directories need to be shared with different share options, <filename>/usr/local</filename> must
be moved to a separate disk slice.</para><para>You can gain access to a file system that is read-only shared
through the file handle of a file system that is read-write shared. However,
the two file systems have to be on the same disk slice. You can create a more
secure situation. Place those file systems that need to be read-write on a
separate partition or separate disk slice from the file systems that you need
to share as read-only.     </para><note><para>For information about how NFS version 4 functions when a file
system is unshared and then reshared, refer to <olink targetptr="rfsrefer-148" remap="internal">Unsharing
and Resharing a File System in NFS Version 4</olink>.</para>
</note><sect3 id="rfsrefer-26"><title>Non-File-System-Specific <command>share</command> Options</title><para>Some of the options that you can include with the <option>o</option> flag
are as follows.  </para><variablelist termlength="wholeline"><varlistentry><term>rw|ro</term><listitem><para>The <replaceable>pathname</replaceable> file system is shared
read-write or read-only for all clients.      </para>
</listitem>
</varlistentry><varlistentry><term>rw=<replaceable>accesslist</replaceable></term><listitem><para>The file system is shared read-write for the clients that are
listed only. All other requests are denied. Starting with the Solaris 2.6
release, the list of clients that are defined in <replaceable>accesslist</replaceable> has
been expanded. See <olink targetptr="rfsrefer-27" remap="internal">Setting Access Lists With
the share Command</olink> for more information. You can use this option to
override an <option>ro</option> option.  </para>
</listitem>
</varlistentry>
</variablelist>
</sect3><sect3 id="rfsrefer-103"><title>NFS-Specific <command>share</command> Options</title><para>The options that you can use with NFS file systems include the following.</para><variablelist termlength="wholeline"><varlistentry><term>aclok</term><listitem><para>This option enables an NFS server that supports the NFS version
2 protocol to be configured to do access control for NFS version 2 clients.
Without this option, all clients are given minimal access. With this option,
the clients have maximal access. For instance, on file systems that are shared
with the <option>aclok</option> option, if anyone has read permissions, everyone
does. However, without this option, you can deny access to a client who should
have access permissions. A decision to permit too much access or too little
access depends on the security systems already in place. See <olink targetdoc="sysadv6" targetptr="secfile-37" remap="external"><citetitle remap="section">Using Access Control Lists to Protect Files</citetitle> in <citetitle remap="book">System Administration Guide: Security Services</citetitle></olink> for more information
about access control lists (ACLs).</para><note><para>To use ACLs, ensure that clients and servers run software that
supports the NFS version 3 and NFS_ACL protocols. If the software only supports
the NFS version 3 protocol, clients obtain correct access but cannot manipulate
the ACLs. If the software supports the NFS_ACL protocol, the clients obtain
correct access and can manipulate the ACLs. Starting with the Solaris 2.5
release, the Solaris system supports both protocols.</para>
</note>
</listitem>
</varlistentry><varlistentry><term>anon=<replaceable>uid</replaceable></term><listitem><para>You use <replaceable>uid</replaceable> to select the user ID of
unauthenticated users. If you set <replaceable>uid</replaceable> to <literal>-1</literal>,
the server denies access to unauthenticated users. You can grant root access
by setting <literal>anon=0</literal>, but this option allows unauthenticated
users to have root access, so use the <literal>root</literal> option instead.
 </para>
</listitem>
</varlistentry><varlistentry><term>index=<replaceable>filename</replaceable></term><listitem><para>When a user accesses an NFS URL, the <option>index=</option><replaceable>filename</replaceable> option forces the HTML file to load, instead of displaying
a list of the directory. This option mimics the action of current browsers
if an <filename>index.html</filename> file is found in the directory that
the HTTP URL is accessing. This option is the equivalent of setting the <literal>DirectoryIndex</literal> option for <command>httpd</command>. For instance,
suppose that the <filename>dfstab</filename> file entry resembles the following:</para><screen>share -F nfs -o ro,public,index=index.html /export/web</screen><para>These URLs then display the same information:</para><screen>nfs://&#60;<replaceable>server</replaceable>>/&#60;<replaceable>dir</replaceable>>
nfs://&#60;<replaceable>server</replaceable>>/&#60;<replaceable>dir</replaceable>>/index.html
nfs://&#60;<replaceable>server</replaceable>>//export/web/&#60;<replaceable>dir</replaceable>>
nfs://&#60;<replaceable>server</replaceable>>//export/web/&#60;<replaceable>dir</replaceable>>/index.html
http://&#60;<replaceable>server</replaceable>>/&#60;<replaceable>dir</replaceable>>
http://&#60;<replaceable>server</replaceable>>/&#60;<replaceable>dir</replaceable>>/index.html</screen>
</listitem>
</varlistentry><varlistentry><term>log=<replaceable>tag</replaceable></term><listitem><para>This option specifies the tag in <filename>/etc/nfs/nfslog.conf</filename> that
contains the NFS server logging configuration information for a file system.
This option must be selected to enable NFS server logging. </para>
</listitem>
</varlistentry><varlistentry><term>nosuid</term><listitem><para>This option signals that all attempts to enable the <literal>setuid</literal> or <literal>setgid</literal> mode should be ignored. NFS clients cannot create files with
the <literal>setuid</literal> or <literal>setgid</literal> bits on.  </para>
</listitem>
</varlistentry><varlistentry><term>public</term><listitem><para>The <option>public</option> option has been added to the <command>share</command> command to enable WebNFS browsing. Only one file system on
a server can be shared with this option.</para>
</listitem>
</varlistentry><varlistentry><term>root=<replaceable>accesslist</replaceable></term><listitem><para>The server gives root access to the hosts in the list. By default,
the server does not give root access to any remote hosts. If the selected
security mode is anything other than <option>sec=sys</option>, you can only
include client host names in the <replaceable>accesslist</replaceable>. Starting
with the Solaris 2.6 release, the list of clients that are defined in <replaceable>accesslist</replaceable> is expanded. See <olink targetptr="rfsrefer-27" remap="internal">Setting
Access Lists With the share Command</olink> for more information.  </para><caution><para>Granting root access to other hosts has wide security implications.
Use the <option>root=</option> option with extreme caution.   </para>
</caution>
</listitem>
</varlistentry><varlistentry><term>root=<replaceable>client-name</replaceable></term><listitem><para>The <replaceable>client-name</replaceable> value is used with
AUTH_SYS authentication to check the client's IP address against a list of
addresses provided by <olink targetdoc="refman1" targetptr="exportfs-1b" remap="external"><citerefentry><refentrytitle>exportfs</refentrytitle><manvolnum>1B</manvolnum></citerefentry></olink>.
 If a match is found, <literal>root</literal> access is given to the file
systems being shared.</para>
</listitem>
</varlistentry><varlistentry><term>root=<replaceable>host-name</replaceable></term><listitem><para>For secure NFS modes, such as AUTH_SYS or RPCSEC_GSS, the
server checks the clients' principal names against a list of host-based principal
names that are derived from an access list. The generic syntax for the client's
principal  name is <literal>root@</literal><replaceable>hostname</replaceable>.
For Kerberos V the syntax is <literal>root/</literal><replaceable>hostname.fully.qualified</replaceable><literal>@REALM</literal>. When you use the <replaceable>host-name</replaceable> value,
the clients on the access list must have the credentials for a principal name.
For Kerberos V, the client must have a valid keytab entry for its <literal>root/</literal><replaceable>hostname.fully.qualified</replaceable><literal>@REALM</literal> principal
name. For more information, see <olink targetdoc="sysadv6" targetptr="setup-148" remap="external"><citetitle remap="section">Configuring Kerberos Clients</citetitle> in <citetitle remap="book">System Administration Guide: Security Services</citetitle></olink>.</para>
</listitem>
</varlistentry><varlistentry><term>sec=<replaceable>mode</replaceable>[:<replaceable>mode</replaceable>]</term><listitem><para><replaceable>mode</replaceable> selects the security modes
that are needed to obtain access to the file system. By default, the security
mode is UNIX authentication. You can specify multiple modes, but use each
security mode only once per command line. Each <option>mode</option> option
applies to any subsequent <option>rw</option>, <option>ro</option>, <option>rw=</option>, <option>ro=</option>, <option>root=</option>, and <option>window=</option> options
until another <option>mode</option> is encountered. The use of <option>sec=none</option> maps
all users to user <literal>nobody</literal>.</para>
</listitem>
</varlistentry><varlistentry><term>window=<replaceable>value</replaceable></term><listitem><para><replaceable>value</replaceable> selects the maximum lifetime
in seconds of a credential on the NFS server. The default value is 30000 seconds
or 8.3 hours.</para>
</listitem>
</varlistentry>
</variablelist>
</sect3><sect3 id="rfsrefer-27"><title>Setting Access Lists With the <command>share</command> Command</title><para>In Solaris releases prior to 2.6, the <replaceable>accesslist</replaceable> that
was included with either the <option>ro=</option>, <option>rw=</option>, or <option>root=</option> option of the <command>share</command> command was restricted
to a list of host names or netgroup names. Starting with the Solaris 2.6 release,
the access list can also include a domain name, a subnet number, or an entry
to deny access. These extensions should simplify file access control on a
single server without having to change the namespace or maintain long lists
of clients.</para><para>This command provides read-only access for most systems but allows
read-write access for <literal>rose</literal> and <literal>lilac</literal>:
     </para><screen># <userinput>share -F nfs -o ro,rw=rose:lilac /usr/src</userinput></screen><para>In the next example, read-only access is assigned to any host
in the <literal>eng</literal> netgroup. The client <literal>rose</literal> is
specifically given read-write access.   </para><screen># <userinput>share -F nfs -o ro=eng,rw=rose /usr/src</userinput></screen><note><para>You cannot specify both <literal>rw</literal> and <literal>ro</literal> without
arguments. If no read-write option is specified, the default is read-write
for all clients.</para>
</note><para>To share one file system with multiple clients, you must type all options
on the same line. Multiple invocations of the <command>share</command> command
on the same object &#8220;remember&#8221; only the last command that is run.
This command enables read-write access to three client systems, but only <literal>rose</literal> and <literal>tulip</literal> are given access to the file system
as <literal>root</literal>.</para><screen># <userinput>share -F nfs -o rw=rose:lilac:tulip,root=rose:tulip /usr/src</userinput></screen><para>When sharing a file system that uses multiple authentication mechanisms,
ensure that you include the <option>ro</option>, <option>ro=</option>, <option>rw</option>, <option>rw=</option>, <option>root</option>, and <option>window</option> options
after the correct security modes. In this example, UNIX authentication is
selected for all hosts in the netgroup that is named <literal>eng</literal>.
These hosts can only mount the file system in read-only mode. The hosts <literal>tulip</literal> and <literal>lilac</literal> can mount the file system read-write
if these hosts use Diffie-Hellman authentication. With these options, <literal>tulip</literal> and <literal>lilac</literal> can mount the file system read-only
even if these hosts are not using DH authentication. However, the host names
must be listed in the <literal>eng</literal> netgroup. </para><screen># <userinput>share -F nfs -o sec=dh,rw=tulip:lilac,sec=sys,ro=eng /usr/src</userinput></screen><para>Even though UNIX authentication is the default security mode, UNIX authentication
is not included if the <option>sec</option> option is used. Therefore, you
must include a <option>sec=sys</option> option if UNIX authentication is to
be used with any other authentication mechanism.</para><para>You can use a DNS domain name in the access list by preceding the actual
domain name with a dot. The string that follows the dot is a domain name,
not a fully qualified host name. The following entry allows mount access to
all hosts in the <literal>eng.example.com</literal> domain:</para><screen># <userinput>share -F nfs -o ro=.:.eng.example.com /export/share/man</userinput></screen><para>In this example, the single &#8220;<literal>.</literal>&#8221; matches
all hosts that are matched through the NIS or NIS+ namespaces.  The results
that are returned from these name services do not include the domain name.
The &#8220;<literal>.eng.example.com</literal>&#8221; entry matches all hosts
that use DNS for namespace resolution. DNS always returns a fully qualified
host name. So, the longer entry is required if you use a combination of DNS
and the other namespaces.</para><para>You can use a subnet number in an access list by preceding the actual
network number or the network name with &#8220;<literal>@</literal>&#8221;.
This character differentiates the network name from a netgroup or a fully
qualified host name. You must identify the subnet in either <filename>/etc/networks</filename> or in an NIS or NIS+ namespace. The following entries have the
same effect if the <literal>192.168</literal> subnet has been identified as
the <literal>eng</literal> network:</para><screen># <userinput>share -F nfs -o ro=@eng /export/share/man</userinput>
# <userinput>share -F nfs -o ro=@192.168 /export/share/man</userinput>
# <userinput>share -F nfs -o ro=@192.168.0.0 /export/share/man</userinput></screen><para>The last two entries show that you do not need to include the full network
address.</para><para>If the network prefix is not byte aligned, as with Classless Inter-Domain
Routing (CIDR), the mask length can be explicitly specified on the command
line. The mask length is defined by following either the network name or the
network number with a slash and the number of significant bits in the prefix
of the address. For example:</para><screen># <userinput>share -f nfs -o ro=@eng/17 /export/share/man</userinput>
# <userinput>share -F nfs -o ro=@192.168.0/17 /export/share/man</userinput></screen><para>In these examples, the &#8220;<literal>/17</literal>&#8221; indicates
that the first 17 bits in the address are to be used as the mask. For additional
information about CIDR, look up RFC 1519.</para><para>You can also select negative access by placing a &#8220;<literal>-</literal>&#8221;
before the entry. Note that the entries are read from left to right. Therefore,
you must place the negative access entries before the entry that the negative
access entries apply to:</para><screen># <userinput>share -F nfs -o ro=-rose:.eng.example.com /export/share/man</userinput></screen><para>This example would allow access to any hosts in the <literal>eng.example.com</literal> domain except the host that is named <literal>rose</literal>.</para>
</sect3>
</sect2><sect2 id="rfsrefer-28"><title><command>unshare</command> Command</title><para>This command allows you to make a previously available file system
unavailable for mounting by clients. You can use the <command>unshare</command> command
to unshare any file system, whether the file system was shared explicitly
with the <command>share</command> command or automatically through <filename>/etc/dfs/dfstab</filename>. If you use the <command>unshare</command> command to unshare
a file system that you shared through the <filename>dfstab</filename> file,
be careful. Remember that the file system is shared again when you exit and
reenter run level 3. You must remove the entry for this file system from the <filename>dfstab</filename> file if the change is to continue.  </para><para>When you unshare an NFS file system, access from clients with existing
mounts is inhibited. The file system might still be mounted on the client,
but the files are not accessible.</para><note><para>For information about how NFS version 4 functions when a file
system is unshared and then reshared, refer to <olink targetptr="rfsrefer-148" remap="internal">Unsharing
and Resharing a File System in NFS Version 4</olink>.</para>
</note><para>The following is an example of unsharing a specific file system:</para><screen># <userinput>unshare /usr/src</userinput></screen>
</sect2><sect2 id="rfsrefer-30"><title><command>shareall</command> Command</title><para>This command allows for multiple file systems to be shared. When used
with no options, the command shares all entries in <filename>/etc/dfs/dfstab</filename>.
You can include a file name to specify the name of a file that lists <command>share</command> command lines. If you do not include a file name, <filename>/etc/dfs/dfstab</filename> is checked. If you use a &#8220;<literal>-</literal>&#8221; to
replace the file name, you can type <command>share</command> commands from
standard input. </para><para>The following is an example of sharing all file systems that are listed
in a local file: </para><screen># <userinput>shareall /etc/dfs/special_dfstab</userinput></screen>
</sect2><sect2 id="rfsrefer-32"><title><command>unshareall</command> Command</title><para>This command makes all currently shared resources unavailable.
The <option>F</option> <replaceable>FSType</replaceable> option selects a
list of file-system types that are defined in <filename>/etc/dfs/fstypes</filename>.
This flag enables you to choose only certain types of file systems to be unshared.
The default file-system type is defined in <filename>/etc/dfs/fstypes</filename>.
To choose specific file systems, use the <command>unshare</command> command.
   </para><para>The following is an example of unsharing all NFS-type file systems:</para><screen># <userinput>unshareall -F nfs</userinput></screen>
</sect2><sect2 id="rfsrefer-34"><title><command>showmount</command> Command</title><itemizedlist><para>This command displays one of the following:</para><listitem><para>All clients that have remotely mounted file systems that are
shared from an NFS server</para>
</listitem><listitem><para>Only the file systems that are mounted by clients</para>
</listitem><listitem><para>The shared file systems with the client access information</para>
</listitem>
</itemizedlist><note><para>The <command>showmount</command> command only shows NFS version
2 and version 3 exports. This command does not show NFS version 4 exports.</para>
</note><para>The command syntax is as follows: </para><para><command>showmount</command> <literal>[</literal> <option>ade</option> <literal>]</literal> <literal>[</literal> <emphasis>hostname</emphasis> <literal>]</literal></para><variablelist><varlistentry><term><option>a</option></term><listitem><para>Prints a list of all the remote mounts. Each entry includes
the client name and the directory.</para>
</listitem>
</varlistentry><varlistentry><term><option>d</option></term><listitem><para>Prints a list of the directories that are remotely mounted
by clients.</para>
</listitem>
</varlistentry><varlistentry><term><option>e</option></term><listitem><para>Prints a list of the files that are shared or are exported.</para>
</listitem>
</varlistentry><varlistentry><term><replaceable>hostname</replaceable></term><listitem><para>Selects the NFS server to gather the information from.</para>
</listitem>
</varlistentry>
</variablelist><para>If <replaceable>hostname</replaceable> is not specified, the local
host is queried.     </para><para>The following command lists all clients and the local directories that
the clients have mounted:</para><screen># <userinput>showmount -a bee</userinput>
lilac:/export/share/man
lilac:/usr/src
rose:/usr/src
tulip:/export/share/man</screen><para>The following command lists the directories that have been mounted:</para><screen># <userinput>showmount -d bee</userinput>
/export/share/man
/usr/src</screen><para>The following command lists file systems that have been shared:</para><screen># <userinput>showmount -e bee</userinput>
/usr/src								(everyone)
/export/share/man					eng</screen>
</sect2><sect2 id="rfsrefer-36"><title><command>setmnt</command> Command</title><para>This command creates an <filename>/etc/mnttab</filename> table.
The <command>mount</command> and <command>umount</command> commands consult
the table. Generally, you do not have to run this command manually, as this
command runs automatically when a system is booted.  </para>
</sect2>
</sect1><sect1 id="rfsrefer-37"><title>Commands for Troubleshooting NFS Problems</title><para>These commands can be useful when troubleshooting NFS problems.</para><sect2 id="rfsrefer-38"><title><command>nfsstat</command> Command</title><para>You can use this command to gather statistical information about NFS
and RPC connections. The syntax of the command is as follows:</para><para><command>nfsstat</command> <literal>[</literal>  <option>cmnrsz</option> <literal>]</literal> </para><variablelist><varlistentry><term><option>c</option></term><listitem><para>Displays client-side information</para>
</listitem>
</varlistentry><varlistentry><term><option>m</option></term><listitem><para>Displays statistics for each NFS-mounted file system</para>
</listitem>
</varlistentry><varlistentry><term><option>n</option></term><listitem><para>Specifies that NFS information is to be displayed on both
the client side and the server side</para>
</listitem>
</varlistentry><varlistentry><term><option>r</option></term><listitem><para>Displays RPC statistics</para>
</listitem>
</varlistentry><varlistentry><term><option>s</option></term><listitem><para>Displays the server-side information</para>
</listitem>
</varlistentry><varlistentry><term><option>z</option></term><listitem><para>Specifies that the statistics should be set to zero</para>
</listitem>
</varlistentry>
</variablelist><para>If no options are supplied on the command line, the <option>cnrs</option> options
are used. </para><para>Gathering server-side statistics can be important for debugging problems
when new software or new hardware is added to the computing environment. Running
this command a minimum of once a week, and storing the numbers, provides a
good history of previous performance.</para><para>Refer to the following example:</para><screen># <userinput>nfsstat -s</userinput>

Server rpc:
Connection oriented:
calls      badcalls   nullrecv   badlen     xdrcall    dupchecks  dupreqs    
719949194  0          0          0          0          58478624   33         
Connectionless:
calls      badcalls   nullrecv   badlen     xdrcall    dupchecks  dupreqs    
73753609   0          0          0          0          987278     7254       

Server nfs:
calls                badcalls             
787783794            3516                 
Version 2: (746607 calls)
null       getattr    setattr    root       lookup     readlink   read       
883 0%     60 0%      45 0%      0 0%       177446 23% 1489 0%    537366 71% 
wrcache    write      create     remove     rename     link       symlink    
0 0%       1105 0%    47 0%      59 0%      28 0%      10 0%      9 0%       
mkdir      rmdir      readdir    statfs     
26 0%      0 0%       27926 3%   108 0%     
Version 3: (728863853 calls)
null          getattr       setattr       lookup        access        
1365467 0%    496667075 68% 8864191 1%    66510206 9%   19131659 2%   
readlink      read          write         create        mkdir         
414705 0%     80123469 10%  18740690 2%   4135195 0%    327059 0%     
symlink       mknod         remove        rmdir         rename        
101415 0%     9605 0%       6533288 0%    111810 0%     366267 0%     
link          readdir       readdirplus   fsstat        fsinfo        
2572965 0%    519346 0%     2726631 0%    13320640 1%   60161 0%      
pathconf      commit        
13181 0%      6248828 0%    
Version 4: (54871870 calls)
null                compound            
266963 0%           54604907 99%        
Version 4: (167573814 operations)
reserved            access              close               commit              
0 0%                2663957 1%          2692328 1%          1166001 0%          
create              delegpurge          delegreturn         getattr             
167423 0%           0 0%                1802019 1%          26405254 15%        
getfh               link                lock                lockt               
11534581 6%         113212 0%           207723 0%           265 0%              
locku               lookup              lookupp             nverify             
230430 0%           11059722 6%         423514 0%           21386866 12%        
open                openattr            open_confirm        open_downgrade      
2835459 1%          4138 0%             18959 0%            3106 0%             
putfh               putpubfh            putrootfh           read                
52606920 31%        0 0%                35776 0%            4325432 2%          
readdir             readlink            remove              rename              
606651 0%           38043 0%            560797 0%           248990 0%           
renew               restorefh           savefh              secinfo             
2330092 1%          8711358 5%          11639329 6%         19384 0%            
setattr             setclientid         setclientid_confirm verify              
453126 0%           16349 0%            16356 0%            2484 0%             
write               release_lockowner   illegal             
3247770 1%          0 0%                0 0%                

Server nfs_acl:
Version 2: (694979 calls)
null        getacl      setacl      getattr     access      getxattrdir 
0 0%        42358 6%    0 0%        584553 84%  68068 9%    0 0%        
Version 3: (2465011 calls)
null        getacl      setacl      getxattrdir 
0 0%        1293312 52% 1131 0%     1170568 47% </screen><para>The previous listing is an example of NFS server statistics. The first
five lines relate to RPC and the remaining lines report NFS activities. In
both sets of statistics, knowing the average number of <literal>badcalls</literal> or <literal>calls</literal> and the number of calls per week can help identify a problem.
The <literal>badcalls</literal> value reports the number of bad messages from
a client. This value can indicate network hardware problems.</para><para>Some of the connections generate write activity on the disks. A sudden
increase in these statistics could indicate trouble and should be investigated.
For NFS version 2 statistics, the connections to note are <literal>setattr</literal>, <literal>write</literal>, <literal>create</literal>, <literal>remove</literal>, <literal>rename</literal>, <literal>link</literal>, <literal>symlink</literal>, <literal>mkdir</literal>,
and <literal>rmdir</literal>. For NFS version 3 and version 4 statistics,
the value to watch is <literal>commit</literal>. If the <literal>commit</literal> level
is high in one NFS server, compared to another almost identical server, check
that the NFS clients have enough memory. The number of <literal>commit</literal> operations
on the server grows when clients do not have available resources.</para>
</sect2><sect2 id="rfsrefer-40"><title><command>pstack</command> Command</title><para>This command displays a stack trace for each process. The <command>pstack</command> command
must be run by the owner of the process or by <literal>root</literal>. You
can use <command>pstack</command> to determine where a process is hung. The
only option that is allowed with this command is the <literal>PID</literal> of
the process that you want to check. See the <olink targetdoc="refman1" targetptr="proc-1" remap="external"><citerefentry><refentrytitle>proc</refentrytitle><manvolnum>1</manvolnum></citerefentry></olink> man page.</para><para>The following example is checking the <command>nfsd</command> process
that is running.</para><screen># <userinput>/usr/bin/pgrep nfsd</userinput>
243
# <userinput>/usr/bin/pstack 243</userinput>
243:    /usr/lib/nfs/nfsd -a 16
 ef675c04 poll     (24d50, 2, ffffffff)
 000115dc ???????? (24000, 132c4, 276d8, 1329c, 276d8, 0)
 00011390 main     (3, efffff14, 0, 0, ffffffff, 400) + 3c8
 00010fb0 _start   (0, 0, 0, 0, 0, 0) + 5c</screen><para>The example shows that the process is waiting for a new connection request,
which  is a normal response. If the stack shows that the process is still
in poll after a request is made, the process might be hung. Follow the instructions
in <olink targetptr="rfsadmin-240" remap="internal">How to Restart NFS Services</olink> to
fix this problem. Review the instructions in <olink targetptr="rfsadmin-215" remap="internal">NFS
Troubleshooting Procedures</olink> to fully verify that your problem is a
hung program.</para>
</sect2><sect2 id="rfsrefer-41"><title><command>rpcinfo</command> Command</title><para>This command generates information about the RPC service that is running
on a system. You can also use this command to change the RPC service. Many
options are available with this command. See the <olink targetdoc="refman1m" targetptr="rpcinfo-1m" remap="external"><citerefentry><refentrytitle>rpcinfo</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> man page. The following is
a shortened synopsis for some of the options that you can use with the command.</para><para><command>rpcinfo</command> <literal>[</literal> <option>m</option> <literal>|</literal> <option>s</option> <literal>]</literal> <literal>[</literal> <replaceable>hostname</replaceable> <literal>]</literal></para><para><command>rpcinfo</command> <option>T</option> <replaceable>transport</replaceable> <replaceable>hostname</replaceable> <literal>[</literal> <replaceable>progname</replaceable> <literal>]</literal></para><para><command>rpcinfo</command> <literal>[</literal> <option>t</option> <literal>|</literal> <option>u</option> <literal>]</literal> <literal>[</literal> <replaceable>hostname</replaceable> <literal>]</literal> <literal>[</literal> <replaceable>progname</replaceable> <literal>]</literal></para><variablelist><varlistentry><term><option>m</option></term><listitem><para>Displays a table of statistics of the <command>rpcbind</command> operations</para>
</listitem>
</varlistentry><varlistentry><term><option>s</option></term><listitem><para>Displays a concise list of all registered RPC programs</para>
</listitem>
</varlistentry><varlistentry><term><option>T</option></term><listitem><para>Displays information about services that use specific transports
or protocols</para>
</listitem>
</varlistentry><varlistentry><term><option>t</option></term><listitem><para>Probes the RPC programs that use TCP</para>
</listitem>
</varlistentry><varlistentry><term><option>u</option></term><listitem><para>Probes the RPC programs that use UDP</para>
</listitem>
</varlistentry><varlistentry><term><replaceable>transport</replaceable></term><listitem><para>Selects the transport or protocol for the services</para>
</listitem>
</varlistentry><varlistentry><term><replaceable>hostname</replaceable></term><listitem><para>Selects the host name of the server that you need information
from</para>
</listitem>
</varlistentry><varlistentry><term><replaceable>progname</replaceable></term><listitem><para>Selects the RPC program to gather information about</para>
</listitem>
</varlistentry>
</variablelist><para>If no value is given for <replaceable>hostname</replaceable>, the local
host name is used. You can substitute the RPC program number for <replaceable>progname</replaceable>, but many users can remember the name and not the number. You
can use the <option>p</option> option in place of the <option>s</option> option
on those systems that do not run the NFS version 3 software.</para><itemizedlist><para>The data that is generated by this command can include the following:</para><listitem><para>The RPC program number</para>
</listitem><listitem><para>The version number for a specific program</para>
</listitem><listitem><para>The transport protocol that is being used</para>
</listitem><listitem><para>The name of the RPC service</para>
</listitem><listitem><para>The owner of the RPC service</para>
</listitem>
</itemizedlist><para>The following example gathers information about the RPC services that
are running on a server. The text that is generated by the command is filtered
by the <command>sort</command> command to make the output more readable. Several
lines that list RPC services have been deleted from the example.</para><screen>% <userinput>rpcinfo -s bee |sort -n</userinput>
   program version(s) netid(s)                         service     owner
    100000  2,3,4     udp6,tcp6,udp,tcp,ticlts,ticotsord,ticots rpcbind     superuser
    100001  4,3,2     ticlts,udp,udp6                  rstatd      superuser
    100002  3,2       ticots,ticotsord,tcp,tcp6,ticlts,udp,udp6 rusersd     superuser
    100003  3,2       tcp,udp,tcp6,udp6                nfs         superuser
    100005  3,2,1     ticots,ticotsord,tcp,tcp6,ticlts,udp,udp6 mountd      superuser
    100007  1,2,3     ticots,ticotsord,ticlts,tcp,udp,tcp6,udp6 ypbind      superuser
    100008  1         ticlts,udp,udp6                  walld       superuser
    100011  1         ticlts,udp,udp6                  rquotad     superuser
    100012  1         ticlts,udp,udp6                  sprayd      superuser
    100021  4,3,2,1   tcp,udp,tcp6,udp6                nlockmgr    superuser
    100024  1         ticots,ticotsord,ticlts,tcp,udp,tcp6,udp6 status      superuser
    100029  3,2,1     ticots,ticotsord,ticlts          keyserv     superuser
    100068  5         tcp,udp                          cmsd        superuser
    100083  1         tcp,tcp6                         ttdbserverd superuser
    100099  3         ticotsord                        autofs      superuser
    100133  1         ticots,ticotsord,ticlts,tcp,udp,tcp6,udp6 -           superuser
    100134  1         ticotsord                        tokenring   superuser
    100155  1         ticots,ticotsord,tcp,tcp6        smserverd   superuser
    100221  1         tcp,tcp6                         -           superuser
    100227  3,2       tcp,udp,tcp6,udp6                nfs_acl     superuser
    100229  1         tcp,tcp6                         metad       superuser
    100230  1         tcp,tcp6                         metamhd     superuser
    100231  1         ticots,ticotsord,ticlts          -           superuser
    100234  1         ticotsord                        gssd        superuser
    100235  1         tcp,tcp6                         -           superuser
    100242  1         tcp,tcp6                         metamedd    superuser
    100249  1         ticots,ticotsord,ticlts,tcp,udp,tcp6,udp6 -           superuser
    300326  4         tcp,tcp6                         -           superuser
    300598  1         ticots,ticotsord,ticlts,tcp,udp,tcp6,udp6 -           superuser
    390113  1         tcp                              -           unknown
 805306368  1         ticots,ticotsord,ticlts,tcp,udp,tcp6,udp6 -           superuser
1289637086  1,5       tcp                              -           26069</screen><para>The following two examples show how to gather information about a particular
RPC service by selecting a particular transport on a server. The first example
checks the <command>mountd</command> service that is running over TCP. The
second example checks the NFS service that is running over UDP.</para><screen>% <userinput>rpcinfo -t bee mountd</userinput>
program 100005 version 1 ready and waiting
program 100005 version 2 ready and waiting
program 100005 version 3 ready and waiting
% <userinput>rpcinfo -u bee nfs</userinput>
program 100003 version 2 ready and waiting
program 100003 version 3 ready and waiting</screen>
</sect2><sect2 id="rfsrefer-43"><title><command>snoop</command> Command</title><para>This command is often used to watch for packets on the network. The <command>snoop</command> command must be run as <literal>root</literal>. The use of
this command is a good way to ensure that the network hardware is functioning
on both the client and the server. Many options are available. See the <olink targetdoc="refman1m" targetptr="snoop-1m" remap="external"><citerefentry><refentrytitle>snoop</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> man page. A shortened synopsis
of the command follows:</para><para><command>snoop</command>  <literal>[</literal> <command>-d</command> <replaceable>device</replaceable> <literal>]</literal> <literal>[</literal> <option>o</option> <replaceable>filename</replaceable> <literal>]</literal> <literal>[</literal> <literal>host</literal> <replaceable>hostname</replaceable> <literal>]</literal>  </para><variablelist><varlistentry><term><option>d</option> <replaceable>device</replaceable></term><listitem><para>Specifies the local network interface</para>
</listitem>
</varlistentry><varlistentry><term><option>o</option> <replaceable>filename</replaceable></term><listitem><para>Stores all the captured packets into the named file</para>
</listitem>
</varlistentry><varlistentry><term><replaceable>hostname</replaceable></term><listitem><para>Displays packets going to and from a specific host only</para>
</listitem>
</varlistentry>
</variablelist><para>The <option>d</option> <replaceable>device</replaceable> option is useful
on those servers that have multiple network interfaces. You can use many expressions
 other than setting the host. A combination of command expressions with <command>grep</command> can often generate data that is specific enough to be useful.</para><para>When troubleshooting, make sure that packets are going to and from the
proper host. Also, look for error messages. Saving the packets to a file can
simplify the review of the data.</para>
</sect2><sect2 id="rfsrefer-44"><title><command>truss</command> Command</title><para>You can use this command to check if a process is hung. The <command>truss</command> command
must be run by the owner of the process or by <literal>root</literal>. You
can use many options with this command. See the <olink targetdoc="refman1" targetptr="truss-1" remap="external"><citerefentry><refentrytitle>truss</refentrytitle><manvolnum>1</manvolnum></citerefentry></olink> man page. A shortened syntax
of the command follows.</para><para><command>truss</command> <literal>[</literal> <command>-t</command> <replaceable>syscall</replaceable> <literal>]</literal> <option>p</option> <replaceable>pid</replaceable></para><variablelist><varlistentry><term><option>t</option> <replaceable>syscall</replaceable></term><listitem><para>Selects system calls to trace</para>
</listitem>
</varlistentry><varlistentry><term><option>p</option> <replaceable>pid</replaceable></term><listitem><para>Indicates the PID of the process to be traced</para>
</listitem>
</varlistentry>
</variablelist><para>The <replaceable>syscall</replaceable> can be a comma-separated list
of system calls to be traced. Also, starting <replaceable>syscall</replaceable> with
an <literal>!</literal> selects to exclude the listed system calls from the
trace.</para><para>This example shows that the process is waiting for another connection
request from a new client. </para><screen># <userinput>/usr/bin/truss -p 243</userinput>
poll(0x00024D50, 2, -1)         (sleeping...)</screen><para>The previous example shows a normal response. If the response does not
change after a new connection request has been made, the process could be
hung. Follow the instructions in <olink targetptr="rfsadmin-240" remap="internal">How to Restart
NFS Services</olink> to fix the hung program. Review the instructions in <olink targetptr="rfsadmin-215" remap="internal">NFS Troubleshooting Procedures</olink> to fully verify
that your problem is a hung program.</para>
</sect2>
</sect1><sect1 id="rfsrefer-154"><title>NFS Over RDMA</title><para>Starting in the Solaris 10 release, the default transport for NFS is
the Remote Direct Memory Access (RDMA) protocol, which is a technology for
memory-to-memory transfer of data over high-speed networks. Specifically,
RDMA provides remote data transfer directly to and from memory without CPU
intervention. RDMA also provides direct data placement, which eliminates 
data copies and, therefore, further eliminates CPU intervention. Thus, RDMA
relieves not only the host CPU, but also reduces contention for the host memory
and I/O buses.  To provide this capability, RDMA combines the interconnect
I/O technology of InfiniBand on SPARC platforms with the Solaris operating
system. The following figure shows the relationship of RDMA to other protocols,
such as UDP and TCP.</para><figure id="rfsrefer-fig-155"><title>Relationship of RDMA to Other Protocols</title><mediaobject><imageobject><imagedata entityref="nfs-diagram.epsi"/>
</imageobject><textobject><simpara>The context describes the graphic.</simpara>
</textobject>
</mediaobject>
</figure><para>Because RDMA is the default transport protocol for NFS, no special <command>share</command> or <command>mount</command> options are required to use RDMA
on a client or server. The existing automounter maps, vfstab and dfstab, work
with the RDMA transport. NFS mounts over the RDMA transport occur transparently
when InfiniBand connectivity exists on SPARC platforms between the client
and the server. If the RDMA transport is not available on both the client
and the server, the TCP transport is the initial fallback, followed by UDP
if TCP is unavailable. Note, however, that if you use the <option role="nodash">proto=rdma</option> mount option, NFS mounts are forced to use RDMA only.</para><para>To specify that TCP and UDP be used only, you can use the <option role="nodash">proto=tcp/udp</option> <command>mount</command> option. This
option disables RDMA on an NFS client. For more information about NFS mount
options, see the <olink targetdoc="refman1m" targetptr="mount-nfs-1m" remap="external"><citerefentry><refentrytitle>mount_nfs</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> man
page and <olink targetptr="rfsrefer-15" remap="internal">mount Command</olink>.</para><note><para>RDMA for InfiniBand uses the IP addressing format and the IP lookup
infrastructure to specify peers. However, because RDMA is a separate protocol
stack, it does not fully implement all IP semantics. For example, RDMA does
not use IP addressing to communicate with peers. Therefore, RDMA might bypass
configurations for various security policies that are based on IP addresses.
However, the NFS and RPC administrative policies, such as <command>mount</command> restrictions
and secure RPC, are not bypassed.</para>
</note>
</sect1><sect1 id="rfsrefer-45"><title>How the NFS Service Works</title><para>The following sections describe some of the complex functions of the
NFS software. Note that some of the feature descriptions in this section are
exclusive to NFS version 4.</para><itemizedlist><listitem><para><olink targetptr="rfsrefer-147" remap="internal">Version Negotiation in NFS</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-134" remap="internal">Features in NFS Version 4</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-47" remap="internal">UDP and TCP Negotiation</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-48" remap="internal">File Transfer Size Negotiation</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-49" remap="internal">How File Systems Are Mounted</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-50" remap="internal">Effects of the -public Option
and NFS URLs When Mounting</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-51" remap="internal">Client-Side Failover</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-55" remap="internal">Large Files</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-105" remap="internal">How NFS Server Logging Works</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-56" remap="internal">How the WebNFS Service Works</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-57" remap="internal">WebNFS Limitations With Web
Browser Use</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-58" remap="internal">Secure NFS System</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-59" remap="internal">Secure RPC</olink></para>
</listitem>
</itemizedlist><note><para>If your system has zones enabled and you want to use this feature
in a non-global zone, see <olink targetdoc="sysadrm" remap="external"><citetitle remap="book">System Administration Guide: Solaris Containers-Resource Management and Solaris Zones</citetitle></olink> for
more information.</para>
</note><sect2 id="rfsrefer-147"><title>Version Negotiation in NFS</title><para>The NFS initiation process includes negotiating the protocol levels
for servers and clients.  If you do not specify the version level, then the
best level is selected by default. For example, if both the client and the
server can support version 3, then version 3 is used. If the client or the
server can only support version 2, then version 2 is used.</para><para>Starting in the Solaris 10 release, you can set the keywords NFS_CLIENT_VERSMIN,
NFS_CLIENT_VERSMAX, NFS_SERVER_VERSMIN, NFS_SERVER_VERSMAX in the <filename>/etc/default/nfs</filename> file. Your specified minimum and maximum values for the server
and the client would replace the default values for these keywords. For both
the client and the server the default minimum value is 2 and the default maximum
value is 4.  See <olink targetptr="rfsrefer-133" remap="internal">Keywords for the /etc/default/nfs
File</olink>. To find the version supported by the server, the NFS client
begins with the setting for NFS_CLIENT_VERSMAX and continues to try each version
until reaching the version setting for NFS_CLIENT_VERSMIN.  As soon as the
supported version is found, the process terminates.  For example, if NFS_CLIENT_VERSMAX=4
and NFS_CLIENT_VERSMIN=2, then the client attempts version 4 first, then version
3, and finally version 2. If NFS_CLIENT_VERSMIN and NFS_CLIENT_VERSMAX are
set to the same value, then the client always uses this version and does not
attempt any other version.  If the server does not offer this version, the
mount fails.</para><note><para>You can override the values that are determined by the negotiation
by using the <option role="nodash">vers</option> option with the <command>mount</command> command.
 See the <olink targetdoc="refman1m" targetptr="mount-nfs-1m" remap="external"><citerefentry><refentrytitle>mount_nfs</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> man
page.</para>
</note><para>For procedural information, refer to <olink targetptr="rfsadmin-68" remap="internal">Setting
Up NFS Services</olink>.</para>
</sect2><sect2 id="rfsrefer-134"><title>Features in NFS Version 4</title><para>Many changes have been made to NFS in version 4. This section provides
descriptions of these new features.</para><itemizedlist><listitem><para><olink targetptr="rfsrefer-148" remap="internal">Unsharing and Resharing a
File System in NFS Version 4</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-136" remap="internal">File-System Namespace in NFS
Version 4</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-137" remap="internal">Volatile File Handles in NFS
Version 4</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-138" remap="internal">Client Recovery in NFS Version
4</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-139" remap="internal">OPEN Share Support in NFS
Version 4</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-140" remap="internal">Delegation in NFS Version
4</olink></para>
</listitem><listitem><para><olink targetptr="gande" remap="internal">ACLs and nfsmapid in NFS Version
4</olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-111" remap="internal">Client-Side Failover in NFS
Version 4</olink></para>
</listitem>
</itemizedlist><note><para>Starting in the Solaris 10 release, NFS version 4 does not support
the LIPKEY/SPKM security flavor. Also, NFS version 4 does not use the <command>mountd</command>, <command>nfslogd</command>, and <command>statd</command> daemons.</para>
</note><para>For procedural information related to using NFS version 4, refer to <olink targetptr="rfsadmin-68" remap="internal">Setting Up NFS Services</olink>.</para><sect3 id="rfsrefer-148"><title>Unsharing and Resharing a File System in NFS
Version 4</title><para>With both NFS version 3 and version 4, if a client attempts to access
a file system that has been unshared, the server responds with an error code.
However, with NFS version 3 the server maintains any locks that the clients
had obtained before the file system was unshared.  Thus, when the file system
is reshared, NFS version 3 clients can access the file system as though that
file system had never been unshared.</para><para>With NFS version 4, when a file system is unshared, all the state for
any open files or file locks in that file system is destroyed. If the client
attempts to access these files or locks, the client receives an error. This
error is usually reported as an <literal>I/O</literal> error to the application.
Note, however, that resharing a currently shared file system to change options
does not destroy any of the state on the server.</para><para>For related information, refer to <olink targetptr="rfsrefer-138" remap="internal">Client
Recovery in NFS Version 4</olink> or see the <olink targetdoc="refman1m" targetptr="unshare-nfs-1m" remap="external"><citerefentry><refentrytitle>unshare_nfs</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> man page.</para>
</sect3><sect3 id="rfsrefer-136"><title>File-System Namespace in NFS Version 4</title><para>NFS version 4 servers create and maintain a pseudo-file system, which
provides clients with seamless access to all exported objects on the server.
Prior to NFS version 4, the pseudo-file system did not exist. Clients were
forced to mount each shared server file system for access. Consider the following
example.</para><figure id="rfsrefer-fig-131"><title>Views of the Server File System and the
Client File System</title><mediaobject><imageobject><imagedata entityref="servr-client-views.epsi"/>
</imageobject><textobject><simpara>The context describes the graphic.</simpara>
</textobject>
</mediaobject>
</figure><para>Note that the client cannot see the <filename>payroll</filename> directory
and the <filename>nfs4x</filename> directory, because these directories are
not exported and do not lead to exported directories. However, the <filename>local</filename> directory is visible to the client, because <filename>local</filename> is
an exported directory. The <filename>projects</filename> directory is visible
to the client, because <filename>projects</filename> leads to the exported
directory, <filename>nfs4</filename>. Thus, portions of the server namespace
that are not explicitly exported are bridged with a pseudo-file system that
views only the exported directories and those directories that lead to server
exports.</para><para>A pseudo-file system is a structure that contains only directories and
is created by the server.  The pseudo-file system permits a client to browse
the hierarchy of exported file systems.  Thus, the client's view of the pseudo-file
system is limited to paths that lead to exported file systems.</para><itemizedlist><para>Previous versions of NFS did not permit a client to traverse server
file systems without mounting each file system.  However, in NFS version 4,
the server namespace does the following:</para><listitem><para>Restricts the client's file-system view to directories that
lead to server exports.</para>
</listitem><listitem><para>Provides clients with seamless access to server exports without
requiring that the client mount each underlying file system. See the previous
example. Note, however, that different operating systems might require the
client to mount each server file system.</para>
</listitem>
</itemizedlist><para>For POSIX-related reasons, the Solaris NFS version 4 client does not
cross server file-system boundaries. When such attempts are made, the client
makes the directory appear to be empty. To remedy this situation, you must
perform a mount for each of the server's file systems.</para>
</sect3><sect3 id="rfsrefer-137"><title>Volatile File Handles in NFS Version 4</title><itemizedlist><para>File handles are created on the server and contain information that
uniquely identifies files and directories. In NFS versions 2 and 3 the server
returned persistent file handles. Thus, the client could guarantee that the
server would generate a file handle that always referred to the same file.
For example:</para><listitem><para>If a file was deleted and replaced with a file of the same
name, the server would generate a new file handle for the new file. If the
client used the old file handle, the server would return an error that the
file handle was stale.</para>
</listitem><listitem><para>If a file was renamed, the file handle would remain the same.</para>
</listitem><listitem><para>If you had to reboot the server, the file handles would remain
the same.</para>
</listitem>
</itemizedlist><para>Thus, when the server received a request from a client that included
a file handle, the resolution was straightforward and the file handle always
referred to the correct file.</para><para>This method of identifying files and directories for NFS operations
was fine for most UNIX-based servers. However, the method could not be implemented
on servers that relied on other methods of identification, such as a file's
path name. To resolve this problem, the NFS version 4 protocol permits a server
to declare that its file handles are volatile. Thus, a file handle could change.
If the file handle does change, the client must find the new file handle.</para><itemizedlist><para>Like NFS versions 2 and 3, the Solaris NFS version 4 server always provides
persistent file handles. However, Solaris NFS version 4 clients that access
non-Solaris NFS version 4 servers must support volatile file handles if the
server uses them. Specifically, when the server tells the client that the
file handle is volatile, the client must cache the mapping between path name
and file handle. The client uses the volatile file handle until it expires.
On expiration, the client does the following:</para><listitem><para>Flushes the cached information that refers to that file handle</para>
</listitem><listitem><para>Searches for that file's new file handle</para>
</listitem><listitem><para>Retries the operation</para>
</listitem>
</itemizedlist><note><para>The server always tells the client which file handles are persistent
and which file handles are volatile.</para>
</note><itemizedlist><para>Volatile file handles might expire for any of these reasons:</para><listitem><para>When you close a file</para>
</listitem><listitem><para>When the filehandle's file system migrates</para>
</listitem><listitem><para>When a client renames a file</para>
</listitem><listitem><para>When the server reboots</para>
</listitem>
</itemizedlist><para>Note that if the client is unable to find the new file handle, an error
message is put in the <filename>syslog</filename> file. Further attempts to
access this file fail with an <literal>I/O</literal> error.</para>
</sect3><sect3 id="rfsrefer-138"><title>Client Recovery in NFS Version 4</title><para>The NFS version 4 protocol is a stateful protocol. A protocol is stateful
when both the client and the server maintain current information about the
following.</para><itemizedlist><listitem><para>Open files</para>
</listitem><listitem><para>File locks</para>
</listitem>
</itemizedlist><para>When a failure occurs, such as a server crash, the client and the server
work together to reestablish the open and lock states that existed prior to
the failure.</para><para>When a server crashes and is rebooted, the server loses its state. The
client detects that the server has rebooted and begins the process of helping
the server rebuild its state. This process is known as client recovery, because
the client directs the process.</para><para>When the client discovers that the server has rebooted, the client immediately
suspends its current activity and begins the process of client recovery. 
When the recovery process starts, a message, such as the following, is displayed
in the system error log <filename>/var/adm/messages</filename>.</para><screen>NOTICE: Starting recovery server <replaceable>basil.example.company.com</replaceable></screen><para>During the recovery process, the client sends the server information
about the client's previous state.  Note, however, that during this period
the client does not send any new requests to the server.  Any new requests
to open files or set file locks must wait for the server to complete its recovery
period before proceeding.</para><para>When the client recovery process is complete, the following message
is displayed in the system error log <filename>/var/adm/messages</filename>.</para><screen>NOTICE: Recovery done for server <replaceable>basil.example.company.com</replaceable></screen><para>Now the client has successfully completed sending its state information
to the server.  However, even though the client has completed this process,
other clients might not have completed their process of sending state information
to the server.  Therefore, for a period of time, the server does not accept
any open or lock requests. This period of time, which is known as the grace
period, is designated to permit all the clients to complete their recovery.</para><para>During the grace period, if the client attempts to open any new files
or establish any new locks, the server denies the request with the <literal>GRACE</literal> error code. On receiving this error, the client must wait for the
grace period to end and then resend the request to the server.  During the
grace period the following message is displayed.</para><screen>NFS server recovering</screen><para>Note that during the grace period the commands that do not open files
or set file locks can proceed.   For example, the commands <command>ls</command> and <command>cd</command> do not open a file or set a file lock. Thus, these commands are
not  suspended. However, a command such as <command>cat</command>, which opens
a file, would be suspended until the grace period ends.</para><para>When the grace period has ended, the following message is displayed.</para><screen>NFS server recovery ok.</screen><para>The client can now send new open and lock requests to  the server.</para><para>Client recovery can fail for a variety of reasons. For example, if a
network partition exists after the server reboots, the client might not be
able to reestablish its state with the server before the grace period ends.
 When the grace period has ended, the server does not permit the client to
reestablish its state because new state operations could create conflicts.
 For example, a new file lock might conflict with an old file lock that the
client is trying to recover.  When such situations occur, the server returns
the <literal>NO_GRACE</literal> error code to the client.</para><para>If the recovery of an open operation for a particular file fails, the
client marks the file as unusable and the following message is displayed.</para><screen>WARNING: The following NFS file could not be recovered and was marked dead 
(can't reopen:  NFS status 70):  file :  <replaceable>filename</replaceable></screen><para>Note that the number <literal>70</literal> is only an example.</para><para>If reestablishing a file lock during recovery fails, the following error
message is posted.</para><screen>NOTICE: nfs4_send_siglost:  pid <replaceable>PROCESS-ID</replaceable> lost
lock on server <replaceable>SERVER-NAME</replaceable></screen><para>In this situation, the <literal>SIGLOST</literal> signal is posted to
the process. The default action for the <literal>SIGLOST</literal> signal
is to terminate the process.</para><para>For you to recover from this state, you must restart any applications
that had files open at the time of  the failure. Note that the following can
occur.</para><itemizedlist><listitem><para>Some processes that did not reopen the file could receive <literal>I/O</literal> errors.</para>
</listitem><listitem><para>Other processes that did reopen the file, or performed the
open operation after the recovery failure, are able to access the file without
any problems.</para>
</listitem>
</itemizedlist><para>Thus, some processes can access a particular file while other processes
cannot.</para>
</sect3><sect3 id="rfsrefer-139"><title><literal>OPEN</literal> Share Support in NFS
Version 4</title><itemizedlist><para>The NFS version 4 protocol provides several file-sharing modes that
the client can use to control file access by other clients. A client can specify
the following:</para><listitem><para><literal>DENY_NONE</literal> mode permits other clients read
and write access to a file.</para>
</listitem><listitem><para><literal>DENY_READ</literal> mode denies other clients read
access  to a file.</para>
</listitem><listitem><para><literal>DENY_WRITE</literal> mode denies other clients write
access to a file.</para>
</listitem><listitem><para><literal>DENY_BOTH</literal> mode denies other clients read
and write access to a file.</para>
</listitem>
</itemizedlist><para>The Solaris NFS version 4 server fully implements these file-sharing
modes. Therefore, if a client attempts to open a file in a way that conflicts
with the current share mode, the server denies the attempt by failing the
operation. When such attempts fail with the initiation of the open or create
operations, the Solaris NFS version 4 client receives a protocol error. This
error is mapped to the application error <literal>EACCES</literal>.</para><para>Even though the protocol provides several sharing modes, currently the
open operation in Solaris does not offer multiple sharing modes. When opening
a file, a Solaris NFS version 4 client can only use the <literal>DENY_NONE</literal> mode.</para><para>Also, even though the Solaris <command>fcntl</command> system call has
an <command>F_SHARE</command> command to control file sharing, the <command>fcntl</command> commands cannot be implemented correctly with NFS version 4.  If
you use these <command>fcntl</command> commands on an NFS version 4 client,
the client returns the <literal>EAGAIN</literal> error to the application.</para>
</sect3><sect3 id="rfsrefer-140"><title>Delegation in NFS Version 4</title><para>NFS version 4 provides both client support and server support for delegation.
Delegation is a technique by which the server delegates the management of
a file to a client. For example, the server could grant either a read delegation
or a write delegation to a client. Read delegations can be granted to multiple
clients at the same time, because these read delegations do not conflict with
each other. A write delegation can be granted to only one client, because
a write delegation conflicts with any file access by any other client.  While
holding a write delegation, the client would not send various operations to
the server because the client is guaranteed exclusive access to a file. Similarly,
the client would not send various operations to the server while holding a
read delegation. The reason is that the server guarantees that no client can
open the file in write mode. The effect of delegation is to greatly reduce
the interactions between the server and the client for delegated files. Therefore,
network traffic is reduced, and performance on the client and the server is
improved. Note, however, that the degree of performance improvement depends
on the kind of file interaction used by an application and the amount of network
and server congestion.</para><para>The decision about whether to grant a delegation is made entirely by
the server. A client does not request a delegation. The server makes decisions
about whether to grant a delegation, based on the access patterns for the
file. If a file has been recently accessed in write mode by several different
clients, the server might not grant a delegation. The reason is that this
access pattern indicates the potential for future conflicts.</para><para>A conflict occurs when a client accesses a file in a manner that is
inconsistent with the delegations that are currently granted for that file.
For example, if a client holds a write delegation on a file and a second client
opens that file for read or write access, the server recalls the first client's
write delegation. Similarly, if a client holds a read delegation and another
client opens the same file for writing, the server recalls the read delegation.
Note that in both situations, the second client is not granted a delegation
because a conflict now exists. When a conflict occurs, the server uses a callback
mechanism to contact the client that currently holds the delegation. On receiving
this callback, the client sends the file's updated state to the server and
returns the delegation. If the client fails to respond to the recall, the
server revokes the delegation. In such instances, the server rejects all operations
from the client for this file, and the client reports the requested operations
as failures. Generally, these failures are reported to the application as <literal>I/O</literal> errors.  To recover from these errors, the file must be closed
and then reopened. Failures from revoked delegations can occur when a network
partition exists between the client and the server while the client holds
a delegation.</para><para>Note that one server does not resolve access conflicts for a file that
is stored on another server.  Thus, an NFS server only resolves conflicts
for files that it stores. Furthermore, in response to conflicts that are caused
by clients that are running various versions of NFS, an NFS server can only
initiate recalls to the client that is running NFS version 4.  An NFS server
cannot initiate recalls for clients that are running earlier versions of NFS.</para><itemizedlist><para>The process for detecting conflicts varies. For example, unlike NFS
version 4, because version 2 and version 3 do not have an open procedure,
the conflict is detected only after the client attempts to read, write, or
lock a file. The server's response to these conflicts varies also.  For example:</para><listitem><para>For NFS version 3, the server returns the <literal>JUKEBOX</literal> error,
which causes the client to halt the access request and try again later. The
client prints the message <literal>File unavailable</literal>.</para>
</listitem><listitem><para>For NFS version 2, because an equivalent of the <literal>JUKEBOX</literal> error
does not exist, the server makes no response, which causes the client to wait
and then try again. The client prints the message <literal>NFS server not
responding</literal>.</para>
</listitem>
</itemizedlist><para>These conditions clear when the delegation conflict has been resolved.</para><para>By default, server delegation is enabled. You can disable delegation
by modifying the <filename>/etc/default/nfs</filename> file. For procedural
information, refer to <olink targetptr="rfsadmin-965" remap="internal">How to Select Different
Versions of NFS on a Server</olink>.</para><para>No keywords are required for client delegation. The NFS version 4 callback
daemon, <command>nfs4cbd</command>, provides the callback service on the client.
This daemon is started automatically whenever a mount for NFS version 4 is
enabled. By default, the client provides the necessary callback information
to the server for all Internet transports that are listed in the <filename>/etc/netconfig</filename> system file. Note that if the client is enabled for IPv6 and if
the IPv6 address for the client's name can be determined, then the callback
daemon accepts IPv6 connections.</para><para>The callback daemon uses a transient program number and a dynamically
assigned port number.  This information is provided to the server, and the
server tests the callback path before granting any delegations. If the callback
path does not test successfully, the server does not grant delegations, which
is the only externally visible behavior.</para><para>Note that because callback information is embedded within an NFS version
4 request, the server is unable to contact the client through a device that
uses Network Address Translation (NAT).  Also, the callback daemon uses a
dynamic port number. Therefore, the server might not be able to traverse a
firewall, even if that firewall enables normal NFS traffic on port 2049. 
In such situations, the server does not grant delegations.</para>
</sect3><sect3 id="gande"><title>ACLs and <command>nfsmapid</command> in NFS Version
4</title><para>An access control list (ACL) provides better file security by
enabling the owner of a file to define file permissions for the file owner,
the group, and other specific users and groups. ACLs are set on the server
and the client by using the <command>setfacl</command> command. See the <olink targetdoc="refman1" targetptr="setfacl-1" remap="external"><citerefentry><refentrytitle>setfacl</refentrytitle><manvolnum>1</manvolnum></citerefentry></olink> man page. In NFS version 4,
the ID mapper, <command>nfsmapid</command>, is used to map user or group IDs
in ACL entries on a server to user or group IDs in ACL entries on a client.
The reverse is also true. The user and group IDs in the ACL entries must exist
on both the client and the server.</para><sect4 id="gandx"><title>Reasons for ID Mapping to Fail</title><itemizedlist><para>The following situations can cause ID mapping to fail:</para><listitem><para>If the user or group that exists in an ACL entry on the server
cannot be mapped to a valid user or group on the client, the user is not allowed
to read the ACL on the client.</para><para>For example, when you issue the <command>ls</command> <option>l</option> command,
you receive the error message, <literal>Permission denied</literal>, for the
files with user or group ID ACL entities that cannot be mapped from the server
to the client. The ID mapper was unable to map a user or group in the ACL.
If the ID mapper had been able to map the user or group, a plus (+) sign would
have appeared after the permissions in the files list that is produced by <command>ls</command> <option>l</option>. For example:</para><screen>% ls -l
-rw-r--rw-+   1 luis    staff      11968 Aug 12  2005 foobar</screen><para>Similarly, the <command>getfacl</command> command can return the <literal>Permission denied</literal> error message for the same reason. For more information
about this command, see the <olink targetdoc="refman1" targetptr="getfacl-1" remap="external"><citerefentry><refentrytitle>getfacl</refentrytitle><manvolnum>1</manvolnum></citerefentry></olink> man
page.</para>
</listitem><listitem><para>If the user or group ID in any ACL entry that is set on the
client cannot be mapped to a valid user or group ID on the server, the <command>setfacl</command> command can fail and return the <literal>Permission denied</literal> error
message.</para>
</listitem><listitem><para>If the client and server have mismatched NFSMAPID_DOMAIN values,
ID mapping fails. For more information, see <olink targetptr="rfsrefer-133" remap="internal">Keywords
for the /etc/default/nfs File</olink>.</para>
</listitem>
</itemizedlist>
</sect4><sect4 id="ganeh"><title>Avoiding ID Mapping Problems With ACLs</title><itemizedlist><para>To avoid ID mapping problems, do the following:</para><listitem><para>Make sure that the value for NFSMAPID_DOMAIN is set correctly
in the <filename>/etc/default/nfs</filename> file.</para>
</listitem><listitem><para>Make sure that all user and group IDs in the ACL entries exist
on both the NFS version 4 client and server.</para>
</listitem>
</itemizedlist>
</sect4><sect4 id="gandj"><title>Checking for Unmapped User or Group IDs</title><para>To determine if any user or group cannot be mapped on the server or
client, use the following script:</para><screen>#! /usr/sbin/dtrace -Fs

sdt:::nfs4-acl-nobody
{
     printf("validate_idmapping: (%s) in the ACL could not be mapped!", 
stringof(arg0));
}</screen><note><para>The probe name that is used in this script is an interface that
could change in the future. For more information, see <olink targetdoc="dynmctrcggd" targetptr="chp-stab-1" remap="external"><citetitle remap="section">Stability Levels</citetitle> in <citetitle remap="book">Solaris Dynamic Tracing Guide</citetitle></olink>.</para>
</note>
</sect4><sect4 id="gandn"><title>Additional Information About ACLs or <command>nfsmapid</command></title><itemizedlist><para>See the following:</para><listitem><para><olink targetdoc="sysadv6" targetptr="secfile-30" remap="external"><citetitle remap="section">Protecting Files With ACLs (Task Map)</citetitle> in <citetitle remap="book">System Administration Guide: Security Services</citetitle></olink></para>
</listitem><listitem><para><olink targetptr="rfsrefer-118" remap="internal">nfsmapid Daemon</olink></para>
</listitem>
</itemizedlist>
</sect4>
</sect3>
</sect2><sect2 id="rfsrefer-47"><title>UDP and TCP Negotiation</title><para>During initiation, the transport protocol is also negotiated. By default,
the first connection-oriented transport that is supported on both the client
and the server is selected. If this selection does not succeed, the first
available connectionless transport protocol is used. The transport protocols
that are supported on a system are listed in <filename>/etc/netconfig</filename>.
TCP is the connection-oriented transport protocol that is supported by the
release. UDP is the connectionless transport protocol.</para><para>When both the NFS protocol version and the transport protocol are determined
by negotiation, the NFS protocol version is given precedence over the transport
protocol. The NFS version 3 protocol that uses UDP is given higher precedence
than the NFS version 2 protocol that is using TCP. You can manually select
both the NFS protocol version and the transport protocol with the <command>mount</command> command.
See the <olink targetdoc="refman1m" targetptr="mount-nfs-1m" remap="external"><citerefentry><refentrytitle>mount_nfs</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> man
page. Under most conditions, allow the negotiation to select the best options.</para>
</sect2><sect2 id="rfsrefer-48"><title>File Transfer Size Negotiation</title><para>The file transfer size establishes the size of the buffers that are
used when transferring data between the client and the server. In general,
larger transfer sizes are better. The NFS version 3 protocol has an unlimited
transfer size. However, starting with the Solaris 2.6 release, the software
bids a default buffer size of 32 Kbytes. The client can bid a smaller transfer
size at mount time if needed, but under most conditions this bid is not necessary.</para><para>The transfer size is not negotiated with systems that use the NFS version
2 protocol. Under this condition, the maximum transfer size is set to 8 Kbytes.</para><para>You can use the <option>rsize</option> and <option>wsize</option> options
to set the transfer size manually with the <command>mount</command> command.
You might need to reduce the transfer size for some PC clients. Also, you
can increase the transfer size if the NFS server is configured to use larger
transfer sizes.</para><note><para>Starting in the Solaris 10 release, restrictions on wire transfer
sizes have been relaxed.  The transfer size is based on the capabilities of
the underlying transport.  For example, the NFS transfer limit for UDP is
still 32 Kbytes.  However, because TCP is a streaming protocol without the
datagram limits of UDP, maximum transfer sizes over TCP have been increased
to 1 Mbyte.</para>
</note>
</sect2><sect2 id="rfsrefer-49"><title>How File Systems Are Mounted</title><para>The following description applies to NFS version 3 mounts. The NFS version
4 mount process does not include the portmap service nor does it include the <command>MOUNT</command> protocol.</para><para>When a client needs to mount a file system from a server, the client
must obtain a file handle from the server. The file handle must correspond
to the file system. This process requires that several transactions occur
between the client and the server. In this example, the client is attempting
to mount <filename>/home/terry</filename> from the server. A <command>snoop</command> trace
for this transaction follows.</para><screen>client -> server PORTMAP C GETPORT prog=100005 (MOUNT) vers=3 proto=UDP
server -> client PORTMAP R GETPORT port=33492
client -> server MOUNT3 C Null
server -> client MOUNT3 R Null 
client -> server MOUNT3 C Mount /export/home9/terry
server -> client MOUNT3 R Mount OK FH=9000 Auth=unix
client -> server PORTMAP C GETPORT prog=100003 (NFS) vers=3 proto=TCP
server -> client PORTMAP R GETPORT port=2049
client -> server NFS C NULL3
server -> client NFS R NULL3 
client -> server NFS C FSINFO3 FH=9000
server -> client NFS R FSINFO3 OK
client -> server NFS C GETATTR3 FH=9000
server -> client NFS R GETATTR3 OK</screen><para>In this trace, the client first requests the mount port number from
the portmap service on the NFS server. After the client receives the mount
port number (<literal>33492</literal>), that number is used to test the availability
of the service on the server. After the client has determined that a service
is running on that port number, the client then makes a mount request. When
the server responds to this request, the server includes the file handle for
the file system (<literal>9000</literal>) being mounted. The client then sends
a request for the NFS port number. When the client receives the number from
the server, the client tests the availability of the NFS service (<command>nfsd</command>).
Also, the client requests NFS information about the file system that uses
the file handle.</para><para>In the following trace, the client is mounting the file system
with the <option role="nodash">public</option> option.</para><screen>client -> server NFS C LOOKUP3 FH=0000 /export/home9/terry
server -> client NFS R LOOKUP3 OK FH=9000
client -> server NFS C FSINFO3 FH=9000
server -> client NFS R FSINFO3 OK
client -> server NFS C GETATTR3 FH=9000
server -> client NFS R GETATTR3 OK</screen><para>By using the default public file handle (which is <literal>0000</literal>),
all the transactions to obtain information from the portmap service and to
determine the NFS port number are skipped.</para><note><para>NFS version 4 provides support for volatile file handles. For
more information, refer to <olink targetptr="rfsrefer-137" remap="internal">Volatile File Handles
in NFS Version 4</olink>.</para>
</note>
</sect2><sect2 id="rfsrefer-50"><title>Effects of the <option>public</option> Option
and NFS URLs When Mounting</title><para>Using the <option>public</option> option can create conditions that
cause a mount to fail. Adding an NFS URL can also confuse the situation. The
following list describes the specifics of how a file system is mounted when
you use these options.</para><itemizedlist mark="none"><listitem><para><emphasis role="strong">Public option with NFS URL</emphasis> &#8211;
Forces the use of the public file handle. The mount fails if the public file
handle is not supported.</para>
</listitem><listitem><para><emphasis role="strong">Public option with regular path</emphasis> &#8211;
Forces the use of the public file handle. The mount fails if the public file
handle is not supported.</para>
</listitem><listitem><para><emphasis role="strong">NFS URL only</emphasis> &#8211; Use
the public file handle if this file handle is enabled on the NFS server. If
the mount fails when using the public file handle, then try the mount with
the <literal>MOUNT</literal> protocol.</para>
</listitem><listitem><para><emphasis role="strong">Regular path only</emphasis> &#8211;
Do not use the public file handle. The <literal>MOUNT</literal> protocol is
used.</para>
</listitem>
</itemizedlist>
</sect2><sect2 id="rfsrefer-51"><title>Client-Side Failover</title><para>By using client-side failover, an NFS client can be aware of multiple
servers that are making the same data available and can switch to an alternate
server when the current server is unavailable. The file system can become
unavailable if one of the following occurs. </para><itemizedlist><listitem><para>If the file system is connected to a server that crashes</para>
</listitem><listitem><para>If the server is overloaded</para>
</listitem><listitem><para>If a network fault occurs</para>
</listitem>
</itemizedlist><para>The failover, under these conditions, is normally transparent to the
user. Thus, the failover can occur at any time without disrupting the processes
that are running on the client.</para><para>Failover requires that the file system be mounted read-only. The file
systems must be identical for the failover to occur successfully. See <olink targetptr="rfsrefer-53" remap="internal">What Is a Replicated File System?</olink> for a description
of what makes a file system identical. A static file system or a file system
that is not changed often is the best candidate for failover. </para><para>You cannot use CacheFS and client-side failover on the same NFS mount.
Extra information is stored for each CacheFS file system. This information
cannot be updated during failover, so only one of these two features can be
used when mounting a file system.</para><para>The number of replicas that need to be established for every file system
depends on many factors. Ideally, you should have a minimum of two servers.
Each server should support multiple subnets. This setup is better than having
a unique server on each subnet. The process requires that each listed server
be checked.  Therefore, if more servers are listed, each mount is slower.</para><sect3 id="rfsrefer-52"><title>Failover Terminology</title><para>To fully comprehend the process, you need to understand two terms.</para><itemizedlist><listitem><para><emphasis>failover</emphasis> &#8211; The process of selecting
a server from a list of servers that support a replicated file system. Normally,
the next server in the sorted list is used, unless it fails to respond.</para>
</listitem><listitem><para><emphasis>remap</emphasis> &#8211; To use a new server. Through
normal use, the clients store the path name for each active file on the remote
file system. During the remap, these path names are evaluated to locate the
files on the new server.</para>
</listitem>
</itemizedlist>
</sect3><sect3 id="rfsrefer-53"><title>What Is a Replicated File System?</title><para>For the purposes of failover, a file system can be called a <emphasis>replica</emphasis> when each file is the same size and has the same file size or
file type as the original file system. Permissions, creation dates, and other
file attributes are not considered. If the file size or file types are different,
the remap fails and the process hangs until the old server becomes available.
In NFS version 4, the behavior is different. See <olink targetptr="rfsrefer-111" remap="internal">Client-Side Failover in NFS Version 4</olink>.</para><itemizedlist><para>You can maintain a replicated file system by using <command>rdist</command>, <command>cpio</command>, or another file transfer mechanism. Because updating the replicated
file systems causes inconsistency, for best results consider these precautions:</para><listitem><para>Renaming the old version of the file before installing a new
version of the file</para>
</listitem><listitem><para>Running the updates at night when client usage is low</para>
</listitem><listitem><para>Keeping the updates small</para>
</listitem><listitem><para>Minimizing the number of copies</para>
</listitem>
</itemizedlist>
</sect3><sect3 id="rfsrefer-54"><title>Failover and NFS Locking</title><para>Some software packages require read locks on files. To prevent these
products from breaking, read locks on read-only file systems are allowed but
are visible to the client side only. The locks persist through a remap because
the server does not &#8220;know&#8221; about the locks. Because the files
should not change, you do not need to lock the file on the server side.</para>
</sect3><sect3 id="rfsrefer-111"><title>Client-Side Failover in NFS Version 4</title><para>In NFS version 4, if a replica cannot be established because the file
sizes are different or the file types are not the same, then the following
happens.</para><itemizedlist><listitem><para>The file is marked dead.</para>
</listitem><listitem><para>A warning is printed.</para>
</listitem><listitem><para>The application receives a system call failure.</para>
</listitem>
</itemizedlist><note><para>If you restart the application and try again to access the file,
you should be successful.</para>
</note><para>In NFS version 4, you no longer receive replication errors for directories
of different sizes. In prior versions of NFS,  this condition was treated
as an error and would impede the remapping process.</para><para>Furthermore, in NFS version 4, if a directory read operation is unsuccessful,
the operation is performed by the next listed server. In previous versions
of NFS, unsuccessful read operations would cause the remap to fail and the
process to hang until the original server was available.</para>
</sect3>
</sect2><sect2 id="rfsrefer-55"><title>Large Files</title><para>Starting with the Solaris 2.6 release, the Solaris OS supports files
that are over 2 Gbytes. By default, UFS file systems are mounted with the <option>largefiles</option> option to support the new capability. Previous releases
cannot handle files of this size. See <olink targetptr="rfsadmin-75" remap="internal">How to
Disable Large Files on an NFS Server</olink> for instructions.</para><para>If the server's file system is mounted with the <option>largefiles</option> option,
a Solaris 2.6 NFS client can access large files without the need for changes.
However, not all Solaris 2.6 commands can handle these large files. See <olink targetdoc="refman5" targetptr="largefile-5" remap="external"><citerefentry><refentrytitle>largefile</refentrytitle><manvolnum>5</manvolnum></citerefentry></olink> for a list
of the commands that can handle the large files. Clients that cannot support
the NFS version 3 protocol with the large file extensions cannot access any
large files. Although clients that run the Solaris 2.5 release can use the
NFS version 3 protocol, large file support was not included in that release.</para>
</sect2><sect2 id="rfsrefer-105"><title>How NFS Server Logging Works</title><para>NFS server logging provides records of NFS reads and writes, as well
as operations that modify the file system. This data can be used to track
access to information. In addition, the records can provide a quantitative
way to measure interest in the information.</para><itemizedlist><para>When a file system with logging enabled is accessed, the kernel writes
raw data into a buffer file. This data includes the following:</para><listitem><para>A timestamp</para>
</listitem><listitem><para>The client IP address</para>
</listitem><listitem><para>The UID of the requester</para>
</listitem><listitem><para>The file handle of the file or directory object that is being
accessed</para>
</listitem><listitem><para>The type of operation that occurred</para>
</listitem>
</itemizedlist><para>The <command>nfslogd</command> daemon converts this raw data into ASCII
records that are stored in log files. During the conversion, the IP addresses
are modified to host names and the UIDs are modified to logins if the name
service that is enabled can find matches. The file handles are also converted
into path names. To accomplish the conversion, the daemon tracks the file
handles and stores information in a separate file handle-to-path table. That
way, the path does not have to be identified again each time a file handle
is accessed. Because no changes to the mappings are made in the file handle-to-path
table if <command>nfslogd</command> is turned off, you must keep the daemon
running.</para><note><para>Server logging is not supported in NFS version 4.</para>
</note>
</sect2><sect2 id="rfsrefer-56"><title>How the WebNFS Service Works</title><para>The WebNFS service makes files in a directory available to clients by
using a public file handle. A file handle is an address that is generated
by the kernel that identifies a file for NFS clients. The <emphasis>public
file handle</emphasis> has a predefined value, so the server does not need
to generate a file handle for the client. The ability to use this predefined
file handle reduces network traffic by eliminating the <literal>MOUNT</literal> protocol.
This ability should also accelerate processes for the clients.</para><para>By default, the public file handle on an NFS server is established on
the root file system. This default provides WebNFS access to any clients that
already have mount privileges on the server. You can change the public file
handle to point to any file system by using the <command>share</command> command.</para><para>When the client has the file handle for the file system, a <command>LOOKUP</command> is
run to determine the file handle for the file to be accessed. The NFS protocol
allows the evaluation of only one path name component at a time. Each additional
level of directory hierarchy requires another <command>LOOKUP</command>. A
WebNFS server can evaluate an entire path name with a single multi-component
lookup transaction when the <command>LOOKUP</command> is relative to the public
file handle. Multi-component lookup enables the WebNFS server to deliver the
file handle to the desired file without exchanging the file handles for each
directory level in the path name.</para><para>In addition, an NFS client can initiate concurrent downloads over a
single TCP connection. This connection provides quick access without the additional
load on the server that is caused by setting up multiple connections. Although
web browser applications support concurrent downloading of multiple files,
each file has its own connection. By using one connection, the WebNFS software
reduces the overhead on the server.</para><para>If the final component in the path name is a symbolic link to another
file system, the client can access the file if the client already has access
through normal NFS activities.</para><para>Normally, an NFS URL is evaluated relative to the public file handle.
The evaluation can be changed to be relative to the server's root file system
by adding an additional slash to the beginning of the path. In this example,
these two NFS URLs are equivalent if the public file handle has been established
on the <filename>/export/ftp</filename> file system.</para><screen>nfs://server/junk
nfs://server//export/ftp/junk</screen><note><para>The NFS version 4 protocol is preferred over the WebNFS service.
NFS version 4 fully integrates all the security negotiation that was added
to the MOUNT protocol and the WebNFS service.</para>
</note>
</sect2><sect2 id="egcod"><title>How WebNFS Security Negotiation Works</title><para>The Solaris 8 release includes a new protocol that enables a WebNFS
client to negotiate a selected security mechanism with a WebNFS server.  The
new protocol uses security negotiation multi-component lookup, which is an
extension to the multi-component lookup that was used in earlier versions
of the WebNFS protocol.</para><para>The WebNFS client initiates the process by making a regular multi&#8211;component
lookup request by using the public file handle. Because the client has no
knowledge of how the path is protected by the server, the default security
mechanism is used. If the default security mechanism is not sufficient, the
server replies with an <literal>AUTH_TOOWEAK</literal> error. This reply indicates
that the default mechanism is not valid. The client needs to use a stronger
default mechanism.</para><para>When the client receives the <literal>AUTH_TOOWEAK</literal> error,
the client sends a request to the server to determine which security mechanisms
are required. If the request succeeds, the server responds with an array of
security mechanisms that are required for the specified path. Depending on
the size of the array of security mechanisms, the client might have to make
more requests to obtain the complete array. If the server does not support
WebNFS security negotiation, the request fails.</para><para>After a successful request, the WebNFS client selects the first security
mechanism from the array that the client supports. The client then issues
a regular multi-component lookup request by using the selected security mechanism
to acquire the file handle. All subsequent NFS requests are made by using
the selected security mechanism and the file handle.</para><note><para>The NFS version 4 protocol is preferred over the WebNFS service.
NFS version 4 fully integrates all the security negotiation that was added
to the MOUNT protocol and the WebNFS service.</para>
</note>
</sect2><sect2 id="rfsrefer-57"><title>WebNFS Limitations With Web Browser Use</title><itemizedlist><para>Several functions that a web site that uses HTTP can provide are not
supported by the WebNFS software. These differences stem from the fact that
the NFS server only sends the file, so any special processing must be done
on the client. If you need to have one web site configured for both WebNFS
and HTTP access, consider the following issues:</para><listitem><para>NFS browsing does not run CGI scripts. So, a file system with
an active web site that uses many CGI scripts might not be appropriate for
NFS browsing.</para>
</listitem><listitem><para>The browser might start different viewers to handle files
in different file formats. Accessing these files through an NFS URL starts
an external viewer if the file type can be determined by the file name. The
browser should recognize any file name extension for a standard MIME type
when an NFS URL is used. The WebNFS software does not check inside the file
to determine the file type. So, the only way to determine a file type is by
the file name extension.</para>
</listitem><listitem><para>NFS browsing cannot utilize server-side image maps (clickable
images). However, NFS browsing can utilize client-side image maps (clickable
images) because the URLs are defined with the location. No additional response
is required from the document server.</para>
</listitem>
</itemizedlist>
</sect2><sect2 id="rfsrefer-58"><title>Secure NFS System</title><para>The NFS environment is a powerful way and convenient way to share
file systems on a network of different computer architectures and operating
systems. However, the same features that make sharing file systems through
NFS operation convenient also pose some security problems. Historically, most
NFS implementations have used UNIX (or AUTH_SYS) authentication, but stronger
authentication methods such as AUTH_DH have also been available. When using
UNIX authentication, an NFS server authenticates a file request by authenticating
the computer that makes the request, but not the user. Therefore, a client
user can run <command>su</command> and impersonate the owner of a file. If
DH authentication is used, the NFS server authenticates the user, making this
sort of impersonation much harder.</para><para>With root access and knowledge of network programming, anyone can introduce
arbitrary data into the network and extract any data from the network. The
most dangerous attacks are those attacks that involve the introduction of
data. An example is the impersonation of a user by generating the right packets
or by recording &#8220;conversations&#8221; and replaying them later. These
attacks affect data integrity. Attacks that involve passive eavesdropping,
which is merely listening to network traffic without impersonating anybody,
are not as dangerous, because data integrity is not compromised. Users can
protect the privacy of sensitive information by encrypting data that is sent
over the network. </para><para>A common approach to network security problems is to leave the solution
to each application. A better approach is to implement a standard authentication
system at a level that covers all applications. </para><para>The Solaris operating system includes an authentication system at the
level of the remote procedure call (RPC), which is the mechanism on which
the NFS operation is built. This system, known as Secure RPC, greatly improves
the security of network environments and provides additional security to services
such as the NFS system. When the NFS system uses the facilities that are provided
by Secure RPC, it is known as a Secure NFS system. </para>
</sect2><sect2 id="rfsrefer-59"><title>Secure RPC</title><para>Secure RPC is fundamental to the Secure NFS system.  The goal
of Secure RPC is to build a system that is at minimum as secure as a time-sharing
system.  In a time-sharing system all users share a single computer. A time-sharing
system authenticates a user through a login password. With Data Encryption
Standard (DES) authentication, the same authentication process is completed.
Users can log in on any remote computer just as users can log in on a local
terminal. The users' login passwords are their assurance of network security.
In a time-sharing environment, the system administrator has an ethical obligation
not to change a password to impersonate someone. In Secure RPC, the network
administrator is trusted not to alter entries in a database that stores <emphasis>public keys</emphasis>.</para><para>You need to be familiar with two terms to understand an RPC authentication
system: credentials and verifiers. Using ID badges as an example, the credential
is what identifies a person: a name, address, and birthday. The verifier is
the photo that is attached to the badge. You can be sure that the badge has
not been stolen by checking the photo on the badge against the person who
is carrying the badge. In RPC, the client process sends both a credential
and a verifier to the server with each RPC request. The server sends back
only a verifier because the client already &#8220;knows&#8221; the server's
credentials. </para><para>RPC's authentication is open ended, which means that a variety
of authentication systems can be plugged into it, such as UNIX, DH, and KERB. </para><para>When UNIX authentication is used by a network service, the credentials
contain the client's host name, UID, GID, and group-access list. However,
the verifier contains nothing. Because no verifier exists, a superuser could
falsify appropriate credentials by using commands such as <command>su</command>.
Another problem with UNIX authentication is that UNIX authentication assumes
all computers on a network are UNIX computers. UNIX authentication breaks
down when applied to other operating systems in a heterogeneous network. </para><para>To overcome the problems of UNIX authentication, Secure RPC uses
DH authentication.     </para><sect3 id="rfsrefer-60"><title>DH Authentication</title><para>DH authentication uses the Data Encryption Standard (DES) and
Diffie-Hellman public-key cryptography to authenticate both users and computers
in the network. DES is a standard encryption mechanism. Diffie-Hellman public-key
cryptography is a cipher system that involves two keys: one public and one
secret. The public keys and secret keys are stored in the namespace. NIS stores
the keys in the public-key map. These maps contain the public key and secret
key for all potential users. See the <olink targetdoc="sysadv5" remap="external"><citetitle remap="book">System Administration Guide: Naming and Directory Services (DNS, NIS, and LDAP)</citetitle></olink>  for more information about how to set
up the maps. </para><itemizedlist><para>The security of DH authentication is based on a sender's ability to
encrypt the current time, which the receiver can then decrypt and check against
its own clock. The timestamp is encrypted with DES. The requirements for this
scheme to work are as follows:</para><listitem><para>The two agents must agree on the current time.</para>
</listitem><listitem><para>The sender and receiver must be using the same encryption
key.</para>
</listitem>
</itemizedlist><para>If a network runs a time-synchronization program, the time on
the client and the server is synchronized automatically. If a time-synchronization
program is not available, timestamps can be computed by using the server's
time instead of the network time. The client asks the server for the time
before starting the RPC session, then computes the time difference between
its own clock and the server's. This difference is used to offset the client's
clock when computing timestamps. If the client and server clocks become unsynchronized
the server begins to reject the client's requests. The DH authentication system
on the client resynchronizes with the server. </para><para>The client and server arrive at the same encryption key by generating
a random <emphasis>conversation key</emphasis>, also known as the <emphasis>session
key</emphasis>, and by using public-key cryptography to deduce a <emphasis>common
key</emphasis>. The common key is a key that only the client and server are
capable of deducing. The conversation key is used to encrypt and decrypt the
client's timestamp. The common key is used to encrypt and decrypt the conversation
key. </para>
</sect3><sect3 id="rfsrefer-61"><title>KERB Authentication</title><para>Kerberos is an authentication system that was developed at MIT. Kerberos
offers a variety of encryption types, including DES. Kerberos support is no
longer supplied as part of Secure RPC, but starting in the Solaris 9 release
a server-side and client-side implementation is included. See <olink targetdoc="sysadv6" targetptr="intro-1" remap="external">Chapter 20, <citetitle remap="chapter">Introduction to the Kerberos Service,</citetitle> in <citetitle remap="book">System Administration Guide: Security Services</citetitle></olink> for more information about the
implementation of Kerberos authentication.</para>
</sect3><sect3 id="rfsrefer-62"><title>Using Secure RPC With NFS</title><itemizedlist><para>Be aware of the following points if you plan to use Secure RPC:
  </para><listitem><para>If a server crashes when no one is around (after a power failure,
for example), all the secret keys that are stored on the system are deleted.
Now no process can access secure network services or mount an NFS file system.
The important processes during a reboot are usually run as <literal>root</literal>.
Therefore, these processes would work if root's secret key were stored away,
but nobody is available to type the password that decrypts it. <command>keylogin
-r</command> allows <filename>root</filename> to store the clear secret key
in <filename>/etc/.rootkey</filename>, which <command>keyserv</command> reads. </para>
</listitem><listitem><para>Some systems boot in single-user mode, with a root login shell
on the console and no password prompt. Physical security is imperative in
such cases. </para>
</listitem><listitem><para>Diskless computer booting is not totally secure. Somebody could
impersonate the boot server and boot a devious kernel that, for example, makes
a record of your secret key on a remote computer. The Secure NFS system provides
protection only after the kernel and the key server are running. Otherwise,
no way exists to authenticate the replies that are given by the boot server.
This limitation could be a serious problem, but the limitation requires a
sophisticated attack, using kernel source code. Also, the crime would leave
evidence. If you polled the network for boot servers, you would discover the
devious boot server's location. </para>
</listitem><listitem><para>Most setuid programs are owned by <literal>root</literal>. If
the secret key for <literal>root</literal> is stored in <filename>/etc/.rootkey</filename>,
these programs behave as they always have. If a setuid program is owned by
a user, however, the setuid program might not always work. For example, suppose
that a setuid program is owned by <literal>dave</literal> and <literal>dave</literal> has
not logged into the computer since it booted. The program would not be able
to access secure network services. </para>
</listitem><listitem><para>If you log in to a remote computer (using <command>login</command>, <command>rlogin</command>, or <command>telnet</command>) and use <command>keylogin</command> to
gain access, you give access to your account. The reason is that your secret
key is passed to that computer's key server, which then stores your secret
key. This process is only a concern if you do not trust the remote computer.
If you have doubts, however, do not log in to a remote computer if the remote
computer requires a password. Instead, use the NFS environment to mount file
systems that are shared by the remote computer. As an alternative, you can
use <command>keylogout</command> to delete the secret key from the key server.</para>
</listitem><listitem><para>If a home directory is shared with the <option>o sec=dh</option> option,
remote logins can be a problem. If the <filename>/etc/hosts.equiv</filename> or <filename>~/.rhosts</filename> files are not set to prompt for a password, the login
succeeds. However, the users cannot access their home directories because
no authentication has occurred locally. If the user is prompted for a password,
the user has access to his or her home directory if the password matches the
network password. </para>
</listitem>
</itemizedlist>
</sect3>
</sect2>
</sect1><sect1 id="rfsrefer-66"><title>Autofs Maps</title><itemizedlist><para>Autofs uses three types of maps:</para><listitem><para>Master map</para>
</listitem><listitem><para>Direct maps </para>
</listitem><listitem><para>Indirect maps</para>
</listitem>
</itemizedlist><sect2 id="rfsrefer-67"><title>Master Autofs Map</title><para>The <literal>auto_master</literal> map associates a directory
with a map. The map is a master list that specifies all the maps that autofs
should check. The following example shows what an <literal>auto_master</literal> file
could contain. </para><example id="rfsrefer-ex-68"><title>Sample <filename>/etc/auto_master</filename> File</title><screen># Master map for automounter 
# 
+auto_master 
/net            -hosts           -nosuid,nobrowse 
/home           auto_home        -nobrowse 
/-              auto_direct     -ro  </screen>
</example><para>This example shows the generic <filename>auto_master</filename> file
with one addition for the <literal>auto_direct</literal> map. Each line in
the master map <filename>/etc/auto_master</filename> has the following syntax: </para><para><replaceable>mount-point map-name</replaceable> <literal>[</literal> <replaceable>mount-options</replaceable> <literal>]</literal></para><variablelist><varlistentry><term><replaceable>mount-point</replaceable></term><listitem><para><replaceable>mount-point</replaceable> is the full (absolute)
path name of a directory. If the directory does not exist, autofs creates
the directory if possible. If the directory exists and is not empty, mounting
on the directory hides its contents. In this situation, autofs issues a warning.</para><para>The notation <literal>/-</literal> as a mount point indicates that this
particular map is a direct map. The notation also means that no particular
mount point is associated with the map. </para>
</listitem>
</varlistentry><varlistentry><term><replaceable>map-name</replaceable></term><listitem><para><replaceable>map-name</replaceable> is the map autofs uses to
find directions to locations, or mount information. If the name is preceded
by a slash (/), autofs interprets the name as a local file. Otherwise, autofs
searches for the mount information by using the search that is specified in
the name-service switch configuration file (<filename>/etc/nsswitch.conf</filename>).
Special maps are also used for <literal>/net</literal>. See <olink targetptr="rfsrefer-70" remap="internal">Mount Point /net</olink> for more information. </para>
</listitem>
</varlistentry><varlistentry><term><replaceable>mount-options</replaceable></term><listitem><para><replaceable>mount-options</replaceable> is an optional, comma-separated
list of options that apply to the mounting of the entries that are specified
in map-name, unless the entries in map-name list other options. Options for
each specific type of file system are listed in the mount man page for that
file system. For example, see the <olink targetdoc="refman1m" targetptr="mount-nfs-1m" remap="external"><citerefentry><refentrytitle>mount_nfs</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> man page for NFS-specific
mount options. For NFS-specific mount points, the <literal>bg</literal> (background)
and <literal>fg</literal> (foreground) options do not apply.</para>
</listitem>
</varlistentry>
</variablelist><para>A line that begins with <literal>#</literal> is a comment. All
the text that follows until the end of the line is ignored.      </para><para>To split long lines into shorter ones, put a backslash (<literal>\</literal>)
at the end of the line. The maximum number of characters of an entry is 1024.
  </para><note><para>If the same mount point is used in two entries, the first entry
is used by the <command>automount</command> command. The second entry is ignored.</para>
</note><sect3 id="rfsrefer-69"><title>Mount Point <filename>/home</filename></title><para>The mount point <filename>/home</filename> is the directory under
which the entries that are listed in <filename>/etc/auto_home</filename> (an
indirect map) are to be mounted.   </para><note><para>Autofs runs on all computers and supports <filename>/net</filename> and <filename>/home</filename> (automounted home directories) by default. These defaults
can be overridden by entries in the NIS <filename>auto.master</filename> map
or NIS+ <filename>auto_master</filename> table, or by local editing of the <filename>/etc/auto_master</filename> file.</para>
</note>
</sect3><sect3 id="rfsrefer-70"><title>Mount Point <filename>/net</filename></title><para>Autofs mounts under the directory <filename>/net</filename> all
the entries in the special map  <literal>-hosts</literal>. The map is a built-in
map that uses only the hosts database. Suppose that the computer <literal>gumbo</literal> is
in the hosts database and it exports any of its file systems. The following
command changes the current directory to the root directory of the computer <literal>gumbo</literal>.</para><screen>% <userinput>cd /net/gumbo</userinput></screen><para>Autofs can mount only the <emphasis>exported</emphasis> file systems
of host <literal>gumbo</literal>, that is, those file systems on a server
that are available to network users instead of those file systems on a local
disk. Therefore, all the files and directories on <literal>gumbo</literal> might
not be available through <filename>/net/gumbo</filename>. </para><para>With the <filename>/net</filename> method of access, the server name
is in the path and is location dependent. If you want to move an exported
file system from one server to another, the path might no longer work. Instead,
you should set up an entry in a map specifically for the file system you want
rather than use <filename>/net</filename>. </para><note><para>Autofs checks the server's export list only at mount time. After
a server's file systems are mounted, autofs does not check with the server
again until the server's file systems are automatically unmounted. Therefore,
newly exported file systems are not &#8220;seen&#8221; until the file systems
on the client are unmounted and then remounted.</para>
</note>
</sect3>
</sect2><sect2 id="rfsrefer-72"><title>Direct Autofs Maps</title><para>A direct map is an automount point. With a direct map, a direct
association exists between a mount point on the client and a directory on
the server. Direct maps have a full path name and indicate the relationship
explicitly. The following is a typical <filename>/etc/auto_direct</filename> map: </para><screen>/usr/local          -ro \
   /bin                   ivy:/export/local/sun4 \
   /share                 ivy:/export/local/share \
   /src                   ivy:/export/local/src
/usr/man            -ro   oak:/usr/man \
                          rose:/usr/man \
                          willow:/usr/man 
/usr/games          -ro   peach:/usr/games 
/usr/spool/news     -ro   pine:/usr/spool/news \
                          willow:/var/spool/news </screen><para>Lines in direct maps have the following syntax: </para><para><replaceable>key</replaceable> <literal>[</literal> <replaceable>mount-options</replaceable> <literal>]</literal> <replaceable>location</replaceable></para><variablelist><varlistentry><term><replaceable>key</replaceable></term><listitem><para><replaceable>key</replaceable> is the path name of the mount
point in a direct map. </para>
</listitem>
</varlistentry><varlistentry><term><replaceable>mount-options</replaceable></term><listitem><para><replaceable>mount-options</replaceable> is the options that
you want to apply to this particular mount. These options are required only
if the options differ from the map default. Options for each specific type
of file system are listed in the mount man page for that file system. For
example, see the <olink targetdoc="refman1m" targetptr="mount-cachefs-1m" remap="external"><citerefentry><refentrytitle>mount_cachefs</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> man page for CacheFS specific mount options. For information
about using CacheFS options with different versions of NFS, see <olink targetptr="rfsadmin-155" remap="internal">Accessing NFS File Systems Using CacheFS</olink>.</para>
</listitem>
</varlistentry><varlistentry><term><replaceable>location</replaceable></term><listitem><para><replaceable>location</replaceable> is the location of the
file system. One or more file systems are  specified as <replaceable>server</replaceable>:<replaceable>pathname</replaceable> for NFS file systems or :<replaceable>devicename</replaceable> for
High Sierra file systems (HSFS). </para><note><para>The <replaceable>pathname</replaceable> should not include an
automounted mount point. The <replaceable>pathname</replaceable> should be
the actual absolute path to the file system. For instance, the location of
a home directory should be listed as <replaceable>server</replaceable><literal>:/export/home/</literal><replaceable>username</replaceable>, not as <replaceable>server</replaceable><literal>:/home/</literal><replaceable>username</replaceable>.</para>
</note>
</listitem>
</varlistentry>
</variablelist><para>As in the master map, a line that begins with <literal>#</literal> is
a comment. All the text that follows until the end of the line is ignored.
Put a backslash at the end of the line to split long lines into shorter ones. </para><para>Of all the maps, the entries in a direct map most closely resemble the
corresponding entries in <filename>/etc/vfstab</filename>. An entry might
appear in <filename>/etc/vfstab</filename> as follows: </para><screen>dancer:/usr/local - /usr/local/tmp nfs - yes ro </screen><para>The equivalent entry appears in a direct map as follows:  </para><screen>/usr/local/tmp     -ro     dancer:/usr/local</screen><note><para>No concatenation of options occurs between the automounter maps.
Any options that are added to an automounter map override all options that
are listed in maps that are searched earlier. For instance, options that are
included in the <literal>auto_master</literal> map would be overridden by
corresponding entries in any other map.</para>
</note><para>See <olink targetptr="rfsrefer-86" remap="internal">How Autofs Selects the Nearest
Read-Only Files for Clients (Multiple Locations)</olink> for other important
features that are associated with this type of map.   </para><sect3 id="rfsrefer-73"><title>Mount Point <literal>/&#45;</literal></title><para>In <olink targetptr="rfsrefer-ex-68" remap="internal">Example 6&#8211;3</olink>,
the mount point <literal>/-</literal> tells autofs not to associate the entries
in <filename>auto_direct</filename> with any specific mount point. Indirect
maps use mount points that are defined in the <literal>auto_master</literal> file.
Direct maps use mount points that are specified in the named map. Remember,
in a direct map the key, or mount point, is a full path name. </para><para>An NIS or NIS+ <filename>auto_master</filename> file can have only one
direct map entry because the mount point must be a unique value in the namespace.
An <filename>auto_master</filename> file that is a local file can have any
number of direct map entries if entries are not duplicated.</para>
</sect3>
</sect2><sect2 id="rfsrefer-74"><title>Indirect Autofs Maps</title><para>An indirect map uses a substitution value of a key to establish
the association between a mount point on the client and a directory on the
server. Indirect maps are useful for accessing specific file systems, such
as home directories. The <filename>auto_home</filename> map is an example
of an indirect map. </para><para>Lines in indirect maps have the following general syntax: </para><para><replaceable>key</replaceable> <literal>[</literal> <replaceable>mount-options</replaceable> <literal>]</literal> <replaceable>location</replaceable></para><variablelist><varlistentry><term><replaceable>key</replaceable></term><listitem><para><replaceable>key</replaceable> is a simple name without slashes
in an indirect map.</para>
</listitem>
</varlistentry><varlistentry><term><replaceable>mount-options</replaceable></term><listitem><para><replaceable>mount-options</replaceable> is the options that
you want to apply to this particular mount. These options are required only
if the options  differ from the map default. Options for each specific type
of file system are listed in the mount man page for that file system. For
example, see the <olink targetdoc="refman1m" targetptr="mount-nfs-1m" remap="external"><citerefentry><refentrytitle>mount_nfs</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> man
page for NFS-specific mount options. </para>
</listitem>
</varlistentry><varlistentry><term><replaceable>location</replaceable></term><listitem><para><replaceable>location</replaceable> is the location of the
file system. One or more file systems are specified as <replaceable>server</replaceable>:<replaceable>pathname</replaceable>. </para><note><para>The <replaceable>pathname</replaceable> should not include an
automounted mount point. The <replaceable>pathname</replaceable> should be
the actual absolute path to the file system. For instance, the location of
a directory should be listed as <replaceable>server</replaceable><literal>:/usr/local</literal>, not as <replaceable>server</replaceable><literal>:/net/</literal><replaceable>server</replaceable><literal>/usr/local</literal>.</para>
</note>
</listitem>
</varlistentry>
</variablelist><para>As in the master map, a line that begins with <literal>#</literal> is
a comment. All the text that follows until the end of the line is ignored.
Put a backslash (<literal>\</literal>) at the end of the line to split long
lines into shorter ones. <olink targetptr="rfsrefer-ex-68" remap="internal">Example 6&#8211;3</olink> shows
an <filename>auto_master</filename> map that contains the following entry:
  </para><screen>/home      auto_home        -nobrowse    </screen><para><literal>auto_home</literal> is the name of the indirect map that
contains the entries to be mounted under <filename>/home</filename>. A typical <literal>auto_home</literal> map might contain the following: </para><screen>david                  willow:/export/home/david
rob                    cypress:/export/home/rob
gordon                 poplar:/export/home/gordon
rajan                  pine:/export/home/rajan
tammy                  apple:/export/home/tammy
jim                    ivy:/export/home/jim
linda    -rw,nosuid    peach:/export/home/linda</screen><para>As an example, assume that the previous map is on host <literal>oak</literal>.
Suppose that the user <literal>linda</literal> has an entry in the password
database that specifies her home directory as <filename>/home/linda</filename>.
Whenever <literal>linda</literal> logs in to computer <literal>oak</literal>,
autofs mounts the directory <filename>/export/home/linda</filename> that resides
on the computer <literal>peach</literal>. Her home directory is mounted read-write,
nosuid. </para><para>Assume the following conditions occur: User linda's home directory is
listed in the password database as <filename>/home/linda</filename>. Anybody,
including Linda, has access to this path from any computer that is set up
with the master map referring to the map in the previous example.</para><para>Under these conditions, user linda can run <command>login</command> or <command>rlogin</command> on any of these computers and have her home directory mounted
in place for her. </para><para>Furthermore, now Linda can also type the following command:  </para><screen>% <userinput>cd ~david</userinput></screen><para>autofs mounts David's home directory for her (if all permissions allow).</para><note><para>No concatenation of options occurs between the automounter maps.
Any options that are added to an automounter map override all options that
are listed in maps that are searched earlier. For instance, options that are
included in the <literal>auto_master</literal> map are overridden by corresponding
entries in any other map.</para>
</note><para>On a network without a name service, you have to change all the
relevant files (such as <filename>/etc/passwd</filename>) on all systems on
the network to allow Linda access to her files. With NIS, make the changes
on the NIS master server and propagate the relevant databases to the slave
servers. On a network that is running NIS+, propagating the relevant databases
to the slave servers is done automatically after the changes are made. </para>
</sect2>
</sect1><sect1 id="rfsrefer-75"><title>How Autofs Works</title><itemizedlist><para>Autofs is a client-side service that automatically mounts the
appropriate file system. The components that work together to accomplish automatic
mounting are the following:</para><listitem><para>The <command>automount</command> command</para>
</listitem><listitem><para>The <command>autofs</command> file system</para>
</listitem><listitem><para>The <command>automountd</command> daemon</para>
</listitem>
</itemizedlist><para>The automount service, <literal>svc:/system/filesystem/autofs</literal>,
which is called at system startup time, reads the master map file <filename>auto_master</filename> to create the initial set of autofs mounts. These autofs mounts
are not automatically mounted at startup time. These mounts are points under
which file systems are mounted in the future. These points are also known
as trigger nodes.</para><para>After the autofs mounts are set up, these mounts can trigger file
systems to be mounted under them. For example, when autofs receives a request
to access a file system that is not currently mounted, autofs calls <command>automountd</command>, which actually mounts the requested file system. </para><para>After initially mounting autofs mounts, the <command>automount</command> command
is used to update autofs mounts as necessary. The command compares the list
of mounts in the <filename>auto_master</filename> map with the list of mounted
file systems in the mount table file <filename>/etc/mnttab</filename> (formerly <filename>/etc/mtab</filename>). <command>automount</command> then makes the appropriate
changes. This process allows system administrators to change mount information
within <filename>auto_master</filename> and have those changes used by the
autofs processes without stopping and restarting the autofs daemon. After
the file system is mounted, further access does not require any action from <command>automountd</command> until the file system is automatically unmounted. </para><para>Unlike <command>mount</command>, <command>automount</command> does
not read the <filename>/etc/vfstab</filename> file (which is specific to each
computer) for a list of file systems to mount. The <command>automount</command> command
is controlled within a domain and on computers through the namespace or local
files. </para><para>The following is a simplified overview of how autofs works. </para><para>The automount daemon <command>automountd</command> is started at boot
time by the service <literal>svc:/system/filesystem/autofs</literal>. See <olink targetptr="rfsrefer-fig-76" remap="internal">Figure 6&#8211;3</olink>. This service also runs
the <command>automount</command> command, which reads the master map and installs
autofs mount points. See <olink targetptr="rfsrefer-79" remap="internal">How Autofs Starts
the Navigation Process (Master Map)</olink> for more information.</para><figure id="rfsrefer-fig-76"><title><literal>svc:/system/filesystem/autofs</literal> Service
Starts <command>automount</command></title><mediaobject><imageobject><imagedata entityref="fig372.epsi"/>
</imageobject><textobject><simpara>The context describes the graphic.</simpara>
</textobject>
</mediaobject>
</figure><para>Autofs is a kernel file system that supports automatic mounting and
unmounting. </para><orderedlist><para>When a request is made to access a file system at an autofs mount point,
the following occurs:</para><listitem><para>Autofs intercepts the request. </para>
</listitem><listitem><para>Autofs sends a message to the <command>automountd</command> for
the requested file system to be mounted.</para>
</listitem><listitem><para><command>automountd</command> locates the file system information
in a map, creates the trigger nodes, and performs the mount. </para>
</listitem><listitem><para>Autofs allows the intercepted request to proceed.</para>
</listitem><listitem><para>Autofs unmounts the file system after a period of inactivity.</para>
</listitem>
</orderedlist><note><para>Mounts that are managed through the autofs service should not
be manually mounted or unmounted. Even if the operation is successful, the
autofs service does not check that the object has been unmounted, resulting
in possible inconsistencies. A reboot clears all the autofs mount points.</para>
</note><sect2 id="rfsrefer-78"><title>How Autofs Navigates Through the Network (Maps)</title><para>Autofs searches a series of maps to navigate through the network.
Maps are files that contain information such as the password entries of all
users on a network or the names of all host computers on a network. Effectively,
the maps contain network-wide equivalents of UNIX administration files. Maps
are available locally or through a network name service such as NIS or NIS+.
You create maps to meet the needs of your environment by using the Solaris
Management Console tools. See <olink targetptr="rfsrefer-94" remap="internal">Modifying How
Autofs Navigates the Network (Modifying Maps)</olink>. </para>
</sect2><sect2 id="rfsrefer-79"><title>How Autofs Starts the Navigation Process (Master
Map)</title><para>The <command>automount</command> command reads the master map
at system startup. Each entry in the master map is a direct map name or an
indirect map name, its path, and its mount options, as shown in <olink targetptr="rfsrefer-fig-80" remap="internal">Figure 6&#8211;4</olink>. The specific order of
the entries is not important. <command>automount</command> compares entries
in the master map with entries in the mount table to generate a current list. </para><figure id="rfsrefer-fig-80"><title>Navigation Through the Master Map</title><mediaobject><imageobject><imagedata entityref="fig373.epsi"/>
</imageobject><textobject><simpara>The context describes the graphic.</simpara>
</textobject>
</mediaobject>
</figure>
</sect2><sect2 id="rfsrefer-82"><title>Autofs Mount Process</title><para>What the autofs service does when a mount request is triggered
depends on how the automounter maps are configured. The mount process is generally
the same for all mounts. However, the final result changes with the mount
point that is specified and the complexity of the maps. Starting with the
Solaris 2.6 release, the mount process has also been changed to include the
creation of the trigger nodes.</para><sect3 id="rfsrefer-83"><title>Simple Autofs Mount</title><para>To help explain the autofs mount process, assume that the following
files are installed.</para><screen>$ <userinput>cat /etc/auto_master</userinput>
# Master map for automounter
#
+auto_master
/net        -hosts        -nosuid,nobrowse
/home       auto_home     -nobrowse
/share      auto_share
$ <userinput>cat /etc/auto_share</userinput>
# share directory map for automounter
#
ws          gumbo:/export/share/ws</screen><para>When the <literal>/share</literal> directory is accessed, the autofs
service creates a trigger node for <literal>/share/ws</literal>, which is
an entry in <filename>/etc/mnttab</filename> that resembles the following
entry:</para><screen>-hosts  /share/ws     autofs  nosuid,nobrowse,ignore,nest,dev=###</screen><orderedlist><para>When the <literal>/share/ws</literal> directory is accessed, the autofs
service completes the process with these steps:</para><listitem><para>Checks the availability of the server's mount service.</para>
</listitem><listitem><para>Mounts the requested file system under <literal>/share</literal>.
Now the <filename>/etc/mnttab</filename> file contains the following entries.</para><screen>-hosts  /share/ws     autofs  nosuid,nobrowse,ignore,nest,dev=###
gumbo:/export/share/ws /share/ws   nfs   nosuid,dev=####    #####</screen>
</listitem>
</orderedlist>
</sect3><sect3 id="rfsrefer-84"><title>Hierarchical Mounting</title><para>When multiple layers are defined in the automounter files, the
mount process becomes more complex. Suppose that you expand the <filename>/etc/auto_shared</filename> file from the previous example to contain the following: </para><screen># share directory map for automounter
#
ws       /       gumbo:/export/share/ws
         /usr    gumbo:/export/share/ws/usr</screen><para>The mount process is basically the same as the previous example
when the <literal>/share/ws</literal> mount point is accessed. In addition,
a trigger node to the next level (<literal>/usr</literal>) is created in the <literal>/share/ws</literal> file system so that the next level can be mounted if it
is accessed. In this example, <literal>/export/share/ws/usr</literal> must
exist on the NFS server for the trigger node to be created.</para><caution><para>Do not use the <option>soft</option> option when specifying
hierarchical layers. Refer to <olink targetptr="rfsrefer-85" remap="internal">Autofs Unmounting</olink> for
an explanation of this limitation.</para>
</caution>
</sect3><sect3 id="rfsrefer-85"><title>Autofs Unmounting</title><para>The unmounting that occurs after a certain amount of idle time
is from the bottom up (reverse order of mounting). If one of the directories
at a higher level in the hierarchy is busy, only file systems below that directory
are unmounted. During the unmounting process, any trigger nodes are removed
and then the file system is unmounted. If the file system is busy, the unmount
fails and the trigger nodes are reinstalled.</para><caution><para>Do not use the <option>soft</option> option when specifying
hierarchical layers. If the <option>soft</option> option is used, requests
to reinstall the trigger nodes can time out. The failure to reinstall the
trigger nodes leaves no access to the next level of mounts. The only way to
clear this problem is to have the automounter unmount all of the components
in the hierarchy. The automounter can complete the unmounting either by waiting
for the file systems to be automatically unmounted or by rebooting the system.</para>
</caution>
</sect3>
</sect2><sect2 id="rfsrefer-86"><title>How Autofs Selects the Nearest Read-Only Files
for Clients (Multiple Locations)</title><para>The example direct map contains the following:</para><screen>/usr/local          -ro \
   /bin                   ivy:/export/local/sun4\
   /share                 ivy:/export/local/share\
   /src                   ivy:/export/local/src
/usr/man            -ro   oak:/usr/man \
                          rose:/usr/man \
                          willow:/usr/man
/usr/games          -ro   peach:/usr/games
/usr/spool/news     -ro   pine:/usr/spool/news \
                          willow:/var/spool/news </screen><para>The mount points <filename>/usr/man</filename> and <filename>/usr/spool/news</filename> list more than one location, three locations for the first mount
point, two locations for the second mount point. Any of the replicated locations
can provide the same service to any user. This procedure is sensible only
when you mount a file system that is read-only, as you must have some control
over the locations of files that you write or modify. You want to avoid modifying
files on one server on one occasion and, minutes later, modifying the &#8220;same&#8221;
file on another server. The benefit is that the best available server is used
automatically without any effort that is required by the user.</para><para>If the file systems are configured as replicas (see <olink targetptr="rfsrefer-53" remap="internal">What Is a Replicated File System?</olink>), the clients
have the advantage of using failover. Not only is the best server automatically
determined, but if that server becomes unavailable, the client automatically
uses the next-best server. Failover was first implemented in the Solaris 2.6
release. </para><para>An example of a good file system to configure as a replica is man pages.
In a large network, more than one server can export the current set of man
pages. Which server you mount the man pages from does not matter if the server
is running and exporting its file systems. In the previous example, multiple
mount locations are expressed as a list of mount locations in the map entry. </para><screen>/usr/man -ro oak:/usr/man rose:/usr/man willow:/usr/man </screen><itemizedlist><para>In this example, you can mount the man pages from the servers <literal>oak</literal>, <literal>rose</literal>, or <literal>willow</literal>. Which server is best depends
on a number of factors, including the following:</para><listitem><para>The number of servers that support a particular NFS protocol
level</para>
</listitem><listitem><para>The proximity of the server</para>
</listitem><listitem><para>The weighting</para>
</listitem>
</itemizedlist><para>During the sorting process, a count is taken of the number of servers
that support each version of the NFS protocol. Whichever version of the protocol
is supported on the most servers becomes the protocol that is used by default.
This selection provides the client with the maximum number of servers to depend
on.</para><para>After the largest subset of servers with the same version of the protocol
is found, that server list is sorted by proximity. To determine proximity
IPv4 addresses are inspected. The IPv4 addresses show which servers are in
each subnet. Servers on a local subnet are given preference over servers on
a remote subnet. Preference for the closest server reduces latency and network
traffic.</para><note><para>Proximity cannot be determined for replicas that are using IPv6
addresses.</para>
</note><para><olink targetptr="rfsrefer-fig-87" remap="internal">Figure 6&#8211;5</olink> illustrates
server proximity.</para><figure id="rfsrefer-fig-87"><title>Server Proximity</title><mediaobject><imageobject><imagedata entityref="fig374.epsi"/>
</imageobject><textobject><simpara>The context describes the graphic.</simpara>
</textobject>
</mediaobject>
</figure><para>If several servers that support the same protocol are on the local subnet,
the time to connect to each server is determined and the fastest server is
used. The sorting can also be influenced by using weighting (see <olink targetptr="egcoc" remap="internal">Autofs and Weighting</olink>).</para><itemizedlist><para>For example, if version 4 servers are more abundant, version 4 becomes
the protocol that is used by default. However, now the sorting process is
more complex. Here are some examples of how the sorting process works:</para><listitem><para>Servers on the local subnet are given preference over servers
on a remote subnet. So, if a version 3 server is on the local subnet and the
closest version 4 server is on a remote subnet, the version 3 server is given
preference. Likewise, if the local subnet consists of version 2 servers, they
are given preference over remote subnets with version 3 and version 4 servers.</para>
</listitem><listitem><para>If the local subnet consists of a varied number of version
2, version 3, and version 4 servers, more sorting is required. The automounter
prefers the highest version on the local subnet. In this instance, version
4 is the highest version. However, if the local subnet has more version 3
or version 2 servers than version 4 servers, the automounter &#8220;bids down&#8221;
from the highest version on the local subnet by one version. For example,
if the local subnet has three servers with version 4, three servers with version
3, and ten servers with version 2, a version 3 server is selected.</para>
</listitem><listitem><para>Similarly, if the local subnet consists of a varied number
of version 2 and version 3 servers, the automounter first looks at the which
version represents the highest version on the local subnet. Next, the automounter
counts the number of servers that run each version. If the highest version
on the local subnet also represents the most servers,    the highest version
is selected. If a lower version has more servers, the automounter bids down
from the highest version on the local subnet by one version. For example,
if more version 2 servers are on the local subnet than version 3 servers,
a version 2 server is selected.</para>
</listitem>
</itemizedlist><note><para>Weighting is also influenced by keyword values in the  <filename>/etc/default/nfs</filename> file. Specifically, values for NFS_SERVER_VERSMIN, NFS_CLIENT_VERSMIN,
NFS_SERVER_VERSMAX, and NFS_CLIENT_VERSMAX can make some versions be excluded
from the sorting process. For more information about these keywords, see <olink targetptr="rfsrefer-133" remap="internal">Keywords for the /etc/default/nfs File</olink>.</para>
</note><para>With failover, the sorting is checked at mount time when a server is
selected. Multiple locations are useful in an  environment where individual
servers might not export their  file systems temporarily.</para><para>Failover is particularly useful in a large network with many subnets.
Autofs chooses the appropriate server and is able to confine NFS network traffic
to a segment of the local network. If a server has multiple network interfaces,
you can list the host name that is associated with each network interface
as if the interface were a separate server. Autofs selects the nearest interface
to the client.</para><note><para>No weighting and no proximity checks are performed with manual
mounts. The <command>mount</command> command prioritizes the servers that
are listed from left to right.</para>
</note><para>For more information, see <olink targetdoc="refman1m" targetptr="automount-1m" remap="external"><citerefentry><refentrytitle>automount</refentrytitle><manvolnum>1M</manvolnum></citerefentry></olink> man page.</para>
</sect2><sect2 id="egcoc"><title>Autofs and Weighting</title><para>You can influence the selection of servers at the same proximity level
by adding a weighting value to the autofs map. For example:</para><screen>/usr/man -ro oak,rose(1),willow(2):/usr/man</screen><para>The numbers in parentheses indicate a weighting. Servers without
a weighting have a value of zero and, therefore, are most likely to be selected.
The higher the weighting value, the lower the chance that the server is selected. </para><note><para>All other server selection factors are more important than weighting.
Weighting is only considered when selecting between servers with the same
network proximity.</para>
</note>
</sect2><sect2 id="rfsrefer-90"><title>Variables in a Map Entry</title><para>You can create a client-specific variable by prefixing a dollar
sign (<literal>$</literal>) to its name. The variable helps you to accommodate
different architecture types that are accessing the same file-system location.
You can also use curly braces to delimit the name of the variable from appended
letters or digits. <olink targetptr="rfsrefer-tbl-91" remap="internal">Table 6&#8211;7</olink> shows
the predefined map variables. </para><table frame="topbot" id="rfsrefer-tbl-91"><title>Predefined Map Variables</title><tgroup cols="4" colsep="0" rowsep="0"><colspec colnum="1" colname="column1" colwidth="2*"/><colspec colnum="2" colname="column2" colwidth="3*"/><colspec colnum="3" colname="column3" colwidth="2*"/><colspec colnum="4" colname="column4" colwidth="2*"/><thead><row rowsep="1"><entry><para>Variable</para>
</entry><entry><para>Meaning</para>
</entry><entry><para>Derived From</para>
</entry><entry><para>Example</para>
</entry>
</row>
</thead><tbody><row><entry><para><command>ARCH</command> </para>
</entry><entry><para>Architecture type</para>
</entry><entry><para><command>uname -m</command></para>
</entry><entry><para><command>sun4</command></para>
</entry>
</row><row><entry><para><command>CPU</command>  </para>
</entry><entry><para>Processor type</para>
</entry><entry><para><command>uname -p</command></para>
</entry><entry><para><command>sparc</command></para>
</entry>
</row><row><entry><para><command>HOST</command> </para>
</entry><entry><para>Host name</para>
</entry><entry><para><command>uname -n</command></para>
</entry><entry><para><command>dinky</command></para>
</entry>
</row><row><entry><para><command>OSNAME</command>  </para>
</entry><entry><para>Operating system name</para>
</entry><entry><para><command>uname -s</command></para>
</entry><entry><para><command>SunOS</command></para>
</entry>
</row><row><entry><para><command>OSREL</command> </para>
</entry><entry><para>Operating system release</para>
</entry><entry><para><command>uname -r</command></para>
</entry><entry><para><command>5.8</command></para>
</entry>
</row><row><entry><para><command>OSVERS</command> </para>
</entry><entry><para>Operating system version (version of the release)</para>
</entry><entry><para><command>uname -v</command></para>
</entry><entry><para><command>GENERIC</command></para>
</entry>
</row>
</tbody>
</tgroup>
</table><para>You can use variables anywhere in an entry line except as a key. For
instance, suppose that you have a file server that exports binaries for SPARC
and x86 architectures from <filename>/usr/local/bin/sparc</filename> and <filename>/usr/local/bin/x86</filename> respectively. The clients can mount through
a map entry such as the following: </para><screen>/usr/local/bin	   -ro	<replaceable>server</replaceable>:/usr/local/bin/$CPU</screen><para>Now the same entry for all clients applies to all architectures.
  </para><note><para>Most applications that are written for any of the sun4 architectures
can run on all sun4 platforms. The <option>ARCH</option> variable is hard-coded
to <literal>sun4</literal>.</para>
</note>
</sect2><sect2 id="rfsrefer-92"><title>Maps That Refer to Other Maps</title><para>A map entry +<replaceable>mapname</replaceable> that is used in
a file map causes automount to read the specified map as if it were included
in the current file. If <replaceable>mapname</replaceable> is not preceded
by a slash, autofs treats the map name as a string of characters and uses
the name-service switch policy to find the map name. If the path name is an
absolute path name, <command>automount</command> checks a local map of that
name. If the map name starts with a dash (<literal>-</literal>), <command>automount</command> consults the appropriate built-in map, such as <literal>hosts</literal>. </para><para>This name-service switch file contains an entry for autofs that is labeled
as <literal>automount</literal>, which contains the order in which the name
services are searched. The following file is an example of a name-service
switch file.</para><screen>#
# /etc/nsswitch.nis:
#
# An example file that could be copied over to /etc/nsswitch.conf;
# it uses NIS (YP) in conjunction with files.
#
# "hosts:" and "services:" in this file are used only if the /etc/netconfig
# file contains "switch.so" as a nametoaddr library for "inet" transports.
# the following two lines obviate the "+" entry in /etc/passwd and /etc/group.
passwd:         files nis
group:          files nis

# consult /etc "files" only if nis is down.
hosts:          nis [NOTFOUND=return] files
networks:       nis [NOTFOUND=return] files
protocols:      nis [NOTFOUND=return] files
rpc:            nis [NOTFOUND=return] files
ethers:         nis [NOTFOUND=return] files
netmasks:       nis [NOTFOUND=return] files
bootparams:     nis [NOTFOUND=return] files
publickey:      nis [NOTFOUND=return] files
netgroup:       nis
automount:      files nis
aliases:        files nis
# for efficient getservbyname() avoid nis
services:       files nis </screen><para>In this example, the local maps are searched before the NIS maps. Therefore,
 you can have a few entries in your local <filename>/etc/auto_home</filename> map
for the most commonly accessed home directories. You can then use the switch
to fall back to the NIS map for other entries. </para><screen>bill               cs.csc.edu:/export/home/bill
bonny              cs.csc.edu:/export/home/bonny</screen><para>After consulting the included map, if no match is found, <command>automount</command> continues scanning the current map. Therefore, you can add more
entries after a <literal>+</literal> entry. </para><screen>bill               cs.csc.edu:/export/home/bill
bonny              cs.csc.edu:/export/home/bonny
+auto_home </screen><para>The map that is included can be a local file or a built-in map. Remember,
only local files can contain <literal>+</literal> entries. </para><screen>+auto_home_finance      # NIS+ map
+auto_home_sales        # NIS+ map
+auto_home_engineering  # NIS+ map
+/etc/auto_mystuff      # local map
+auto_home              # NIS+ map
+-hosts                 # built-in hosts map </screen><note><para>You cannot use <literal>+</literal> entries in NIS+ or NIS maps.
   </para>
</note>
</sect2><sect2 id="rfsrefer-93"><title>Executable Autofs Maps</title><para>You can create an autofs map that executes some commands to generate
the autofs mount points. You could benefit from using an executable autofs
map if you need to be able to create the autofs structure from a database
or a flat file. The disadvantage to using an executable map is that the map
needs to be installed on each host. An executable map cannot be included in
either the NIS or the NIS+ name service.</para><para>The executable map must have an entry in the <literal>auto_master</literal> file.</para><screen>/execute    auto_execute</screen><para>Here is an example of an executable map:</para><screen>#!/bin/ksh
#
# executable map for autofs
#

case $1 in
	         src)  echo '-nosuid,hard bee:/export1' ;;
esac</screen><para>For this example to work, the file must be installed as <literal>/etc/auto_execute</literal> and must have the executable bit set. Set permissions to <literal>744</literal>.
Under these circumstances, running the following command causes the <literal>/export1</literal> file system from <literal>bee</literal> to be mounted:</para><screen>% ls /execute/src</screen>
</sect2><sect2 id="rfsrefer-94"><title>Modifying How Autofs Navigates the Network
(Modifying Maps)</title><para>You can modify, delete, or add entries to maps to meet the needs of
your environment. As applications and other file systems that users require
change their location, the maps must reflect those changes. You can modify
autofs maps at any time. Whether your modifications are effective the next
time <command>automountd</command> mounts a file system depends on which map
you modify and what kind of modification you make. </para>
</sect2><sect2 id="rfsrefer-95"><title>Default Autofs Behavior With Name Services</title><para>At boot time autofs is invoked by the service <command>svc:/system/filesystem/autofs</command> and autofs checks for the master <filename>auto_master</filename> map.
Autofs is subject to the rules that are discussed subsequently. </para><para>Autofs uses the name service that is specified in the automount entry
of the <filename>/etc/nsswitch.conf</filename> file. If NIS+ is specified,
as opposed to local files or NIS, all map names are used as is. If NIS is
selected and autofs cannot find a map that autofs needs, but finds a map name
that contains one or more underscores, the underscores are changed to dots.
This change allows the old NIS file names to work. Then autofs checks the
map again, as shown in <olink targetptr="rfsrefer-fig-96" remap="internal">Figure 6&#8211;6</olink>. </para><figure id="rfsrefer-fig-96"><title>How Autofs Uses the Name Service</title><mediaobject><imageobject><imagedata entityref="fig375.epsi"/>
</imageobject><textobject><simpara>The context describes the graphic.</simpara>
</textobject>
</mediaobject>
</figure><para>The screen activity for this session would resemble the following example. </para><screen>$ <userinput>grep /home /etc/auto_master</userinput>
/home           auto_home

$ <userinput>ypmatch brent auto_home</userinput>
Can't match key brent in map auto_home.  Reason: no such map in
server's domain.

$ <userinput>ypmatch brent auto.home</userinput>
diskus:/export/home/diskus1/&#38;</screen><para>If &#8220;files&#8221; is selected as the name service, all maps are
assumed to be local files in the <filename>/etc</filename> directory. Autofs
interprets a map name that begins with a slash (<literal>/</literal>) as local
regardless of which name service autofs uses.     </para>
</sect2>
</sect1><sect1 id="rfsrefer-98"><title>Autofs Reference</title><para>The remaining sections of this chapter describe more advanced
autofs features and topics. </para><sect2 id="rfsrefer-99"><title>Autofs and Metacharacters</title><para>Autofs recognizes some characters as having a special meaning.
Some characters are used for substitutions, and some characters are used to
protect other characters from the autofs map parser. </para><sect3 id="rfsrefer-151"><title>Ampersand (<literal>&#38;</literal>)</title><para>If you have a map with many subdirectories specified, as in the
following, consider using string substitutions.  </para><screen>john        willow:/home/john
mary        willow:/home/mary
joe         willow:/home/joe
able        pine:/export/able
baker       peach:/export/baker</screen><para>You can use the ampersand character (<literal>&#38;</literal>) to substitute
the key wherever the key appears. If you use the ampersand, the previous map
changes to the following: </para><screen>john        willow:/home/&#38;
mary        willow:/home/&#38;
joe         willow:/home/&#38;
able        pine:/export/&#38;
baker       peach:/export/&#38;</screen><para>You could also use key substitutions in a direct map, in situations
such as the following: </para><screen>/usr/man						willow,cedar,poplar:/usr/man</screen><para>You can also simplify the entry further as follows:</para><screen>/usr/man						willow,cedar,poplar:&#38;</screen><para>Notice that the ampersand substitution uses the whole key string. Therefore,
if the key in a direct map starts with a <literal>/</literal> (as it should),
the slash is included in the substitution. Consequently, for example, you
could not do the following: </para><screen>/progs				&#38;1,&#38;2,&#38;3:/export/src/progs </screen><para>The reason is that autofs would interpret the example as the following: </para><screen>/progs 				/progs1,/progs2,/progs3:/export/src/progs</screen>
</sect3><sect3 id="rfsrefer-150"><title>Asterisk (<literal>*</literal>)</title><para>You can use the universal substitute character, the asterisk (<literal>*</literal>), to match any key. You could mount the <literal>/export</literal> file
system from all hosts through this map entry.  </para><screen>*						&#38;:/export</screen><para>Each ampersand is substituted by the value of any given key. Autofs
interprets the asterisk as an end-of-file character.</para>
</sect3>
</sect2><sect2 id="rfsrefer-100"><title>Autofs and Special Characters</title><para>If you have a map entry that contains special characters, you
might have to mount directories that have names that confuse the autofs map
parser. The autofs parser is sensitive to names that contain colons, commas,
and spaces, for example. These names should be enclosed in double-quotes,
as in the following: </para><screen>/vms    -ro    vmsserver: -  -  - "rc0:dk1 - "
/mac    -ro    gator:/ - "Mr Disk - "</screen>
</sect2>
</sect1>
</chapter>